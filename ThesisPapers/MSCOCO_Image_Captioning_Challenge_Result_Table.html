<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0047)http://mscoco.org/dataset/#captions-leaderboard -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script src="./MSCOCO_Image_Captioning_Challenge_Result_Table_files/jquery.min.js"></script>
<link rel="stylesheet" href="./MSCOCO_Image_Captioning_Challenge_Result_Table_files/jquery-ui.css">
<script src="./MSCOCO_Image_Captioning_Challenge_Result_Table_files/jquery-ui.min.js"></script>
<link rel="stylesheet" href="./MSCOCO_Image_Captioning_Challenge_Result_Table_files/bootstrap.min.css">
<link rel="stylesheet" href="./MSCOCO_Image_Captioning_Challenge_Result_Table_files/bootstrap-theme.min.css">
<script src="./MSCOCO_Image_Captioning_Challenge_Result_Table_files/bootstrap.min.js"></script>

<link rel="stylesheet" href="./MSCOCO_Image_Captioning_Challenge_Result_Table_files/styles.css">
<script>
  // function to highlight selected tab
  $(function(){ highlightTab = function(tab) {
    $('#'+tab+'TabHead').css('background-color','#e16c03');
    $('#'+tab+'TabBody').css('background-color','#02b770');
    $('#'+tab+'TabBody').css('color', '#1d2125');
    }});
</script>


<script src="./MSCOCO_Image_Captioning_Challenge_Result_Table_files/jquery.dataTables.min.js"></script>
<link rel="stylesheet" href="./MSCOCO_Image_Captioning_Challenge_Result_Table_files/jquery.dataTables.min.css">
<script src="./MSCOCO_Image_Captioning_Challenge_Result_Table_files/dataTables.buttons.min.js"></script>
<script src="./MSCOCO_Image_Captioning_Challenge_Result_Table_files/buttons.html5.min.js"></script>
<script src="./MSCOCO_Image_Captioning_Challenge_Result_Table_files/jszip.min.js"></script>
<script src="./MSCOCO_Image_Captioning_Challenge_Result_Table_files/pdfmake.min.js"></script>
<script src="./MSCOCO_Image_Captioning_Challenge_Result_Table_files/vfs_fonts.js"></script>
<link rel="stylesheet" href="./MSCOCO_Image_Captioning_Challenge_Result_Table_files/buttons.dataTables.min.css">


  
  <title>COCO - Common Objects in Context</title>
  <script>$(window).load(function(){highlightTab('download')});</script>
  <script>
    // Enable subtab navigation controls. See also: http://getbootstrap.com/javascript/#tabs and
    // http://stackoverflow.com/questions/12131273/twitter-bootstrap-tabs-url-doesnt-change
    $(function(){
      var hash = window.location.hash;
      if(!hash) window.location.hash = '#overview'
      if(hash) $('ul.nav a[href="' + hash + '"]').tab('show');
      if(hash) $('html,body').scrollTop(0);
      $('.nav-tabs a').click(function (e) {
        if(this.hash) window.location.hash = this.hash;
        $('html,body').scrollTop(0);
      });
      $(window).bind( 'hashchange', function(e) {
        var hash = window.location.hash;
        $('html,body').scrollTop(0);
        $('#li-' + hash.substring(1) +' a').tab('show');
      });
    });
  </script>
</head>

<body>
  <div id="header">
  <div id="logoTabs">
    <div style="display:inline-block; width:313px; margin-left:14px; margin-top:4px">
      <img id="logoImage" style="width:285px; height:80px" src="./MSCOCO_Image_Captioning_Challenge_Result_Table_files/logo.png" alt="Logo">
    </div>
    <div style="width:400px; color:white;display:inline-block;position: absolute;bottom: 5px;">
      <div style="width:376px; height:55px; color:white;display:inline-block; margin-left: 40pt; font-size:10pt;text-align:right">
        cocodataset@outlook.com
      </div>
      <div style="width:440px;margin-left:0px;display:inline-block">
        <div id="homeTab" class="tabDivSelected" style="width:75px" onclick="window.location=&#39;/home&#39;">
          <div id="homeTabHead" class="tabDivSeparaterSelected1"></div>
          <div class="tabDivSeparater2"></div>
          <div id="homeTabBody" style="height:46px;font-size:16pt">
            Home
          </div>
          <div class="tabDivSeparater2" style="height:5px"></div>
        </div>
        <div id="peopleTab" class="tabDiv" style="width:75px" onclick="window.location=&#39;/people&#39;">
          <div id="peopleTabHead" class="tabDivSeparater1"></div>
          <div class="tabDivSeparater2"></div>
          <div id="peopleTabBody" style="height:46px;font-size:16pt">
            People
          </div>
          <div class="tabDivSeparater2" style="height:5px"></div>
        </div>
        <div id="exploreTab" class="tabDiv" style="width:85px" onclick="window.location=&#39;/explore&#39;">
          <div id="exploreTabHead" class="tabDivSeparater1"></div>
          <div class="tabDivSeparater2"></div>
          <div id="exploreTabBody" style="height:46px;font-size:16pt">
            Explore
          </div>
          <div class="tabDivSeparater2" style="height:5px"></div>
        </div>
        <div id="downloadTab" class="tabDiv" style="width:85px" onclick="window.location=&#39;/dataset&#39;">
          <div id="downloadTabHead" class="tabDivSeparater1" style="background-color: rgb(225, 108, 3);"></div>
          <div class="tabDivSeparater2"></div>
          <div id="downloadTabBody" style="height: 46px; font-size: 16pt; color: rgb(29, 33, 37); background-color: rgb(2, 183, 112);">
            Dataset
          </div>
          <div class="tabDivSeparater2" style="height:5px"></div>
        </div>
        <div id="externalTab" class="tabDiv" style="width:90px" onclick="window.location=&#39;/external&#39;">
          <div id="externalTabHead" class="tabDivSeparater1"></div>
          <div class="tabDivSeparater2"></div>
          <div id="externalTabBody" style="height:46px;font-size:16pt">
            External
          </div>
          <div class="tabDivSeparater2" style="height:5px"></div>
        </div>
      </div>
    </div>
  </div>
</div>

  <div id="contentRow">
    <div class="centerwrapper" style="position:relative; top:10px; margin-bottom: 10px">
      <div id="contentColumn" style="color:black;width:800px">
        <div class="bs-docs-example">
          <ul id="myTab" class="nav nav-tabs">
            <li id="li-overview" class="">
              <a href="http://mscoco.org/dataset/#overview" data-toggle="tab" aria-expanded="false"><span class="glyphicon glyphicon-info-sign" aria-hidden="true"></span> Overview</a>
            </li>
            <li id="li-challenge" class="dropdown">
              <a class="dropdown-toggle" data-toggle="dropdown" aria-expanded="false"><span class="glyphicon glyphicon-flag" aria-hidden="true"></span> Challenges<b class="caret"></b></a>
              <ul class="dropdown-menu">
                <li id="li-keypoints-challenge2016"><a href="http://mscoco.org/dataset/#keypoints-challenge2016" data-toggle="tab" aria-expanded="false">Keypoints 2016</a></li>
                <li id="li-detections-challenge2016"><a href="http://mscoco.org/dataset/#detections-challenge2016" data-toggle="tab" aria-expanded="false">Detection 2016</a></li>
                <li id="li-detections-challenge2015"><a href="http://mscoco.org/dataset/#detections-challenge2015" data-toggle="tab" aria-expanded="false">Detection 2015</a></li>
                <li id="li-captions-challenge2015" class=""><a href="http://mscoco.org/dataset/#captions-challenge2015" data-toggle="tab" aria-expanded="false">Captioning 2015</a></li>
              </ul>
            </li>
            <li id="li-download">
              <a id="a-download" href="http://mscoco.org/dataset/#download" data-toggle="tab"><span class="glyphicon glyphicon-download" aria-hidden="true"></span> Download</a>
            </li>
            <li id="li-evaluate" class="dropdown">
              <a class="dropdown-toggle" data-toggle="dropdown" aria-expanded="false"><span class="glyphicon glyphicon-stats" aria-hidden="true"></span> Evaluate<b class="caret"></b></a>
              <ul class="dropdown-menu">
                <li id="li-format"><a href="http://mscoco.org/dataset/#format" data-toggle="tab" aria-expanded="false">Results Format</a></li>
                <li class="divider"></li>
                <li id="li-detections-eval"><a href="http://mscoco.org/dataset/#detections-eval" data-toggle="tab" aria-expanded="false">Detections: Eval</a></li>
                <li id="li-detections-upload"><a href="http://mscoco.org/dataset/#detections-upload" data-toggle="tab" aria-expanded="false">Detections: Upload</a></li>
                <li class="divider"></li>
                <li id="li-keypoints-eval"><a href="http://mscoco.org/dataset/#keypoints-eval" data-toggle="tab" aria-expanded="false">Keypoints: Eval</a></li>
                <li id="li-keypoints-upload"><a href="http://mscoco.org/dataset/#keypoints-upload" data-toggle="tab" aria-expanded="false">Keypoints: Upload</a></li>
                <li class="divider"></li>
                <li id="li-captions-eval" class=""><a href="http://mscoco.org/dataset/#captions-eval" data-toggle="tab" aria-expanded="false">Captions: Eval</a></li>
                <li id="li-captions-upload"><a href="http://mscoco.org/dataset/#captions-upload" data-toggle="tab" aria-expanded="false">Captions: Upload</a></li>
              </ul>
            </li>
            <li id="li-leaderboard" class="dropdown active">
              <a class="dropdown-toggle" data-toggle="dropdown" aria-expanded="false"><span class="glyphicon glyphicon-th-list" aria-hidden="true"></span> Leaderboard<b class="caret"></b></a>
              <ul class="dropdown-menu">
                <li id="li-detections-leaderboard"><a href="http://mscoco.org/dataset/#detections-leaderboard" data-toggle="tab">Detection</a></li>
                <li id="li-keypoints-leaderboard"><a href="http://mscoco.org/dataset/#keypoints-leaderboard" data-toggle="tab">Keypoints</a></li>
                <li id="li-captions-leaderboard" class="active"><a href="http://mscoco.org/dataset/#captions-leaderboard" data-toggle="tab" aria-expanded="true">Captioning</a></li>
              </ul>
            </li>
          </ul>
          <!-- Tab panes -->
          <div class="tab-content">
            <div role="tabpanel" class="tab-pane fade" id="overview"> <!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">
<span class="glyphicon glyphicon-flag" aria-hidden="true"></span> Challenges: <span style="font-size:80%;">
  <a href="http://mscoco.org/dataset/#detections-challenge2016">Detections</a> | <a href="http://mscoco.org/dataset/#captions-challenge2015">Captions</a> | <a href="http://mscoco.org/dataset/#keypoints-challenge2016">Keypoints</a>
</span>
</p>
<p class="bodyNormal" style="color:black;margin-bottom:40px" align="justify">
Begin by learning about the individual challenges. Compete to earn prizes and opportunities to present your work. Come to the workshops to learn about the state of the art!
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">
<span class="glyphicon glyphicon-download" aria-hidden="true"></span>
<a href="http://mscoco.org/dataset/#download">Download</a>
</p>
<p class="bodyNormal" style="color:black;margin-bottom:40px" align="justify">
Download the dataset, including the dataset tools, images, and annotations. Learn about the annotation format. See <span class="func_or_var">cocoDemo</span> in either the Matlab or Python code.<br>
<a href="https://github.com/pdollar/coco/blob/master/PythonAPI/pycocoDemo.ipynb" target="_blank">
  <span class="glyphicon glyphicon-file" aria-hidden="true"></span> cocoDemo</a>
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">
<span class="glyphicon glyphicon-stats" aria-hidden="true"></span>
<a href="http://mscoco.org/dataset/#format"> Results Format </a>
</p>
<p class="bodyNormal" style="color:black;margin-bottom:40px" align="justify">
Develop your algorithm. Run your algorithm on COCO and save the results using the format described on this page. See <span class="func_or_var">evalDemo</span> in either the Matlab or Python code.<br>
<a href="https://github.com/pdollar/coco/blob/master/PythonAPI/pycocoEvalDemo.ipynb" target="_blank">
  <span class="glyphicon glyphicon-file" aria-hidden="true"></span> evalDemo</a>
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">
<span class="glyphicon glyphicon-stats" aria-hidden="true"></span> Evaluate: <span style="font-size:80%;">
  <a href="http://mscoco.org/dataset/#detections-eval">Detections</a> | <a href="http://mscoco.org/dataset/#captions-eval">Captions</a>
</span>
</p>
<p class="bodyNormal" style="color:black;margin-bottom:40px" align="justify">
Evaluate results of your system on the validation set. The same code will run on the evaluation server when you upload your results. See <span class="func_or_var">evalDemo</span> in either the Matlab or Python code and <span class="func_or_var">evalCapDemo</span> in the Python code for detection and caption demo code, respectively.<br>
<a href="https://github.com/pdollar/coco/blob/master/PythonAPI/pycocoEvalDemo.ipynb" target="_blank">
  <span class="glyphicon glyphicon-file" aria-hidden="true"></span> evalDemo</a> |
<a href="https://github.com/tylin/coco-caption/blob/master/cocoEvalCapDemo.ipynb" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span> evalCapDemo</a>
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%;">
<span class="glyphicon glyphicon-stats" aria-hidden="true"></span> Upload: <span style="font-size:80%;">
  <a href="http://mscoco.org/dataset/#detections-upload">Detections</a> | <a href="http://mscoco.org/dataset/#captions-upload">Captions</a>
</span>
</p>
<p class="bodyNormal" style="color:black;margin-bottom:40px" align="justify">
Upload your results to the evaluation server.
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%;">
<span class="glyphicon glyphicon-th-list" aria-hidden="true"></span> Leaderboard: <span style="font-size:80%;">
  <a href="http://mscoco.org/dataset/#detections-leaderboard">Detections</a> | <a href="http://mscoco.org/dataset/#captions-leaderboard">Captions</a>
</span>
</p>
<p class="bodyNormal" style="color:black;margin-bottom:40px" align="justify">
Check out the state-of-the-art! See what algorithms are best at the various tasks.
</p>

<br>
<br>
</div>
            <div role="tabpanel" class="tab-pane fade" id="download">           <!------------------------------------------------------------------------------------------------>
<div id="contentColumnNoPadLeft" style="width:28%;height:200px">
  <p class="titleSegoeLight">Tools</p>
  <p class="bodyNormal" style="font-size:100%;">
  <a href="https://github.com/pdollar/coco" target="_blank">Matlab+Python+Lua APIs [Version 2.0]</a>
  </p>
  <p style="font-size:8pt;" align="justify">V2.0 of the API was completed 07/2015 and includes detection evaluation code. The Lua API, added 05/2016, supports only load and view functionality (no eval code).</p>
</div>
<div id="contentColumn" style="width:34%;color:black;height:200px">
  <p class="titleSegoeLight">Images</p>
  <p class="bodyNormal" style="font-size:70%;line-height:200%;">
  <a href="http://msvocds.blob.core.windows.net/coco2014/train2014.zip">2014 Training images [80K/13GB]</a><br>
  <a href="http://msvocds.blob.core.windows.net/coco2014/val2014.zip">2014 Val. images [40K/6.2GB]</a><br>
  <a href="http://msvocds.blob.core.windows.net/coco2014/test2014.zip">2014 Testing images [40K/6.2GB]</a><br>
  <a href="http://msvocds.blob.core.windows.net/coco2015/test2015.zip">2015 Testing images [80K/12.4G]</a>
  </p>
</div>
<div id="contentColumnNoPadLeft" style="width:38%;height:200px">
  <p class="titleSegoeLight">Annotations</p>
  <p class="bodyNormal" style="font-size:70%;line-height:200%;">
  <a href="http://msvocds.blob.core.windows.net/annotations-1-0-3/instances_train-val2014.zip">2014 Train/Val object instances [158MB]</a><br>
  <a href="http://msvocds.blob.core.windows.net/annotations-1-0-3/person_keypoints_trainval2014.zip">2014 Train/Val person keypoints [70MB]</a><br>
  <a href="http://msvocds.blob.core.windows.net/annotations-1-0-3/captions_train-val2014.zip">2014 Train/Val image captions [18.8MB]</a><br>
  <a href="http://msvocds.blob.core.windows.net/annotations-1-0-4/image_info_test2014.zip">2014 Testing Image info [0.74MB]</a><br>
  <a href="http://msvocds.blob.core.windows.net/annotations-1-0-4/image_info_test2015.zip">2015 Testing Image info [1.83MB]</a>
  </p><p style="font-size:8pt;color:black" align="justify">
  Note: annotations updated on 07/23/2015 with the addition of a "coco_url" field (for allowing of direct downloads of individual images).
  </p>
</div>

<div style="color: black; margin-top:-10px:width:100%">
  <!------------------------------------------------------------------------------------------------>
  <p class="titleSegoeLight" style="width:760px">1. Overview</p>
  <p class="bodyNormal" style="color:black;" align="justify">
  The 2014 Testing Images are for the <a href="http://mscoco.org/dataset/#captions-challenge2015">COCO Captioning Challenge</a>, while the 2015 Testing Images are for the <a href="http://mscoco.org/dataset/#detections-challenge2016">Detection</a> and <a href="http://mscoco.org/dataset/#keypoints-challenge2016">Keypoint</a> Challenges. The train and val data are common to all challenges. Note also that as an alternative to downloading the large image zip files, individual images may be downloaded from the COCO website using the "coco_url" field specified in the image info struct (see details below).
  </p>
  <p class="bodyNormal" style="color:black;" align="justify">
  Please follow the instructions in the <a href="https://github.com/pdollar/coco" target="_blank">README</a> to download and setup the COCO data (annotations and images). By downloading this dataset, you agree to our <a href="http://mscoco.org/terms_of_use" target="_blank">Terms of Use</a>.
  </p>

  <!------------------------------------------------------------------------------------------------>
  <p class="titleSegoeLight" style="width:100%">2. COCO API</p>
  <p class="bodyNormal" style="color:black;" align="justify">
  The COCO API assists in loading, parsing, and visualizing annotations in COCO. The API supports object instance, object keypoint, and image caption annotations (for captions not all functionality is defined). For additional details see: <a href="https://github.com/pdollar/coco/blob/master/MatlabAPI/CocoApi.m" target="_blank">CocoApi.m</a>, <a href="https://github.com/pdollar/coco/blob/master/PythonAPI/pycocotools/coco.py" target="_blank">coco.py</a>, and <a href="https://github.com/pdollar/coco/blob/master/LuaAPI/CocoApi.lua" target="_blank">CocoApi.lua</a> for Matlab, Python, and Lua code, respectively, and also the <a href="https://github.com/pdollar/coco/blob/master/PythonAPI/pycocoDemo.ipynb" target="_blank">Python API demo</a>.</p>
  <div class="bodyNormal function" style="color:black;width:100%">
    <div>Throughout the API "ann"=annotation, "cat"=category, and "img"=image.</div>
    <div>
      <div class="function-title" style="margin-top:10px; width:25%; display:inline-block">download</div>
      <div style="margin-top:10px; width:70%; display:inline-block">Download COCO images from mscoco.org server.</div>
    </div>
    <div>
      <div class="function-title" style="margin-top:10px; width:25%; display:inline-block">getAnnIds</div>
      <div style="margin-top:10px; width:70%; display:inline-block">Get ann ids that satisfy given filter conditions.</div>
    </div>
    <div>
      <div class="function-title" style="margin-top:10px; width:25%; display:inline-block">getCatIds</div>
      <div style="margin-top:10px; width:70%; display:inline-block">Get cat ids that satisfy given filter conditions.</div>
    </div>
    <div>
      <div class="function-title" style="margin-top:10px; width:25%; display:inline-block">getImgIds</div>
      <div style="margin-top:10px; width:70%; display:inline-block">Get img ids that satisfy given filter conditions.</div>
    </div>
    <div>
      <div class="function-title" style="margin-top:10px; width:25%; display:inline-block">loadAnns</div>
      <div style="margin-top:10px; width:70%; display:inline-block">Load anns with the specified ids.</div>
    </div>
    <div>
      <div class="function-title" style="margin-top:10px; width:25%; display:inline-block">loadCats</div>
      <div style="margin-top:10px; width:70%; display:inline-block">Load cats with the specified ids.</div>
    </div>
    <div>
      <div class="function-title" style="margin-top:10px; width:25%; display:inline-block">loadImgs</div>
      <div style="margin-top:10px; width:70%; display:inline-block">Load imgs with the specified ids.</div>
    </div>
    <div>
      <div class="function-title" style="margin-top:10px; width:25%; display:inline-block">loadRes</div>
      <div style="margin-top:10px; width:70%; display:inline-block">Load algorithm results and create API for accessing them.</div>
    </div>
    <div>
      <div class="function-title" style="margin-top:10px; width:25%; display:inline-block">showAnns</div>
      <div style="margin-top:10px; width:70%; display:inline-block">Display the specified annotations.</div>
    </div>
  </div>

  <!------------------------------------------------------------------------------------------------>
  <p class="titleSegoeLight" style="width:100%">3. MASK API</p>
  <p class="bodyNormal" style="color:black;" align="justify">
  COCO provides segmentation masks for every object instance. This creates two challenges: storing masks compactly and performing mask computations efficiently. We solve both challenges using a custom Run Length Encoding (RLE) scheme. The size of the RLE representation is proportional to the number of boundaries pixels of a mask and operations such as area, union, or intersection can be computed efficiently directly on the RLE. Specifically, assuming fairly simple shapes, the RLE representation is O(√n) where n is number of pixels in the object, and common computations are likewise O(√n). Naively computing the same operations on the decoded masks (stored as an array) would be O(n).
  </p>
  <p class="bodyNormal" style="color:black;" align="justify">
  The MASK API provides an interface for manipulating masks stored in RLE format. The API is defined below, for additional details see: <a href="https://github.com/pdollar/coco/blob/master/MatlabAPI/MaskApi.m" target="_blank">MaskApi.m</a>, <a href="https://github.com/pdollar/coco/blob/master/PythonAPI/pycocotools/mask.py" target="_blank">mask.py</a>, or <a href="https://github.com/pdollar/coco/blob/master/LuaAPI/MaskApi.lua" target="_blank">MaskApi.lua</a>. Finally, we note that a majority of ground truth masks are stored as polygons (which are quite compact), these polygons are converted to RLE when needed.
  </p>
  <div class="bodyNormal function" style="color:black;width:100%">
    <div>
      <div class="function-title" style="margin-top:10px; width:25%; display:inline-block">encode</div>
      <div style="margin-top:10px; width:70%; display:inline-block">Encode binary masks using RLE.</div>
    </div>
    <div>
      <div class="function-title" style="margin-top:10px; width:25%; display:inline-block">decode</div>
      <div style="margin-top:10px; width:70%; display:inline-block">Decode binary masks encoded via RLE.</div>
    </div>
    <div>
      <div class="function-title" style="margin-top:10px; width:25%; display:inline-block">merge</div>
      <div style="margin-top:10px; width:70%; display:inline-block">Compute union or intersection of encoded masks.</div>
    </div>
    <div>
      <div class="function-title" style="margin-top:10px; width:25%; display:inline-block">iou</div>
      <div style="margin-top:10px; width:70%; display:inline-block">Compute intersection over union between masks.</div>
    </div>
    <div>
      <div class="function-title" style="margin-top:10px; width:25%; display:inline-block">area</div>
      <div style="margin-top:10px; width:70%; display:inline-block">Compute area of encoded masks.</div>
    </div>
    <div>
      <div class="function-title" style="margin-top:10px; width:25%; display:inline-block">toBbox</div>
      <div style="margin-top:10px; width:70%; display:inline-block">Get bounding boxes surrounding encoded masks.</div>
    </div>
    <div>
      <div class="function-title" style="margin-top:10px; width:25%; display:inline-block">frBbox</div>
      <div style="margin-top:10px; width:70%; display:inline-block">Convert bounding boxes to encoded masks.</div>
    </div>
    <div>
      <div class="function-title" style="margin-top:10px; width:25%; display:inline-block">frPoly</div>
      <div style="margin-top:10px; width:70%; display:inline-block">Convert polygon to encoded mask.</div>
    </div>
  </div>

  <!------------------------------------------------------------------------------------------------>
  <p class="titleSegoeLight">4. Annotation format</p>
  <p class="bodyNormal" style="color:black" align="justify">
  COCO currently has three annotation types: object instances, object keypoints, and image captions. The annotations are stored using the <a href="http://json.org/" target="_blank">JSON</a> file format. All annotations share the basic data structure below:
  </p>
  <div class="bodyNormal json" style="color:black;width:100%">
    <div class="jsontabstartend"> {</div>
    <div class="jsontab"><div class="jsonfield">"info"           </div><span>: </span><div class="jsonvalue">info,</div></div>
    <div class="jsontab"><div class="jsonfield">"images"         </div><span>: </span><div class="jsonvalue">[image],</div></div>
    <div class="jsontab"><div class="jsonfield">"annotations"    </div><span>: </span><div class="jsonvalue">[annotation],</div></div>
    <div class="jsontab"><div class="jsonfield">"licenses"       </div><span>: </span><div class="jsonvalue">[license],</div></div>
    <div class="jsontabstartend"> }</div>
    <br>
    <div class="jsontabstartend"> <span class="jsonobject">info </span>{</div>
    <div class="jsontab"><div class="jsonfield">"year"           </div><span>: </span><div class="jsonvalue">int,</div></div>
    <div class="jsontab"><div class="jsonfield">"version"        </div><span>: </span><div class="jsonvalue">str,</div></div>
    <div class="jsontab"><div class="jsonfield">"description"    </div><span>: </span><div class="jsonvalue">str,</div></div>
    <div class="jsontab"><div class="jsonfield">"contributor"    </div><span>: </span><div class="jsonvalue">str,</div></div>
    <div class="jsontab"><div class="jsonfield">"url"            </div><span>: </span><div class="jsonvalue">str,</div></div>
    <div class="jsontab"><div class="jsonfield">"date_created"   </div><span>: </span><div class="jsonvalue">datetime,</div></div>
    <div class="jsontabstartend"> }</div>
    <br>
    <div class="jsontabstartend"> <span class="jsonobject">image</span>{</div>
    <div class="jsontab"><div class="jsonfield">"id"             </div><span>: </span><div class="jsonvalue">int,</div></div>
    <div class="jsontab"><div class="jsonfield">"width"          </div><span>: </span><div class="jsonvalue">int,</div></div>
    <div class="jsontab"><div class="jsonfield">"height"         </div><span>: </span><div class="jsonvalue">int,</div></div>
    <div class="jsontab"><div class="jsonfield">"file_name"      </div><span>: </span><div class="jsonvalue">str,</div></div>
    <div class="jsontab"><div class="jsonfield">"license"        </div><span>: </span><div class="jsonvalue">int,</div></div>
    <div class="jsontab"><div class="jsonfield">"flickr_url"     </div><span>: </span><div class="jsonvalue">str,</div></div>
    <div class="jsontab"><div class="jsonfield">"coco_url"       </div><span>: </span><div class="jsonvalue">str,</div></div>
    <div class="jsontab"><div class="jsonfield">"date_captured"  </div><span>: </span><div class="jsonvalue">datetime,</div></div>
    <div class="jsontabstartend"> }</div>
    <br>
    <div class="jsontabstartend"> <span class="jsonobject">license</span>{</div>
    <div class="jsontab"><div class="jsonfield">"id"             </div><span>: </span><div class="jsonvalue">int,</div></div>
    <div class="jsontab"><div class="jsonfield">"name"           </div><span>: </span><div class="jsonvalue">str,</div></div>
    <div class="jsontab"><div class="jsonfield">"url"            </div><span>: </span><div class="jsonvalue">str,</div></div>
    <div class="jsontabstartend"> }</div>
  </div>
  <p class="bodyNormal" style="color:black">
  The data structures specific to the various annotation types are described below.
  </p>

  <!------------------------------------------------------------------------------------------------>
  <p class="subtitleSegoeLight" style="font-size:18pt">
  4.1. Object Instance Annotations
  </p>
  <p class="bodyNormal" style="color:black" align="justify">
  Each instance annotation contains a series of fields, including the category id and segmentation mask of the object. The segmentation format depends on whether the instance represents a single object (iscrowd=0 in which case polygons are used) or a collection of objects (iscrowd=1 in which case RLE is used). Note that a single object (iscrowd=0) may require multiple polygons, for example if occluded. Crowd annotations (iscrowd=1) are used to label large groups of objects (e.g. a crowd of people). In addition, an enclosing bounding box is provided for each object (box coordinates are measured from the top left image corner and are 0-indexed). Finally, the categories field of the annotation structure stores the mapping of category id to category and supercategory names. See also the <a href="http://mscoco.org/dataset/#detections-challenge2016">Detection Challenge</a>.
  </p>
  <div class="bodyNormal json" style="color:black;width:100%">
    <div class="jsontabstartend"> <span class="jsonobject">annotation</span>{</div>
    <div class="jsontab"><div class="jsonfield">"id"             </div><span>: </span><div class="jsonvalue">int,</div></div>
    <div class="jsontab"><div class="jsonfield">"image_id"       </div><span>: </span><div class="jsonvalue">int,</div></div>
    <div class="jsontab"><div class="jsonfield">"category_id"    </div><span>: </span><div class="jsonvalue">int,</div></div>
    <div class="jsontab"><div class="jsonfield">"segmentation"   </div><span>: </span><div class="jsonvalue">RLE or [polygon],</div></div>
    <div class="jsontab"><div class="jsonfield">"area"           </div><span>: </span><div class="jsonvalue">float,</div></div>
    <div class="jsontab"><div class="jsonfield">"bbox"           </div><span>: </span><div class="jsonvalue">[x,y,width,height],</div></div>
    <div class="jsontab"><div class="jsonfield">"iscrowd"        </div><span>: </span><div class="jsonvalue">0 or 1,</div></div>
    <div class="jsontabstartend"> }</div>
    <br>
    <div class="jsontabstartend"> <span class="jsonobject">categories</span>[{</div>
    <div class="jsontab"><div class="jsonfield">"id"             </div><span>: </span><div class="jsonvalue">int,</div></div>
    <div class="jsontab"><div class="jsonfield">"name"           </div><span>: </span><div class="jsonvalue">str,</div></div>
    <div class="jsontab"><div class="jsonfield">"supercategory"  </div><span>: </span><div class="jsonvalue">str,</div></div>
    <div class="jsontabstartend"> }]</div>
  </div>

  <!------------------------------------------------------------------------------------------------>
  <p class="subtitleSegoeLight">
  4.2. Object Keypoint Annotations
  </p>
  <p class="bodyNormal" style="color:black" align="justify">
  A keypoint annotation contains all the data of the object annotation (including id, bbox, etc.) and two additional fields. First, "keypoints" is a length 3k array where k is the total number of keypoints defined for the category. Each keypoint has a 0-indexed location x,y and a visibility flag v defined as v=0: not labeled (in which case x=y=0), v=1: labeled but not visible, and v=2: labeled and visible. A keypoint is considered visible if it falls inside the object segment. "num_keypoints" indicates the number of labeled keypoints (v&gt;0) for a given object (many objects, e.g. crowds and small objects, will have num_keypoints=0). Finally, for each category, the categories struct has two additional fields: "keypoints," which is a length k array of keypoint names, and "skeleton", which defines connectivity via a list of keypoint edge pairs and is used for visualization. Currently keypoints are only labeled for the person category (for most medium/large non-crowd person instances). See also the <a href="http://mscoco.org/dataset/#keypoints-challenge2016">Keypoint Challenge</a>.
  </p>
  <div class="bodyNormal json" style="color:black;width:100%">
    <div class="jsontabstartend"> <span class="jsonobject">annotation</span>{</div>
    <div class="jsontab"><div class="jsonfield">"keypoints"        </div><span>: </span><div class="jsonvalue">[x1,y1,v1,...],</div></div>
    <div class="jsontab"><div class="jsonfield">"num_keypoints"    </div><span>: </span><div class="jsonvalue">int,</div></div>
    <div class="jsontab"><div class="jsonfield">"[cloned]"         </div><span>: </span><div class="jsonvalue">...,</div></div>
    <div class="jsontabstartend"> }</div>
    <br>
    <div class="jsontabstartend"> <span class="jsonobject">categories</span>[{</div>
    <div class="jsontab"><div class="jsonfield">"keypoints"        </div><span>: </span><div class="jsonvalue">[str],</div></div>
    <div class="jsontab"><div class="jsonfield">"skeleton"         </div><span>: </span><div class="jsonvalue">[edge],</div></div>
    <div class="jsontab"><div class="jsonfield">"[cloned]"         </div><span>: </span><div class="jsonvalue">...,</div></div>
    <div class="jsontabstartend"> }]</div>
    <br>
    <div class="jsontabstartend">"[cloned]": denotes fields copied from object instance annotations defined in 4.1.</div>
  </div>

  <!------------------------------------------------------------------------------------------------>
  <p class="subtitleSegoeLight">
  4.3. Image Caption Annotations
  </p>
  <p class="bodyNormal" style="color:black" align="justify">
  These annotations are used to store image captions. Each caption describes the specified image and each image has at least 5 captions (some images have more). See also the <a href="http://mscoco.org/dataset/#captions-challenge2015">Captioning Challenge</a>.
  </p>
  <div class="bodyNormal json" style="color:black;width:100%">
    <div class="jsontabstartend"> <span class="jsonobject">annotation</span>{</div>
    <div class="jsontab"><div class="jsonfield">"id"               </div><span>: </span><div class="jsonvalue">int,</div></div>
    <div class="jsontab"><div class="jsonfield">"image_id"         </div><span>: </span><div class="jsonvalue">int,</div></div>
    <div class="jsontab"><div class="jsonfield">"caption"          </div><span>: </span><div class="jsonvalue">str,</div></div>
    <div class="jsontabstartend"> }</div>
  </div>

</div>
<br>
<br>
</div>
            <div role="tabpanel" class="tab-pane fade" id="format">             <!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">1. Results Format Overview</p>
<p class="bodyNormal" style="color:black;" align="justify">
This page describes the <span style="font-style:italic">results format</span> used by COCO. The general structure of the results format is similar for all annotation types: for both object detection (using either bounding boxes or object segments) and image caption generation. Submitting algorithm results on COCO for evaluation requires using the formats described below.
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">2. Results Format</p>
<p class="bodyNormal" style="color:black;" align="justify">
The results format used by COCO closely mimics the format of the ground truth as described on the <a href="http://mscoco.org/dataset/#download">download </a> page. We suggest reviewing the ground truth format before proceeding.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
Each algorithmically produced result, such as an object bounding box, object segment, or image caption, is stored separately in its own <span style="font-style:italic">result</span> struct.
This singleton <span style="font-style:italic">result</span> struct must contains the id of the image from which the result was generated (note that a single image will typically have multiple associated results). Results across the whole dataset are aggregated in an array of such <span style="font-style:italic">result</span> structs. Finally, the entire <span style="font-style:italic">result</span> struct array is stored to disk as a single JSON file (save via
<a href="https://github.com/pdollar/coco/blob/master/MatlabAPI/gason.m" target="_blank" style="font-family: courier new">gason</a>
in Matlab or
<a href="https://docs.python.org/2/library/json.html" target="_blank" style="font-family: courier new">json.dump</a>
in Python).
</p>
<p class="bodyNormal" style="color:black;" align="justify">
The data struct for each of the three result types is described below. The format of the individual fields below (category_id, bbox, segmentation, etc.) is the same as for the ground truth (for details see the <a href="http://mscoco.org/dataset/#download">download</a> page).
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="font-size:18pt">
2.1. Object detection (bounding boxes)
</p>
<div class="bodyNormal json" style="color:black;width:100%">
  <div class="jsontabstartend"> [{</div>
  <div class="jsontab"> <div class="jsonfield">"image_id"     </div><span>:</span> <div class="jsonvalue">int,  </div></div>
  <div class="jsontab"> <div class="jsonfield">"category_id"  </div><span>:</span> <div class="jsonvalue">int,  </div></div>
  <div class="jsontab"> <div class="jsonfield">"bbox"         </div><span>:</span> <div class="jsonvalue">[x,y,width,height],</div></div>
  <div class="jsontab"> <div class="jsonfield">"score"        </div><span>:</span> <div class="jsonvalue">float,</div></div>
  <div class="jsontabstartend"> }]</div>
</div>
<p class="bodyNormal" style="color:black;" align="justify">
Note: box coordinates are floats measured from the top left image corner (and are 0-indexed). We recommend rounding coordinates to the nearest tenth of a pixel to reduce resulting JSON file size.
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="font-size:18pt">
2.2. Object detection (segmentation)
</p>
<div class="bodyNormal json" style="color:black;width:100%">
  <div class="jsontabstartend"> [{</div>
  <div class="jsontab"> <div class="jsonfield">"image_id"     </div><span>:</span> <div class="jsonvalue">int,  </div></div>
  <div class="jsontab"> <div class="jsonfield">"category_id"  </div><span>:</span> <div class="jsonvalue">int,  </div></div>
  <div class="jsontab"> <div class="jsonfield">"segmentation" </div><span>:</span> <div class="jsonvalue">RLE,  </div></div>
  <div class="jsontab"> <div class="jsonfield">"score"        </div><span>:</span> <div class="jsonvalue">float,</div></div>
  <div class="jsontabstartend"> }]</div>
</div>
<p class="bodyNormal" style="color:black;" align="justify">
Note: a binary mask containing an object segment should be encoded to RLE using the MaskApi function <span class="func_or_var">encode()</span>. For additional details see either <a href="https://github.com/pdollar/coco/blob/master/MatlabAPI/MaskApi.m" target="_blank">MaskApi.m</a> or <a href="https://github.com/pdollar/coco/blob/master/PythonAPI/pycocotools/mask.py" target="_blank">mask.py</a>. Note that the core RLE code is written in c (see <a href="https://github.com/pdollar/coco/blob/master/MatlabAPI/private/maskApi.h" target="_blank">maskApi.h</a>), so it is possible to perform encoding without using Matlab or Python, but we do not provide support for this case.
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="font-size:18pt">
2.3. Keypoint detection
</p><div class="bodyNormal json" style="color:black;width:100%;">
  <div class="jsontabstartend"> [{</div>
  <div class="jsontab"> <div class="jsonfield">"image_id"     </div><span>:</span> <div class="jsonvalue">int,  </div></div>
  <div class="jsontab"> <div class="jsonfield">"category_id"  </div><span>:</span> <div class="jsonvalue">int,  </div></div>
  <div class="jsontab"> <div class="jsonfield">"keypoints"    </div><span>:</span> <div class="jsonvalue"> [x<sub>1</sub>,y<sub>1</sub>,v<sub>1</sub>,...,x<sub>k</sub>,y<sub>k</sub>,v<sub>k</sub>],</div></div>
  <div class="jsontab"> <div class="jsonfield">"score"        </div><span>:</span> <div class="jsonvalue">float,</div></div>
  <div class="jsontabstartend"> }]</div>
</div>
<p class="bodyNormal" style="color:black;" align="justify">
Note: keypoint coordinates are floats measured from the top left image corner (and are 0-indexed). We recommend rounding coordinates to the nearest pixel to reduce file size. Note also that the visibility flags v<sub>i</sub> are <i>not</i> currently used (except for controlling visualization), we recommend simply setting v<sub>i</sub>=1.
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="font-size:18pt">
2.4. Caption generation
</p>
<div class="bodyNormal json" style="color:black;width:100%">
  <div class="jsontabstartend"> [{</div>
  <div class="jsontab"> <div class="jsonfield">"image_id"     </div><span>:</span>  <div class="jsonvalue">int,      </div> </div>
  <div class="jsontab"> <div class="jsonfield">"caption"      </div><span>:</span>  <div class="jsonvalue">str,      </div> </div>
  <div class="jsontabstartend"> }]</div>
</div>

<p class="titleSegoeLight" style="width:100%">3. Storing and Browsing Results</p>
<p class="bodyNormal" style="color:black;" align="justify">
Example result JSON files are available in <a href="https://github.com/pdollar/coco/tree/master/results" target="_blank">coco/results/</a> as part of the github package. Because the results format is similar to the ground truth annotation format, the CocoApi for accessing the ground truth can also be used to visualize and browse algorithm results. For details please see <span style="font-family: courier new;">evalDemo</span> (<a href="https://github.com/pdollar/coco/blob/master/PythonAPI/pycocoEvalDemo.ipynb" target="_blank">demo</a>) and also <span style="font-family: courier new;">loadRes()</span> in the CocoApi.
</p>
<br>
<br>
</div>
            <div role="tabpanel" class="tab-pane fade" id="detections-eval">    <!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">1. Detection Evaluation</p>
<p class="bodyNormal" style="color:black;" align="justify">
This page describes the <span style="font-style:italic">detection evaluation code</span> used by COCO. The evaluation code provided here can be used to obtain results on the publicly available COCO validation set. It computes multiple metrics described below. To obtain results on the COCO test set, for which ground truth annotations are hidden, generated results must be submitted to the <span style="font-style:italic">evaluation server</span>. For instructions on submitting results to the evaluation server please see the <a href="http://mscoco.org/dataset/#detections-upload">upload</a> page. The exact same evaluation code, described below, is used to evaluate detections on the test set.
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">2. Metrics</p>
<p class="bodyNormal" style="color:black;" align="justify">
The following 12 metrics are used for characterizing the performance of an object detector on COCO:
</p>
<div class="bodyNormal json" style="color:black; width:100%; font-family:courier new; ">
  <div style="display:inline-block; width:30%">
    <b>Average Precision (AP):</b><br>
    <div class="jsontab">AP<br></div>
    <div class="jsontab">AP<sup>IoU=.50</sup><br></div>
    <div class="jsontab">AP<sup>IoU=.75</sup><br></div>
    <b>AP Across Scales:</b><br>
    <div class="jsontab">AP<sup>small</sup><br></div>
    <div class="jsontab">AP<sup>medium</sup><br></div>
    <div class="jsontab">AP<sup>large</sup><br></div>
    <b>Average Recall (AR):</b><br>
    <div class="jsontab">AR<sup>max=1</sup><br></div>
    <div class="jsontab">AR<sup>max=10</sup><br></div>
    <div class="jsontab">AR<sup>max=100</sup><br></div>
    <b>AR Across Scales:</b><br>
    <div class="jsontab">AR<sup>small</sup><br></div>
    <div class="jsontab">AR<sup>medium</sup><br></div>
    <div class="jsontab">AR<sup>large</sup><br></div>
  </div>
  <div style="display:inline-block; width:74%; margin-left:-35px">
    <div class="jsontab">% AP at IoU=.50:.05:.95 <b>(primary challenge metric)</b></div>
    <div class="jsontab">% AP at IoU=.50 (PASCAL VOC metric)</div>
    <div class="jsontab">% AP at IoU=.75 (strict metric)</div>
    <br>
    <div class="jsontab">% AP for small objects: area &lt; 32<sup>2</sup></div>
    <div class="jsontab">% AP for medium objects: 32<sup>2</sup> &lt; area &lt; 96<sup>2</sup></div>
    <div class="jsontab">% AP for large objects: area &gt; 96<sup>2</sup></div>
    <br>
    <div class="jsontab">% AR given 1 detection per image </div>
    <div class="jsontab">% AR given 10 detections per image</div>
    <div class="jsontab">% AR given 100 detections per image</div>
    <br>
    <div class="jsontab">% AR for small objects: area &lt; 32<sup>2</sup></div>
    <div class="jsontab">% AR for medium objects: 32<sup>2</sup> &lt; area &lt; 96<sup>2</sup></div>
    <div class="jsontab">% AR for large objects: area &gt; 96<sup>2</sup></div>
  </div>
</div>
<ol class="bodyNormal" style="color:black;font-size:75%;margin-top:15px" align="justify">
  <li>Unless otherwise specified, <i>AP and AR are averaged over multiple Intersection over Union (IoU) values</i>. Specifically we use 10 IoU thresholds of .50:.05:.95. This is a break from tradition, where AP is computed at a single IoU of .50 (which corresponds to our metric AP<sup>IoU=.50</sup>). Averaging over IoUs rewards detectors with better localization.</li>
  <li>AP is averaged over all categories. Traditionally, this is called "mean average precision" (mAP). We make no distinction between AP and mAP (and likewise AR and mAR) and assume the difference is clear from context.</li>
  <li>AP (averaged across all 10 IoU thresholds and all 80 categories) will determine the challenge winner. This should be considered the single most important metric when considering performance on COCO.</li>
  <li>In COCO, there are more small objects than large objects. Specifically: approximately 41% of objects are small (area &lt; 32<sup>2</sup>), 34% are medium (32<sup>2</sup> &lt; area &lt; 96<sup>2</sup>), and 24% are large (area &gt; 96<sup>2</sup>). Area is measured as the number of pixels in the segmentation mask.</li>
  <li>AR is the maximum recall given a fixed number of detections per image, averaged over categories and IoUs. AR is related to the metric of the same name used in <a href="http://arxiv.org/abs/1502.05082" target="_blank">proposal evaluation</a> but is computed on a per-category basis.</li>
  <li>All metrics are computed allowing for at most 100 top-scoring detections per image (across all categories).</li>
  <li>The evaluation metrics for detection with bounding boxes and segmentation masks are identical in all respects except for the IoU computation (which is performed over boxes or masks, respectively).</li>
</ol>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">3. Results Format</p>
<p class="bodyNormal" style="color:black;" align="justify">
The results format used for storing generated detections is described on the <a href="http://mscoco.org/dataset/#format">results format</a> page. For reference, here is a summary of the detection results format for boxes and segments, respectively:
</p>
<div class="bodyNormal json" style="color:black;width:100%;">
  <div class="jsontabstartend"> [{</div>
  <div class="jsontab"> <div class="jsonfield">"image_id"     </div><span>:</span> <div class="jsonvalue">int,  </div></div>
  <div class="jsontab"> <div class="jsonfield">"category_id"  </div><span>:</span> <div class="jsonvalue">int,  </div></div>
  <div class="jsontab"> <div class="jsonfield">"bbox"         </div><span>:</span> <div class="jsonvalue">[x,y,width,height],</div></div>
  <div class="jsontab"> <div class="jsonfield">"score"        </div><span>:</span> <div class="jsonvalue">float,</div></div>
  <div class="jsontabstartend"> }]</div>
</div>
<p class="bodyNormal" style="color:black;" align="justify">
Note: box coordinates are floats measured from the top left image corner (and are 0-indexed). We recommend rounding coordinates to the nearest tenth of a pixel to reduce resulting JSON file size.
</p>

<div class="bodyNormal json" style="color:black;width:100%;margin-top:10px;">
  <div class="jsontabstartend"> [{</div>
  <div class="jsontab"> <div class="jsonfield">"image_id"     </div><span>:</span> <div class="jsonvalue">int,  </div></div>
  <div class="jsontab"> <div class="jsonfield">"category_id"  </div><span>:</span> <div class="jsonvalue">int,  </div></div>
  <div class="jsontab"> <div class="jsonfield">"segmentation" </div><span>:</span> <div class="jsonvalue">RLE,  </div></div>
  <div class="jsontab"> <div class="jsonfield">"score"        </div><span>:</span> <div class="jsonvalue">float,</div></div>
  <div class="jsontabstartend"> }]</div>
</div>
<p class="bodyNormal" style="color:black;" align="justify">
Note: binary masks should be encoded via RLE using the MaskApi function <span class="func_or_var">encode()</span>.
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">4. Evaluation Code</p>
<p class="bodyNormal" style="color:black;" align="justify">
Evaluation code is available on the <a href="https://github.com/pdollar/coco" target="_blank">COCO github</a>. Specifically, see either <a href="https://github.com/pdollar/coco/blob/master/MatlabAPI/CocoEval.m" target="_blank">CocoEval.m</a> or <a href="https://github.com/pdollar/coco/blob/master/PythonAPI/pycocotools/cocoeval.py" target="_blank">cocoeval.py</a> in the Matlab or Python code, respectively. Also see <span class="func_or_var">evalDemo</span> in either the Matlab or Python code (<a href="https://github.com/pdollar/coco/blob/master/PythonAPI/pycocoEvalDemo.ipynb" target="_blank">demo</a>).
</p>
<p class="bodyNormal" style="color:black;" align="justify">
The evaluation parameters are as follows (defaults in brackets, in general no need to change):
</p>
<div class="bodyNormal json" style="color:black;width:100%">
  <div class="jsontabstartend"> params{</div>
  <div class="jsontab"><div class="jsonfield">"imgIds"  </div><span>:</span> <div class="jsonvalue">[all] N img ids to use for evaluation</div></div>
  <div class="jsontab"><div class="jsonfield">"catIds"  </div><span>:</span> <div class="jsonvalue">[all] K cat ids to use for evaluation</div></div>
  <div class="jsontab"><div class="jsonfield">"iouThrs" </div><span>:</span> <div class="jsonvalue">[.5:.05:.95] T=10 IoU thresholds for evaluation</div></div>
  <div class="jsontab"><div class="jsonfield">"recThrs" </div><span>:</span> <div class="jsonvalue">[0:.01:1] R=101 recall thresholds for evaluation</div></div>
  <div class="jsontab"><div class="jsonfield">"areaRng" </div><span>:</span> <div class="jsonvalue">[all,small,medium,large] A=4 area ranges for evaluation</div></div>
  <div class="jsontab"><div class="jsonfield">"maxDets" </div><span>:</span> <div class="jsonvalue">[1 10 100] M=3 thresholds on max detections per image</div></div>
  <div class="jsontab"><div class="jsonfield">"useSegm" </div><span>:</span> <div class="jsonvalue">[1] if true evaluate against ground-truth segments</div></div>
  <div class="jsontab"><div class="jsonfield">"useCats" </div><span>:</span> <div class="jsonvalue">[1] if true use category labels for evaluation</div></div>
  <div class="jsontabstartend"> }</div>
</div>
<p class="bodyNormal" style="color:black;" align="justify">
Running the evaluation code via calls to <span class="func_or_var">evaluate()</span> and <span class="func_or_var">accumulate()</span> produces two data structures that measure detection quality. The two structs are <span class="func_or_var">evalImgs</span> and <span class="func_or_var">eval</span>, which measure quality per-image and aggregated across the entire dataset, respectively. The evalImgs struct has KxA entries, one per evaluation setting, while the eval struct combines this information into precision and recall arrays. Details for the two structs are below (see also <a href="https://github.com/pdollar/coco/blob/master/MatlabAPI/CocoEval.m" target="_blank">CocoEval.m</a> or <a href="https://github.com/pdollar/coco/blob/master/PythonAPI/pycocotools/cocoeval.py" target="_blank">cocoeval.py</a>):
</p>
<div class="bodyNormal json" style="color:black;width:100%">
  <div class="jsontabstartend"> evalImgs[{</div>
  <div class="jsontab"><div class="jsonfield">"dtIds"     </div><span>:</span> <div class="jsonvalue">[1xD] id for each of the D detections (dt)</div></div>
  <div class="jsontab"><div class="jsonfield">"gtIds"     </div><span>:</span> <div class="jsonvalue">[1xG] id for each of the G ground truths (gt)</div></div>
  <div class="jsontab"><div class="jsonfield">"dtImgIds"  </div><span>:</span> <div class="jsonvalue">[1xD] image id for each dt</div></div>
  <div class="jsontab"><div class="jsonfield">"gtImgIds"  </div><span>:</span> <div class="jsonvalue">[1xG] image id for each gt</div></div>
  <div class="jsontab"><div class="jsonfield">"dtMatches" </div><span>:</span> <div class="jsonvalue">[TxD] matching gt id at each IoU or 0</div></div>
  <div class="jsontab"><div class="jsonfield">"gtMatches" </div><span>:</span> <div class="jsonvalue">[TxG] matching dt id at each IoU or 0</div></div>
  <div class="jsontab"><div class="jsonfield">"dtScores"  </div><span>:</span> <div class="jsonvalue">[1xD] confidence of each dt</div></div>
  <div class="jsontab"><div class="jsonfield">"dtIgnore"  </div><span>:</span> <div class="jsonvalue">[TxD] ignore flag for each dt at each IoU</div></div>
  <div class="jsontab"><div class="jsonfield">"gtIgnore"  </div><span>:</span> <div class="jsonvalue">[1xG] ignore flag for each gt</div></div>
  <div class="jsontabstartend"> }]</div>
</div>
<div class="bodyNormal json" style="color:black;width:100%;margin-top:10px;">
  <div class="jsontabstartend"> eval{</div>
  <div class="jsontab"><div class="jsonfield">"params"    </div><span>:</span> <div class="jsonvalue">parameters used for evaluation</div></div>
  <div class="jsontab"><div class="jsonfield">"date"      </div><span>:</span> <div class="jsonvalue">date evaluation was performed</div></div>
  <div class="jsontab"><div class="jsonfield">"counts"    </div><span>:</span> <div class="jsonvalue">[T,R,K,A,M] parameter dimensions (see above)</div></div>
  <div class="jsontab"><div class="jsonfield">"precision" </div><span>:</span> <div class="jsonvalue">[TxRxKxAxM] precision for every evaluation setting</div></div>
  <div class="jsontab"><div class="jsonfield">"recall"    </div><span>:</span> <div class="jsonvalue">[TxKxAxM] max recall for every evaluation setting</div></div>
  <div class="jsontabstartend"> }</div>
</div>
<p class="bodyNormal" style="color:black;" align="justify">
Finally <span class="func_or_var">summarize()</span> computes the 12 detection metrics defined earlier based on the <span class="func_or_var">eval</span> struct.
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">5. Analysis Code</p>
<p class="bodyNormal" style="color:black;" align="justify">
In addition to the evaluation code, we also provide a function <span class="func_or_var">analyze()</span> for performing a detailed breakdown of false positives. This was inspired by <a href="http://web.engr.illinois.edu/~dhoiem/projects/detectionAnalysis/" target="_blank">Diagnosing Error in Object Detectors</a> by Derek Hoiem et al., but is quite different in implementation and details. The code generates plots like this:
</p>
<p class="bodyNormal" align="center" style="margin-top:20px">
<img src="./MSCOCO_Image_Captioning_Challenge_Result_Table_files/MSRA_analysis_person.jpg" style="width:40%; margin-right:20px" align="center">
<img src="./MSCOCO_Image_Captioning_Challenge_Result_Table_files/MSRA_analysis_all.jpg" style="width:42%" align="center">
</p>
<p class="bodyNormal" style="color:black;" align="justify">
Both plots show analysis of the <a href="http://arxiv.org/abs/1512.03385" target="_blank">ResNet</a> (bbox) detector from Kaiming He et al., winner of the <a href="http://mscoco.org/dataset/#detections-challenge2015">2015 Detection Challenge</a>. The first plot shows a breakdown of errors of ResNet for the person class; the second plot is an overall analysis of ResNet averaged over all categories.
</p><p class="bodyNormal" style="color:black;" align="justify">
Each plot is a series of precision recall curves where each PR curve is guaranteed to be strictly higher than the previous as the evaluation setting becomes more permissive. The curves are as follows:
</p>
<ol class="bodyNormal" style="color:black;font-size:75%;" align="justify">
  <li><b>C75</b>: PR at IoU=.75 (AP at strict IoU), area under curve corresponds to AP<sup>IoU=.75</sup> metric.</li>
  <li><b>C50</b>: PR at IoU=.50 (AP at PASCAL IoU), area under curve corresponds to AP<sup>IoU=.50</sup> metric.</li>
  <li><b>Loc</b>: PR at IoU=.10 (localization errors ignored, but not duplicate detections). All remaining settings use IoU=.1.</li>
  <li><b>Sim</b>: PR after supercategory false positives (fps) are removed. Specifically, any matches to objects with a different class label but that belong to the same supercategory don't count as either a fp (or tp). Sim is computed by setting all objects in the same supercategory to have the same class label as the class in question and setting their ignore flag to 1. Note that person is a singleton supercategory so its Sim result is identical to Loc.</li>
  <li><b>Oth</b>: PR after all class confusions are removed. Similar to Sim, except now if a detection matches <i>any</i> other object it is no longer a fp (or tp). Oth is computed by setting all other objects to have the same class label as the class in question and setting their ignore flag to 1.</li>
  <li><b>BG</b>: PR after all background (and class confusion) fps are removed. For a single category, BG is a step function that is 1 until max recall is reached then drops to 0 (the curve is smoother after averaging across categories).</li>
  <li><b>FN</b>: PR after all remaining errors are removed (trivially AP=1).</li>
</ol>
<p class="bodyNormal" style="color:black;" align="justify">
The area under each curve is shown in brackets in the legend. In the case of the ResNet detector, overall AP at IoU=.75 is .399 and perfect localization would increase AP to .682. Interesting, removing all class confusions (both within supercategory and across supercategories) would only raise AP slightly to .713. Removing background fp would bump performance to .870 AP and the rest of the errors are missing detections (although presumably if more detections were added this would also add lots of fps). In summary, ResNet's errors are dominated by imperfect localization and background confusions.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
For a given detector, the code generates a total of 372 plots! There are 80 categories, 12 supercategories, and 1 overall result, for a total of 93 different settings, and the analysis is performed at 4 scales (all, small, medium, large, so 93*4=372 plots). The file naming is [supercategory]-[category]-[size].pdf for the 80*4 per-category results, overall-[supercategory]-[size].pdf for the 12*4 per supercategory results, and overall-all-[size].pdf for the 1*4 overall results. Of all the plots, typically the overall and supercategory results are of the most interest.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
<b>Note:</b> <span class="func_or_var">analyze()</span> can take significant time to run, please be patient. As such, we typically do not run this code on the evaluation server; you must run the code locally using the validation set. Finally, currently <span class="func_or_var">analyze()</span> is only part of the Matlab API; <i>Python code coming soon</i>.
</p>
<br>
</div>
            <div role="tabpanel" class="tab-pane fade" id="detections-upload">  <!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">1. Detections Upload</p>
<p class="bodyNormal" style="color:black;" align="justify">
This page describes the <i>upload instructions</i> for submitting results to the detection <a href="https://www.codalab.org/competitions/5181" target="_blank">evaluation server</a>. Before proceeding, please review the <a href="http://mscoco.org/dataset/#format">results format</a> and <a href="http://mscoco.org/dataset/#detections-eval">evaluation details</a>. Submitting results allows you to participate in the <a href="http://mscoco.org/dataset/#detections-challenge2016">COCO Detection Challenge</a> and compare results to the state-of-the-art on the detection <a href="http://mscoco.org/dataset/#detections-leaderboard">leaderboard</a>.
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">2. Competition Details</p>
<p class="bodyNormal" style="color:black;" align="justify">
The COCO 2015 Test Set can be obtained on the <a href="http://mscoco.org/dataset/#download">download page.</a> The recommended training data consists of the COCO 2014 Training and Validation sets. External data of any form is allowed (except of course any form of annotation on the COCO Test set is forbidden). Please specify any and all external data used for training in the "method description" when uploading results to the evaluation server.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
There are two distinct detection challenges and associated leaderboards: for detectors that output bounding boxes and for detectors that output object segments. The bounding box challenge provides continuity with past challenges such as the PASCAL VOC; the detection by segmentation challenge encourages higher accuracy object localization. The evaluation code in the two cases computes IoU using boxes or segments, respectively, but is otherwise identical. Please see the <a href="http://mscoco.org/dataset/#detections-eval">evaluation details</a>.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
Please limit the number of entries to the evaluation server to a reasonable number, e.g. one entry per paper. To avoid overfitting, the <i>number of submissions per user is limited to 2 upload per day and a maximum of 5 submissions per user</i>. It is not acceptable to create multiple accounts for a single project to circumvent this limit. The exception to this is if a group publishes two papers describing unrelated methods, in this case both sets of results can be submitted for evaluation.
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="font-size:18pt">
2.1. Test Set Splits
</p>
<p class="bodyNormal" style="color:black;" align="justify">
The 2015 COCO Test set consists of ~80K test images. To limit overfitting while giving researchers more flexibility to test their system, we have divided the test set into four roughly equally sized splits of ~20K images each: <i>test-dev</i>, <i>test-standard</i>, <i>test-challenge</i>, and <i>test-reserve</i>. Submission to the test set automatically results in submission on each split (identities of the splits are not publicly revealed). In addition, to allow for debugging and validation experiments, we allow researcher <i>unlimited</i> submission to test-dev. Each test split serves a distinct role; details below.
</p>
<div class="bodyNormal json" style="color:black; width:100%; font-family:courier new; ">
  <table style="border-spacing: 5px;">
    <tbody><tr>
      <th style="padding-right: 30px">split</th>
      <th style="padding-right: 30px">#imgs</th>
      <th style="padding-right: 30px">submission</th>
      <th style="padding-right: 30px">scores reported</th>
    </tr>
    <tr>
      <td style="padding-right: 30px">Test-Dev</td>
      <td style="padding-right: 30px">~20K</td>
      <td style="padding-right: 30px">unlimited</td>
      <td style="padding-right: 30px">immediately</td>
    </tr>
    <tr>
      <td style="padding-right: 30px">Test-Standard</td>
      <td style="padding-right: 30px">~20K</td>
      <td style="padding-right: 30px">limited</td>
      <td style="padding-right: 30px">immediately</td>
    </tr>
    <tr>
      <td style="padding-right: 30px">Test-Challenge</td>
      <td style="padding-right: 30px">~20K</td>
      <td style="padding-right: 30px">limited</td>
      <td style="padding-right: 30px">challenge</td>
    </tr>
    <tr>
      <td style="padding-right: 30px">Test-Reserve</td>
      <td style="padding-right: 30px">~20K</td>
      <td style="padding-right: 30px">limited</td>
      <td style="padding-right: 30px">never</td>
    </tr>
  </tbody></table>
</div>
<p class="bodyNormal" style="color:black;" align="justify">
Test-Dev: We place <i>no limit</i> on the number of submissions allowed to test-dev. In fact, <i>we encourage use of the test-dev for performing validation experiments</i>. Use test-dev to debug and finalize your method before submitting to the full test set.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
Test-Standard: The test-standard split is the default test data for the detection competition. <i>When  comparing to the state of the art, results should be reported on test-standard</i>.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
Test-Challenge: The test-challenge split is used for the <a href="http://mscoco.org/dataset/#detections-challenge2016">COCO Detection Challenge</a>. Results will be revealed during the <a href="http://image-net.org/challenges/ilsvrc+coco2016" target="_blank">ImageNet and COCO Visual Recognition Challenges</a> Workshop.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
Test-Reserve: The test-reserve split is used to protect against possible overfitting. If there are substantial differences between a method's scores on test-standard and test-reserve this will raise a red-flag and prompt further investigation. Results on test-reserve will not be publicly revealed.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
We emphasize that except for test-dev, results <i>cannot</i> be submitted to a single split and must instead be submitted on the full test set. A submission to the test set populates three leaderboards: test-dev, test-standard and test-challenge (the updated test-challenge leaderboard will not be revealed until the ECCV 2016 Workshop). It is not possible to submit to test-standard without submitting to test-challenge or vice-versa (however, it is possible to submit to the test set without making results public, see  below). The identity of the images in each split is <i>not</i> revealed, except for test-dev.
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="font-size:18pt">
2.2. Test-Dev Best Practices
</p>
<p class="bodyNormal" style="color:black;" align="justify">
The test-dev 2015 set is a subset of the 2015 Testing set. The specific images belonging to test-dev are listed in the "image_info_test-dev2015.json" file available on the <a href="http://mscoco.org/dataset/#download">download page</a> as part of the "2015 Testing Image info" download. As discussed, we place <i>no limit</i> on the number of submissions allowed on test-dev. Note that while submitting to test-dev will produce evaluation results, doing so will not populate the public test-dev leaderboard. Instead, submitting to the full test set populates the test-dev leaderboard. This limits the number of results displayed on the test-dev leaderboard.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
Test-dev should be used only for validation and debugging: in a publication <i>it is not acceptable to report results on test-dev only</i>. However, for validation experiments it is acceptable to report results of competing methods on test-dev (obtained from the public test-dev leaderboard). While test-dev is prone to some overfitting, we expect this may still be useful in practice. We emphasize that final comparisons should always be performed on test-standard.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
The differences between the validation and test-dev sets are threefold: guaranteed consistent evaluation of test-dev using the evaluation server, test-dev cannot be used for training (annotations are private), and a leaderboard is provided for test-dev, allowing for comparison with the state-of-the-art. We note that the continued popularity of the outdated PASCAL VOC 2007 dataset partially stems from the fact that it allows for simultaneous validation experiments and comparisons to the state-of-the-art. Our goal with test-dev is to provide similar functionality (while keeping annotations private).
</p>


<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">3. Enter The Competition</p>
<p class="bodyNormal" style="color:black;" align="justify">
First you need to create an account on <a href="https://codalab.org/" target="_blank">CodaLab</a>. From your account you will be able to participate in all COCO challenges.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
Before uploading your results to the evaluation server, you will need to create a JSON file containing your results in the correct <a href="http://mscoco.org/dataset/#format">format</a>. The file should be named "detections_[testset]_[alg]_results.json". Replace [alg] with your algorithm name and [testset] with either "test-dev2015" or "test2015" depending on the test split you are using. Place the JSON file into a zip file named "detections_[testset]_[alg]_results.zip".
</p>
<p class="bodyNormal" style="color:black;" align="justify">
To submit your zipped result file to the COCO Detection Challenge click on the “Participate” tab on the CodaLab <a href="https://www.codalab.org/competitions/5181" target="_blank">evaluation server</a>. Select the challenge type (bbox or segm) and test split (test-dev or test). When you select “Submit / View Results” you will be given the option to submit new results. Please fill in the required fields and click “Submit”. A pop-up will prompt you to select the results zip file for upload. After the file is uploaded the evaluation server will begin processing. To view the status of your submission please select “Refresh Status”. Please be patient, the evaluation may take quite some time to complete (~20min on test-dev and ~80min on the full test set). If the status of your submission is “Failed” please check your file is named correctly and has the right <a href="http://mscoco.org/dataset/#format">format</a>.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
After you submit your results to the evaluation server, you can control whether your results are publicly posted to the CodaLab leaderboard. To toggle the public visibility of your results please select either “post to leaderboard” or “remove from leaderboard”. For now only one result can be published to the leaderboard at any time, we may change this in the future.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
In addition to the CodaLab leaderboard, we also host our own more detailed <a href="http://mscoco.org/dataset/#detections-leaderboard">leaderboard</a> that includes additional results and method information (such as paper references). Note that the CodaLab leaderboard may contain results not yet migrated to our own leaderboard.
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">4. Download Evaluation Results</p>
<p class="bodyNormal" style="color:black;" align="justify">
After evaluation is complete and the server shows a status of “Finished”, you will have the option to download your evaluation results by selecting “Download evaluation output from scoring step.” The zip file will contain three files:
</p>
<div class="bodyNormal json" style="color:black;width:100%; font-family: courier new; ">
  <div class="jsontabstartend">
    <div style="display:inline-block; width:52%">
      detections_[testset]_[alg]_eval.json<br>
      metadata<br>
      scores.txt
    </div>
    <div style="display:inline-block; width:45%;font-family:arial">
      % aggregated evaluation on test <br>
      % automatically generated (safe to ignore) <br>
      % automatically generated (safe to ignore)
    </div>
  </div>
</div>
<p class="bodyNormal" style="color:black;" align="justify">
The format of the eval file is described on the <a href="http://mscoco.org/dataset/#detections-eval">detection evaluation</a> page.
</p>
<br>
</div>
            <div role="tabpanel" class="tab-pane fade" id="keypoints-eval">     <!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">1. Keypoint Evaluation</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
<b>Note: Evaluation metrics were updated 09/05/2016. They are likely finalized, but are still subject to change if we discover any issues before the competition deadline. If you discover any flaws or pitfalls in the proposed metrics please contact us asap.</b>
</p>
<p class="bodyNormal" style="color:black;" align="justify">
This page describes the keypoint evaluation metric used by COCO. The COCO keypoint task requires simultaneously detecting objects and localizing their keypoints (object locations are not given at test time). As the task of <i>simultaneous detection and keypoint estimation</i> is relatively new, we chose to adopt a novel metric inspired by object detection metrics. For simplicity, we refer to this task as <i>keypoint detection</i> and the prediction algorithm as the <i>keypoint detector</i>.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
We suggest reviewing the evaluation metrics for <a href="http://mscoco.org/dataset/#detections-eval">object detection</a> before proceeding. As in the other COCO tasks, the evaluation code can be used to evaluate results on the publicly available validation set. To obtain results on the test set, for which ground truth annotations are hidden, generated results must be submitted to the evaluation server. For instructions on submitting results to the evaluation server please see the <a href="http://mscoco.org/dataset/#keypoints-upload">upload</a> page.
</p>

<!------------------------------------------------------------------------------------------------>
<p class="subtitleSegoeLight" style="font-size:18pt">1.1. Evaluation Overview</p>
<p class="bodyNormal" style="color:black;" align="justify">
The core idea behind evaluating keypoint detection is to mimic the evaluation metrics used for object detection, namely average precision (AP) and average recall (AR) and their variants. At the heart of these metrics is a similarity measure between ground truth objects and predicted objects. In the case of object detection, the IoU serves as this similarity measure (for both boxes and segments). Thesholding the IoU defines matches between the ground truth and predicted objects and allows computing precision-recall curves. To adopt AP/AR for keypoints detection, we thus only need to define an analogous similarity measure. We do so next by defining an <i>object keypoint similarity</i> (OKS) which plays the same role as the IoU.
</p>

<!------------------------------------------------------------------------------------------------>
<p class="subtitleSegoeLight" style="font-size:18pt">1.2. Object Keypoint Similarity</p>
<p class="bodyNormal" style="color:black;" align="justify">
For each object, ground truth keypoints have the form [x<sub>1</sub>,y<sub>1</sub>,v<sub>1</sub>,...,x<sub>k</sub>,y<sub>k</sub>,v<sub>k</sub>], where x,y are the keypoint locations and v is a visibility flag defined as v=0: not labeled, v=1: labeled but not visible, and v=2: labeled and visible. Each ground truth object also has a scale s which we define as the square root of the object segment area. For details on the ground truth format please see the <a href="http://mscoco.org/dataset/#download">download</a> page.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
For each object, the keypoint detector must output keypoint locations and an object-level confidence. Predicted keypoints for an object should have the same form as the ground truth: [x<sub>1</sub>,y<sub>1</sub>,v<sub>1</sub>,...,x<sub>k</sub>,y<sub>k</sub>,v<sub>k</sub>]. However, the detector's predicted v<sub>i</sub> are <i>not</i> currently used during evaluation, that is the keypoint detector is not required to predict per-keypoint visibilities or confidences.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
We define the object keypoint similarity (OKS) as:
</p>
<div class="bodyNormal json" style="color:black;width:100%; font-family: courier new; ">
  <div class="jsontabstartend">
    <div style="display:inline-block; width:100%; font-size:100%">
      <b>OKS = Σ<sub>i</sub>[exp(-d<sub>i</sub><sup>2</sup>/2s<sup>2</sup>κ<sub>i</sub><sup>2</sup>)δ(v<sub>i</sub>&gt;0)] / Σ<sub>i</sub>[δ(v<sub>i</sub>&gt;0)]</b>
    </div>
  </div>
</div>
<p class="bodyNormal" style="color:black;" align="justify">
The d<sub>i</sub> are the Euclidean distances between each corresponding ground truth and detected keypoint and the v<sub>i</sub> are the visibility flags of the ground truth (the detector's predicted v<sub>i</sub> are not used). To compute OKS, we pass the d<sub>i</sub> through an unnormalized Guassian with standard deviation sκ<sub>i</sub>, where s is the object scale and κ<sub>i</sub> is a per-keypont constant that controls falloff. For each keypoint this yields a keypoint <i>similarity</i> that ranges between 0 and 1. These similarities are averaged over all labeled keypoints (keypoints for which v<sub>i</sub>&gt;0). Predicted keypoints that are not labeled (v<sub>i</sub>=0) do not affect the OKS. Perfect predictions will have OKS=1 and predictions for which all keypoints are off by more than a few standard deviations sκ<sub>i</sub> will have OKS~0. The OKS is analogous to the IoU. Given the OKS, we can compute AP and AR just as the IoU allows us to compute these metrics for box/segment detection.
</p>

<!------------------------------------------------------------------------------------------------>
<p class="subtitleSegoeLight" style="font-size:18pt">
1.3. Tuning OKS
</p>
<p class="bodyNormal" style="color:black;" align="justify">
We tune the κ<sub>i</sub> such that the OKS is a perceptually meaningful and easy to interpret similarity measure. First, using 5000 redundantly annotated images in val, for each keypoint type i we measured the per-keypoint standard deviation σ<sub>i</sub> with respect to object scale s. That is we compute <b>σ<sub>i</sub><sup>2</sup>=E[d<sub>i</sub><sup>2</sup>/s<sup>2</sup>]</b>. σ<sub>i</sub> varies substantially for different keypoints: keypoints on a person's body (shoulders, knees, hips, etc.) tend to have a σ much larger than on a person's head (eyes, nose, ears).
</p>
<p class="bodyNormal" style="color:black;" align="justify">
To obtain a perceptually meaningful and interpretable similarity metric we set <b>κ<sub>i</sub>=2σ<sub>i</sub></b>. With this setting of κ<sub>i</sub>, at one, two, and three standard deviations of d<sub>i</sub>/s the keypoint similarity exp(-d<sub>i</sub><sup>2</sup>/2s<sup>2</sup>κ<sub>i</sub><sup>2</sup>) takes on values of e<sup>-1/8</sup>=.88, e<sup>-4/8</sup>=.61 and e<sup>-9/8</sup>=.32. As expected, human annotated keypoints are normally distributed (ignoring occasional outliers). Thus, recalling the <a href="https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule" target="_blank">68–95–99.7 </a>rule, setting κ<sub>i</sub>=2σ<sub>i</sub> means that 68%, 95%, and 99.7% of human annotated keypoints should have a keypoint similarity of .88, .61, or .32 or higher, respectively (in practice the percentages are 75%, 95% and 98.7%).
</p>
<p class="bodyNormal" style="color:black;" align="justify">
The OKS is the average keypoint similarity across all (labeled) object keypoints. Below we plot the predicted OKS distribution with κ<sub>i</sub>=2σ<sub>i</sub> assuming 10 independent keypoints per object (blue curve) and the actual distribution of human OKS scores on the dually annotated data (green curve):
</p>
<p class="bodyNormal" align="center">
<img src="./MSCOCO_Image_Captioning_Challenge_Result_Table_files/keypoints-oks-person.png" style="width:60%;" align="center">
</p>
<p class="bodyNormal" style="color:black;" align="justify">
The curves don't match exactly for a few reasons: (1) object keypoints are not independent, (2) the number of labeled keypoints per objects varies, and (3) the real data contains 1-2% outliers (most of which are caused by annotators mistaking left for right or annotating the wrong person when two people are nearby). Nevertheless, the behavior is roughly as expected. We conclude with a few observations about human performance: (1) at OKS of .50, human performance is nearly perfect (95%), (2) median human OKS is ~.91, (3) human performance drops rapidly after an OKS of .95. Note that this OKS distribution can be used to predict human AR (as AR doesn't depend on false positives).
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">2. Metrics</p>
<p class="bodyNormal" style="color:black;" align="justify">
The following 10 metrics are used for characterizing the performance of a keypoint detector on COCO:
</p>
<div class="bodyNormal json" style="color:black; width:100%; font-family:courier new; ">
  <div style="display:inline-block; width:30%">
    <b>Average Precision (AP):</b><br>
    <div class="jsontab">AP<br></div>
    <div class="jsontab">AP<sup>OKS=.50</sup><br></div>
    <div class="jsontab">AP<sup>OKS=.75</sup><br></div>
    <b>AP Across Scales:</b><br>
    <div class="jsontab">AP<sup>medium</sup><br></div>
    <div class="jsontab">AP<sup>large</sup><br></div>
    <b>Average Recall (AR):</b><br>
    <div class="jsontab">AR<br></div>
    <div class="jsontab">AR<sup>OKS=.50</sup><br></div>
    <div class="jsontab">AR<sup>OKS=.75</sup><br></div>
    <b>AR Across Scales:</b><br>
    <div class="jsontab">AR<sup>medium</sup><br></div>
    <div class="jsontab">AR<sup>large</sup><br></div>
  </div>
  <div style="display:inline-block; width:74%; margin-left:-35px">
    <div class="jsontab">% AP at OKS=.50:.05:.95 <b>(primary challenge metric)</b></div>
    <div class="jsontab">% AP at OKS=.50 (loose metric)</div>
    <div class="jsontab">% AP at OKS=.75 (strict metric)</div>
    <br>
    <div class="jsontab">% AP for medium objects: 32<sup>2</sup> &lt; area &lt; 96<sup>2</sup></div>
    <div class="jsontab">% AP for large objects: area &gt; 96<sup>2</sup></div>
    <br>
    <div class="jsontab">% AR at OKS=.50:.05:.95</div>
    <div class="jsontab">% AR at OKS=.50</div>
    <div class="jsontab">% AR at OKS=.75</div>
    <br>
    <div class="jsontab">% AR for medium objects: 32<sup>2</sup> &lt; area &lt; 96<sup>2</sup></div>
    <div class="jsontab">% AR for large objects: area &gt; 96<sup>2</sup></div>
  </div>
</div>
<ol class="bodyNormal" style="color:black;font-size:75%;margin-top:15px" align="justify">
  <li>Unless otherwise specified, AP and AR are averaged over multiple OKS values (.50:.05:.95).</li>
  <li>As discussed, we set κ<sub>i</sub>=2σ<sub>i</sub> for each keypoint type i. For people, the σ's are .026, .025, .035, .079, .072, .062, .107, .087, &amp; .089 for the nose, eyes, ears, shoulders, elbows, wrists, hips, knees, &amp; ankles, respectively.</li>
  <li>AP (averaged across all 10 OKS thresholds) will determine the challenge winner. This should be considered the single most important metric when considering keypoint performance on COCO.</li>
  <li>All metrics are computed allowing for at most 20 top-scoring detections per image (we use 20 detections, not 100 as in the object detection challenge, as currently person is the only category with keypoints).</li>
  <li>Small objects (segment area &lt; 32<sup>2</sup>) do not contain keypoint annotations.</li>
  <li>For objects without labeled keypoints, including crowds, we use a lenient heuristic that allows matching of detections based on hallucinated keypoints (placed within the ground truth objects so as to maximize OKS). This is very similar to how ignore regions are handled for detection with boxes/segments. See the code for details.</li>
  <li>Each object is given equal importance, regardless of the number of labeled/visible keypoints. We do not filter objects with only a few keypoints, nor do we weight object examples by the number of keypoints present.</li>
</ol>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">3. Results Format</p>
<p class="bodyNormal" style="color:black;" align="justify">
The results format used for storing generated keypoints is described on the <a href="http://mscoco.org/dataset/#format">results format</a> page. For reference, here is a summary of the keypoint results:
</p>
<div class="bodyNormal json" style="color:black;width:100%;">
  <div class="jsontabstartend"> [{</div>
  <div class="jsontab"> <div class="jsonfield">"image_id"     </div><span>:</span> <div class="jsonvalue">int,  </div></div>
  <div class="jsontab"> <div class="jsonfield">"category_id"  </div><span>:</span> <div class="jsonvalue">int,  </div></div>
  <div class="jsontab"> <div class="jsonfield">"keypoints"    </div><span>:</span> <div class="jsonvalue"> [x<sub>1</sub>,y<sub>1</sub>,v<sub>1</sub>,...,x<sub>k</sub>,y<sub>k</sub>,v<sub>k</sub>],</div></div>
  <div class="jsontab"> <div class="jsonfield">"score"        </div><span>:</span> <div class="jsonvalue">float,</div></div>
  <div class="jsontabstartend"> }]</div>
</div>
<p class="bodyNormal" style="color:black;" align="justify">
Note: keypoint coordinates are floats measured from the top left image corner (and are 0-indexed). We recommend rounding coordinates to the nearest pixel to reduce file size. Note also that the visibility flags v<sub>i</sub> are <i>not</i> currently used (except for controlling visualization), we recommend simply setting v<sub>i</sub>=1.
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">4. Evaluation Code</p>
<p class="bodyNormal" style="color:black;" align="justify">
</p><p class="bodyNormal" style="color:black;" align="justify">
Evaluation code is available on the <a href="https://github.com/pdollar/coco" target="_blank">COCO github</a>. Specifically, see either <a href="https://github.com/pdollar/coco/blob/master/MatlabAPI/CocoEval.m" target="_blank">CocoEval.m</a> or <a href="https://github.com/pdollar/coco/blob/master/PythonAPI/pycocotools/cocoeval.py" target="_blank">cocoeval.py</a> in the Matlab or Python code, respectively. Also see <span class="func_or_var">evalDemo</span> in either the Matlab or Python code (<a href="https://github.com/pdollar/coco/blob/master/PythonAPI/pycocoEvalDemo.ipynb" target="_blank">demo</a>).
</p>
<br>
</div>
            <div role="tabpanel" class="tab-pane fade" id="keypoints-upload">   <!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">1. Keypoints Upload</p>
<p class="bodyNormal" style="color:black;" align="justify">
This page describes the <i>upload instructions</i> for submitting results to the keypoint <a href="https://competitions.codalab.org/competitions/12061">evaluation server</a>. Before proceeding, please review the <a href="http://mscoco.org/dataset/#format">results format</a> and <a href="http://mscoco.org/dataset/#keypoints-eval">evaluation details</a>. Submitting results allows you to participate in the <a href="http://mscoco.org/dataset/#keypoints-challenge2016">COCO Keypoints Challenge</a> and compare results to the state-of-the-art on the keypoints <a href="http://mscoco.org/dataset/#keypoints-leaderboard">leaderboard</a>.
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">2. Competition Details</p>
<p class="bodyNormal" style="color:black;" align="justify">
The COCO 2015 Test Set can be obtained on the <a href="http://mscoco.org/dataset/#download">download page.</a> The recommended training data consists of the COCO 2014 Training and Validation sets. External data of any form is allowed (except of course any form of annotation on the COCO Test set is forbidden). Please specify any and all external data used for training in the "method description" when uploading results to the evaluation server.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
Please limit the number of entries to the evaluation server to a reasonable number, e.g. one entry per paper. To avoid overfitting, the <i>number of submissions per user is limited to 2 upload per day and a maximum of 5 submissions per user</i>. It is not acceptable to create multiple accounts for a single project to circumvent this limit. The exception to this is if a group publishes two papers describing unrelated methods, in this case both sets of results can be submitted for evaluation.
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="font-size:18pt">
2.1. Test Set Splits
</p>
<p class="bodyNormal" style="color:black;" align="justify">
The 2015 COCO Test set consists of ~80K test images. To limit overfitting while giving researchers more flexibility to test their system, we have divided the test set into four roughly equally sized splits of ~20K images each: <i>test-dev</i>, <i>test-standard</i>, <i>test-challenge</i>, and <i>test-reserve</i>. Submission to the test set automatically results in submission on each split (identities of the splits are not publicly revealed). In addition, to allow for debugging and validation experiments, we allow researcher <i>unlimited</i> submission to test-dev. Each test split serves a distinct role; details below.
</p>
<div class="bodyNormal json" style="color:black; width:100%; font-family:courier new; ">
  <table style="border-spacing: 5px;">
    <tbody><tr>
      <th style="padding-right: 30px">split</th>
      <th style="padding-right: 30px">#imgs</th>
      <th style="padding-right: 30px">submission</th>
      <th style="padding-right: 30px">scores reported</th>
    </tr>
    <tr>
      <td style="padding-right: 30px">Test-Dev</td>
      <td style="padding-right: 30px">~20K</td>
      <td style="padding-right: 30px">unlimited</td>
      <td style="padding-right: 30px">immediately</td>
    </tr>
    <tr>
      <td style="padding-right: 30px">Test-Standard</td>
      <td style="padding-right: 30px">~20K</td>
      <td style="padding-right: 30px">limited</td>
      <td style="padding-right: 30px">immediately</td>
    </tr>
    <tr>
      <td style="padding-right: 30px">Test-Challenge</td>
      <td style="padding-right: 30px">~20K</td>
      <td style="padding-right: 30px">limited</td>
      <td style="padding-right: 30px">challenge</td>
    </tr>
    <tr>
      <td style="padding-right: 30px">Test-Reserve</td>
      <td style="padding-right: 30px">~20K</td>
      <td style="padding-right: 30px">limited</td>
      <td style="padding-right: 30px">never</td>
    </tr>
  </tbody></table>
</div>
<p class="bodyNormal" style="color:black;" align="justify">
These are identical to the test splits used for the object detection challenge. To understand their role in more detail, and for best practices, please see the <a href="http://mscoco.org/dataset/#detections-upload">detection upload</a> page (section 2).
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">3. Enter The Competition</p>
<p class="bodyNormal" style="color:black;" align="justify">
First you need to create an account on <a href="https://codalab.org/" target="_blank">CodaLab</a>. From your account you will be able to participate in all COCO challenges.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
Before uploading your results to the evaluation server, you will need to create a JSON file containing your results in the correct <a href="http://mscoco.org/dataset/#format">format</a>. The file should be named "person_keypoints_[testset]_[alg]_results.json". Replace [alg] with your algorithm name and [testset] with either "test-dev2015" or "test2015" depending on the test split you are using. Place the JSON file into a zip file named "person_keypoints_[testset]_[alg]_results.zip".
</p>
<p class="bodyNormal" style="color:black;" align="justify">
To submit your zipped result file to the COCO Detection Challenge click on the “Participate” tab on the CodaLab <a href="https://www.codalab.org/competitions/12061" target="_blank">evaluation server</a>. Select test split (test-dev or test). When you select “Submit / View Results” you will be given the option to submit new results. Please fill in the required fields and click “Submit”. A pop-up will prompt you to select the results zip file for upload. After the file is uploaded the evaluation server will begin processing. To view the status of your submission please select “Refresh Status”. If the status of your submission is “Failed” please check your file is named correctly and has the right <a href="http://mscoco.org/dataset/#format">format</a>.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
After you submit your results to the evaluation server, you can control whether your results are publicly posted to the CodaLab leaderboard. To toggle the public visibility of your results please select either “post to leaderboard” or “remove from leaderboard”. For now only one result can be published to the leaderboard at any time, we may change this in the future.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
In addition to the CodaLab leaderboard, we also host our own more detailed <a href="http://mscoco.org/dataset/#keypoints-leaderboard">leaderboard</a> that includes additional results and method information (such as paper references). Note that the CodaLab leaderboard may contain results not yet migrated to our own leaderboard.
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">4. Download Evaluation Results</p>
<p class="bodyNormal" style="color:black;" align="justify">
After evaluation is complete and the server shows a status of “Finished”, you will have the option to download your evaluation results by selecting “Download evaluation output from scoring step.” The zip file will contain three files:
</p>
<div class="bodyNormal json" style="color:black;width:100%; font-family: courier new; ">
  <div class="jsontabstartend">
    <div style="display:inline-block; width:52%">
      eval.json<br>
      metadata<br>
      scores.txt
    </div>
    <div style="display:inline-block; width:45%;font-family:arial">
      % aggregated evaluation on test <br>
      % automatically generated (safe to ignore) <br>
      % automatically generated (safe to ignore)
    </div>
  </div>
</div>
<p class="bodyNormal" style="color:black;" align="justify">
The format of the eval file is described on the <a href="http://mscoco.org/dataset/#keypoints-eval">keypoints evaluation</a> page.
</p>
<br>
</div>
            <div role="tabpanel" class="tab-pane fade" id="captions-eval">      <!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">1. Caption Evaluation</p>
<p class="bodyNormal" style="color:black;" align="justify">
This page describes the <span style="font-style:italic">caption evaluation code</span> used by COCO. The evaluation code provided here can be used to obtain results on the publicly available COCO validation set. It computes multiple common metrics, including BLEU, METEOR, ROUGE-L, and CIDEr (the writeup below contains references and descriptions of each metric). If you use the captions, evaluation code, or server, we ask that you cite <a href="http://arxiv.org/abs/1504.00325" target="_blank">Microsoft COCO Captions: Data Collection and Evaluation Server</a>:</p>
<div class="bodyNormal json" style="font-size:8pt;color:black;width:100%">
  @article{capeval2015,
  <div style="padding-left:30px">
    Author={X. Chen and H. Fang and TY Lin and R. Vedantam and S. Gupta and P. Dollár and C. L. Zitnick}, <br>
    Journal = {arXiv:1504.00325}, <br>
    Title = {Microsoft COCO Captions: Data Collection and Evaluation Server}, <br>
    Year = {2015}
  </div>
  }
</div>
<p class="bodyNormal" style="color:black;" align="justify">
To obtain results on the COCO test set, for which ground truth annotations are hidden, generated results must be submitted to the <span style="font-style:italic">evaluation server</span>. For instructions on submitting results to the evaluation server please see the <a href="http://mscoco.org/dataset/#captions-upload">upload</a> page. The exact same evaluation code, described below, is used to evaluate generated captions on the test set.
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">2. Results Format</p>
<p class="bodyNormal" style="color:black;" align="justify">
The results format used for storing generated captions is described on the <a href="http://mscoco.org/dataset/#format">results format</a> page. For reference, here is a summary of the caption results format:
</p>
<div class="bodyNormal json" style="color:black;width:100%">
  <div class="jsontabstartend"> [{</div>
  <div class="jsontab"> <div class="jsonfield">"image_id"     </div><span>:</span>  <div class="jsonvalue">int,      </div> </div>
  <div class="jsontab"> <div class="jsonfield">"caption"      </div><span>:</span>  <div class="jsonvalue">str,      </div> </div>
  <div class="jsontabstartend"> }]</div>
</div>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">3. Evaluation Code</p>
<p class="bodyNormal" style="color:black;" align="justify">
Evaluation code can be obtained on the <a href="http://github.com/tylin/coco-caption" target="_blank">coco-captions github</a> page. Unlike the general COCO API, the COCO caption evaluation code is only available under Python.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
Running the evaluation code produces two data structures that summarize caption quality. The two structs are <span class="func_or_var">evalImgs</span> and <span class="func_or_var">eval</span>, which summarize caption quality per-image and aggregated across the entire test set, respectively. Details for the two data structures are given below. We recommend running the <a href="https://github.com/tylin/coco-caption/blob/master/cocoEvalCapDemo.ipynb" target="_blank">python caption evaluation demo</a> for more details.
</p>

<div class="bodyNormal json" style="color:black;width:100%">
  <div class="jsontabstartend"> evalImgs[{</div>
  <div class="jsontab"> <div class="jsonfield">"image_id"     </div><span>:</span>  <div class="jsonvalue">int,        </div> </div>
  <div class="jsontab"> <div class="jsonfield">"BLEU_1"       </div><span>:</span>  <div class="jsonvalue">float,      </div> </div>
  <div class="jsontab"> <div class="jsonfield">"BLEU_2"       </div><span>:</span>  <div class="jsonvalue">float,      </div> </div>
  <div class="jsontab"> <div class="jsonfield">"BLEU_3"       </div><span>:</span>  <div class="jsonvalue">float,      </div> </div>
  <div class="jsontab"> <div class="jsonfield">"BLEU_4"       </div><span>:</span>  <div class="jsonvalue">float,      </div> </div>
  <div class="jsontab"> <div class="jsonfield">"METEOR"       </div><span>:</span>  <div class="jsonvalue">float,      </div> </div>
  <div class="jsontab"> <div class="jsonfield">"ROUGE_L"      </div><span>:</span>  <div class="jsonvalue">float,      </div> </div>
  <div class="jsontab"> <div class="jsonfield">"CIDEr"        </div><span>:</span>  <div class="jsonvalue">float,      </div> </div>
  <div class="jsontabstartend"> }]</div>
</div>

<div class="bodyNormal json" style="color:black;width:100%;margin-top:10px; margin-bottom:40px">
  <div class="jsontabstartend"> eval{</div>
  <div class="jsontab"> <div class="jsonfield">"BLEU_1"       </div><span>:</span>  <div class="jsonvalue">float,      </div> </div>
  <div class="jsontab"> <div class="jsonfield">"BLEU_2"       </div><span>:</span>  <div class="jsonvalue">float,      </div> </div>
  <div class="jsontab"> <div class="jsonfield">"BLEU_3"       </div><span>:</span>  <div class="jsonvalue">float,      </div> </div>
  <div class="jsontab"> <div class="jsonfield">"BLEU_4"       </div><span>:</span>  <div class="jsonvalue">float,      </div> </div>
  <div class="jsontab"> <div class="jsonfield">"METEOR"       </div><span>:</span>  <div class="jsonvalue">float,      </div> </div>
  <div class="jsontab"> <div class="jsonfield">"ROUGE_L"      </div><span>:</span>  <div class="jsonvalue">float,      </div> </div>
  <div class="jsontab"> <div class="jsonfield">"CIDEr"        </div><span>:</span>  <div class="jsonvalue">float,      </div> </div>
  <div class="jsontabstartend"> }</div>
</div>
</div>
            <div role="tabpanel" class="tab-pane fade" id="captions-upload">    <!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">1. Captions Upload</p>
<p class="bodyNormal" style="color:black;" align="justify">
This page describes the <i>upload instructions</i> for submitting results to the caption <a href="https://www.codalab.org/competitions/3221" target="_blank">evaluation server</a>. Before proceeding, please review the <a href="http://mscoco.org/dataset/#format">results format</a> and <a href="http://mscoco.org/dataset/#captions-eval">evaluation details</a>. Submitting results allows you to participate in the <a href="http://mscoco.org/dataset/#captions-challenge2015">COCO Captioning Challenge 2015</a> and compare results to the state-of-the-art on the captioning <a href="http://mscoco.org/dataset/#captions-leaderboard">leaderboard</a>.
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">2. Competition Details</p>
<p class="bodyNormal" style="color:black;" align="justify">
<i>Training Data</i>: The recommended training set for the captioning challenge is the COCO 2014 Training Set. The COCO 2014 Validation Set may also be used for training when submitting results on the test set. External data of any form is allowed (except any form of annotation on the COCO Testing set is forbidden). Please specify any and all external data used for training in the "method description" when uploading results to the evaluation server.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
Please limit the number of entries to the captioning challenge to a reasonable number, e.g. one entry per paper. To avoid overfitting to the test data, the <i>number of submissions per user is limited to 1 upload per day and a maximum of 5 submissions per user</i>. It is not acceptable to create multiple accounts for a single project to circumvent this limit. The exception to this is if a group publishes two papers describing unrelated methods, in this case both sets of results can be submitted for evaluation.
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">3. Enter The Competition</p>
<p class="bodyNormal" style="color:black;" align="justify">
First you need to create an account on <a href="https://codalab.org/" target="_blank">CodaLab</a>. From your account you will be able to participate in all COCO challenges.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
Before uploading your results to the evaluation server, you will need to create two JSON files containing your captioning results in the correct <a href="http://mscoco.org/dataset/#format">results format</a>. One file should correspond to your results on the 2014 validation dataset, and the other to the 2014 test dataset. Both sets of results are required for submission. Your files should be named as follows:
</p><div class="bodyNormal json" style="color:black;width:100%; font-family: courier new; font-weight:500">
  <div class="jsontabstartend">
    <b>results.zip</b><br>
    &nbsp; captions_val2014_[alg]_results.json<br>
    &nbsp; captions_test2014_[alg]_results.json
  </div>
</div>
<p class="bodyNormal" style="color:black;" align="justify">
Replace [alg] with your algorithm name and place both files into a single zip file named "results.zip". </p>
<p class="bodyNormal" style="color:black;" align="justify">
To submit your zipped result file to the <a href="https://www.codalab.org/competitions/3221" target="_blank">COCO Captioning Challenge</a> click on the “Participate” tab on the CodaLab webpage. When you select “Submit / View Results” you will be given the option to submit new results. Please fill in the required fields and click “Submit”. A pop-up will prompt you to select the results zip file for upload. After the file is uploaded the evaluation server will begin processing. To view the status of your submission please select “Refresh Status”. Please be patient, the evaluation may take quite some time to complete. If the status of your submission is “Failed” please check to make sure your files are named correctly, they have the right <a href="http://mscoco.org/dataset/#format">format</a>, and your zip file contains two files corresponding to the validation and testing datasets.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
After you submit your results to the evaluation server, you can control whether your results are publicly posted to the CodaLab leaderboard. To toggle the public visibility of your results please select either “post to leaderboard” or “remove from leaderboard”. For now only one result can be published to the leaderboard at any time, we may change this in the future. After your results are posted to the CodaLab leaderboard, your captions on the validation dataset will be publicly available. Your captions on the test set will not be publicly released.
</p>
<p class="bodyNormal" style="color:black;" align="justify">
In addition to the <a href="https://www.codalab.org/competitions/3221#results" target="_blank">CodaLab leaderboard</a>, we also host our own more detailed <a href="http://mscoco.org/dataset/#captions-leaderboard">leaderboard</a> that includes additional results and method information (such as paper references). Note that the CodaLab leaderboard may contain results not yet migrated to our own leaderboard.
</p>

<!------------------------------------------------------------------------------------------------>
<p class="titleSegoeLight" style="width:100%">4. Download Evaluation Results</p>
<p class="bodyNormal" style="color:black;" align="justify">
After evaluation is complete and the server shows a status of “Finished”, you will have the option to download your evaluation results by selecting “Download evaluation output from scoring step.” The zip file will contain five files:
</p>
<div class="bodyNormal json" style="color:black;width:100%; font-family: courier new; ">
  <div class="jsontabstartend">
    <div style="display:inline-block; width:52%">
      captions_val2014_[alg]_evalimgs.json<br>
      captions_val2014_[alg]_eval.json<br>
      captions_test2014_[alg]_eval.json<br>
      metadata<br>
      scores.txt
    </div>
    <div style="display:inline-block; width:45%;font-family:arial">
      % per image evaluation on val <br>
      % aggregated evaluation on val <br>
      % aggregated evaluation on test <br>
      % automatically generated (safe to ignore) <br>
      % automatically generated (safe to ignore)
    </div>
  </div>
</div>
<p class="bodyNormal" style="color:black;" align="justify">
The format of the evaluation files is described on the <a href="http://mscoco.org/dataset/#captions-eval">caption evaluation</a> page. Please note that the *_evalImgs.json file is only available for download on the validation dataset, and not the test set.
</p>
<br>
</div>
            <div role="tabpanel" class="tab-pane fade" id="captions-challenge2015">     <!------------------------------------------------------------------------------------------------>
<p align="center" class="titleSegoeLight" style="width:100%">Welcome to the COCO Captioning Challenge!<br>
<span class="titleSegoeLight" style="font-style: italic;width:80%">Winners were announced at CVPR 2015</span><br>
<span class="titleSegoeLight" style="font-style: italic;width:80%">Caption evaluation server remains open!</span></p>
<p align="center"><img src="./MSCOCO_Image_Captioning_Challenge_Result_Table_files/captions-challenge2015.jpg" style="width:80%"></p>

<!------------------------------------------------------------------------------------------------>
<p align="justify" class="titleSegoeLight" style="width:100%">1. Introduction</p>
<p align="justify" class="bodyNormal" style="color:black">
<b>Update:</b> The COCO caption <a href="https://www.codalab.org/competitions/3221" target="_blank">evaluation server</a> remains open. Please submit new results to compare to state-of-the-art methods using several automatic evaluation metrics. The COCO 2015 Captioning Challenge is now, however, complete. Results were presented as part of the CVPR 2015 <a href="http://lsun.cs.princeton.edu/" target="_blank">Large-scale Scene Understanding (LSUN)</a> workshop and are available to view on the <a href="http://mscoco.org/dataset/#captions-leaderboard">leaderboard</a>.
</p>
<p align="justify" class="bodyNormal" style="color:black">
The COCO Captioning Challenge is designed to spur the development of algorithms producing image captions that are informative and accurate. Teams will be competing by training their algorithms on the COCO 2014 dataset and having their results scored by <b>human judges</b>.
</p>

<!------------------------------------------------------------------------------------------------>
<p align="justify" class="titleSegoeLight" style="width:100%">2. Dates</p>
<div class="json" style="width:99%;margin-bottom:20px">
  <div style="width:20%;display:inline-block">April 1, 2015</div> <div style="width:75%;display:inline-block">Training and testing data, and evaluation software released</div>
  <div style="width:20%;display:inline-block">May  29, 2015</div> <div style="width:75%;display:inline-block">Submission deadline at 11:59 PST</div>
  <div style="width:20%;display:inline-block">June  5, 2015</div> <div style="width:75%;display:inline-block">Challenge results (human judgment) released</div>
  <div style="width:20%;display:inline-block">June 12, 2015</div> <div style="width:75%;display:inline-block">Winner presents at the LSUN Workshop at CVPR 2015</div>
</div>
<p align="justify" class="bodyNormal" style="color:black">
This captioning challenge is part of the <a href="http://lsun.cs.princeton.edu/" target="_blank">Large-scale Scene Understanding (LSUN)</a> CVPR 2015 workshop organized by Princeton University. For further details please visit the LSUN website.
</p>

<!------------------------------------------------------------------------------------------------>
<p align="justify" class="titleSegoeLight" style="width:100%">3. Organizers</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
Yin Cui (Cornell)<br>
Matteo Ruggero Ronchi (Caltech)<br>
Tsung-Yi Lin (Cornell)<br>
Piotr Dollár (Facebook AI Research)<br>
Larry Zitnick (Microsoft Research)
</p>

<!------------------------------------------------------------------------------------------------>
<p align="justify" class="titleSegoeLight" style="width:500px">4. Challenge Guidelines</p>
<p align="justify" class="bodyNormal" style="color:black">
Participants are recommended but not restricted to train their algorithms on COCO 2014 dataset. The results should contain a single caption for each validation and test image and they must be submitted and publicly published on the <a href="https://www.codalab.org/competitions/3221" target="_blank">CodaLab</a> leaderboard. Please specify any and all external data used for training in the "method description" when uploading results to the evaluation server.
</p>
<p align="justify" class="bodyNormal" style="color:black">
By the challenge deadline, both results on the validation and test sets must be submitted to the evaluation server. The results on validation will be public and used for performance diagnosis and visualization. The competitors' algorithms will be evaluated based on the feedback from <b>human judges</b> and the top performing teams will be awarded prizes. Two or three teams will also be invited to present at the LSUN workshop.
</p>
<p align="justify" class="bodyNormal" style="color:black">
Please follow the instructions in the <a href="http://mscoco.org/dataset/#format">format</a>, <a href="http://mscoco.org/dataset/#captions-eval">evaluate</a>, and <a href="http://mscoco.org/dataset/#captions-upload">upload</a> tabs which describe the results format, evaluation code, and upload instructions, resprecitvely. The <a href="https://github.com/tylin/coco-caption" target="_blank">COCO Caption Evaluation Toolkit</a> is also available. The tooklit provides evaluation code for common metrics for caption analysis, including the BLEU, METEOR, ROUGE-L, and CIDEr metrics. Note that for the competition, instead of automated metrics, human judges will evaluate algorithm results.
</p>

<p><br></p>
</div>
            <div role="tabpanel" class="tab-pane fade" id="detections-challenge2015">   <!------------------------------------------------------------------------------------------------>
<p align="center" class="titleSegoeLight" style="width:100%">Welcome to the COCO 2015 Detection Challenge!<br><br>
<span class="titleSegoeLight" style="width:100%">1st Place Detection and Segmentation: Team MSRA</span><br>
<span class="titleSegoeLight" style="width:100%">2nd Place Detection and Segmentation: Team FAIRCNN</span><br>
<span class="titleSegoeLight" style="width:100%">Best Student Entry: Team ION</span></p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
Detection results and winners' methods were presented at the ICCV 2015 <a href="http://image-net.org/challenges/ilsvrc+mscoco2015" target="_blank">ImageNet and COCO Visual Recognition Challenges Joint Workshop</a> (slides and recording of all talks are now available). Challenge winners along with up-to-date results are available to view on the <a href="http://mscoco.org/dataset/#detections-leaderboard">leaderboard</a>. The evaluation server remains open for <a href="http://mscoco.org/dataset/#detections-upload">upload</a> of new results.
</p>
<p align="center"><img src="./MSCOCO_Image_Captioning_Challenge_Result_Table_files/detections-challenge2015.png" style="width:100%"></p>

<!------------------------------------------------------------------------------------------------>
<p align="justify" class="titleSegoeLight" style="width:100%">1. Overview</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
We are pleased to announce the COCO 2015 Detection Challenge. This competition is designed to push the state of the art in object detection forward. Teams are encouraged to compete in either (or both) of two object detection challenges: using bounding box output or object segmentation output.
</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
The COCO train, validation, and test sets, containing more than 200,000 images and 80 object categories, are available on the <a href="http://mscoco.org/dataset/#download">download</a> page. All object instance are annotated with a detailed segmentation mask. Annotations on the training and validation sets (with over 500,000 object instances segmented) are publicly available.
</p>

<!------------------------------------------------------------------------------------------------>
<p align="justify" class="titleSegoeLight" style="width:100%">2. Dates</p>
<div class="json" style="width:99%;margin-bottom:20px">
  <div style="width:30%;display:inline-block">July 23, 2015</div> <div style="width:65%;display:inline-block">Training and testing data, and evaluation software released</div>
  <div style="color:blue;width:30%;display:inline-block">November 25, 2015</div> <div style="width:65%;display:inline-block">Extended submission deadline (11:59 PST)</div>
  <div style="width:30%;display:inline-block">December 10, 2015</div> <div style="width:65%;display:inline-block">Challenge results released</div>
  <div style="width:30%;display:inline-block">December 17, 2015</div> <div style="width:65%;display:inline-block">Winner presents at ICCV 2015 Workshop</div>
</div>
<div align="justify">
  This detection challenge is part of the <a href="http://image-net.org/challenges/ilsvrc+mscoco2015" target="_blank">ImageNet and COCO Visual Recognition Challenges</a> joint workshop at ICCV 2015. For further details about the joint workshop please visit the workshop website.
</div>

<!------------------------------------------------------------------------------------------------>
<p align="justify" class="titleSegoeLight" style="width:100%">3. Organizers</p>
<div class="bodyNormal" style="color:black;font-size:90%">Tsung-Yi Lin (Cornell)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Yin Cui (Cornell)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Genevieve Patterson (Brown)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Matteo Ruggero Ronchi (Caltech)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Larry Zitnick (Microsoft Research)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Piotr Dollár (Facebook AI Research)</div>

<!------------------------------------------------------------------------------------------------>
<p align="justify" class="titleSegoeLight" style="width:100%">4. Award Committee</p>
<div class="bodyNormal" style="color:black;font-size:90%">Yin Cui (Cornell)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Genevieve Patterson (Brown)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Matteo Ruggero Ronchi (Caltech)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Serge Belongie (Cornell)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Lubomir Bourdev (UC Berkeley)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Michael Maire (TTI Chicago)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Pietro Perona (Caltech)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Deva Ramanan (CMU)</div>

<!------------------------------------------------------------------------------------------------>
<p align="justify" class="titleSegoeLight" style="width:500px">5. Challenge Guidelines</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
The <a href="http://mscoco.org/dataset/#detections-eval">detection evaluation</a> page lists detailed information regarding how submissions will be scored. Instructions for submitting results are available on the <a href="http://mscoco.org/dataset/#detections-upload">detection upload</a> page.
</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
To limit overfitting while giving researchers more flexibility to test their system, we have divided the test set into a number of splits, including test-dev, test-standard, and test-challenge. Test-dev is used for debugging and validation experiments and allows for unlimited submission to the evaluation server. Test-standard is used to maintain a public <a href="http://mscoco.org/dataset/#detections-leaderboard">leaderboard</a> that is updated upon submission. Finally, test-challenge is used for the workshop competition; results will be revealed during the workshop at ICCV 2015. A more thorough explanation is available on the <a href="http://mscoco.org/dataset/#detections-upload">upload</a> page.
</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
Competitors are recommended but not restricted to train their algorithms on COCO 2014 train and val sets. The <a href="http://mscoco.org/dataset/#download">download</a> page contains links to all COCO 2014 train+val images and associated annotations as well as the 2015 test images.  Please specify any and all external data used for training in the "method description" when uploading results to the evaluation server.
</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
By the challenge deadline, results must be submitted to the evaluation server. Competitors' algorithms will be evaluated according to the rules described on the <a href="http://mscoco.org/dataset/#detections-eval">evaluation</a> page. Challenge participants with the most successful and innovative methods will be invited to present.
</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
After careful consideration, this challenge uses a more comprehensive comparison metric than the traditional AP at Intersection over Union (IoU) threshold of 0.5. Specifically, AP is averaged over multiple IoU values between 0.5 and 1.0; this rewards detectors with better localization. Please refer to the "Metrics" section of the <a href="http://mscoco.org/dataset/#detections-eval">evaluation</a> page for a detailed explanation of the competition metrics.
</p>

<!------------------------------------------------------------------------------------------------>
<p align="justify" class="titleSegoeLight" style="width:500px">6. Tools and Instructions</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
We provide extensive API support for the COCO images, annotations, and evaluation code. To download the COCO API, please visit our <a href="https://github.com/pdollar/coco">GitHub repository</a>. For an overview of how to use the API, please visit the <a href="http://mscoco.org/dataset/#download">download</a> page and consult the sections entitled COCO API and MASK API.
</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
Due to the large size of the COCO dataset and the complexity of this challenge, the process of competing in this challenge may not seem simple. To help guide competitors to victory, we provide explanations and instructions for each step of the process on the  <a href="http://mscoco.org/dataset/download">download</a>, <a href="http://mscoco.org/dataset/#format">format</a>, <a href="http://mscoco.org/dataset/#detections-eval">evaluation</a>, and <a href="http://mscoco.org/dataset/#detections-upload">upload</a> pages. For additional questions, please contact <a href="mailto:cocodataset@outlook.com">cocodataset@outlook.com</a>.
</p>
<br>
<br>
</div>
            <div role="tabpanel" class="tab-pane fade" id="detections-challenge2016">   <!------------------------------------------------------------------------------------------------>
<p align="center" class="titleSegoeLight" style="width:100%">Welcome to the COCO 2016 Detection Challenge!</p>
<p align="center"><img src="./MSCOCO_Image_Captioning_Challenge_Result_Table_files/detections-challenge2015.png" style="width:100%"></p>

<!------------------------------------------------------------------------------------------------>
<p align="justify" class="titleSegoeLight" style="width:100%">1. Overview</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
The COCO 2016 Detection Challenge is designed to push the state of the art in object detection forward. Teams are encouraged to compete in either (or both) of two object detection challenges: using bounding box output or object segmentation output.
</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
This challenge is part of the <a href="http://image-net.org/challenges/ilsvrc+coco2016">ImageNet and COCO Visual Recognition</a> workshop at ECCV 2016. For further details about the joint workshop please visit the workshop website. Participants are encouraged to participate in both the COCO and ImageNet detection challenges. Please also see the concurrent <a href="http://mscoco.org/dataset/#keypoints-challenge2016">COCO 2016 Keypoint Challenge</a>.
</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
The COCO train, validation, and test sets, containing more than 200,000 images and 80 object categories, are available on the <a href="http://mscoco.org/dataset/#download">download</a> page. All object instances are annotated with a detailed segmentation mask. Annotations on the training and validation sets (with over 500,000 object instances segmented) are publicly available.
</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
This is the second COCO detection challenge and it closely follows the <a href="http://mscoco.org/dataset/#detections-challenge2015">COCO 2015 Detection Challenge</a>. In particular, the same data and metrics are being used for this year's challenge.
</p>

<!------------------------------------------------------------------------------------------------>
<p align="justify" class="titleSegoeLight" style="width:100%">2. Dates</p>
<div class="json" style="width:99%;margin-bottom:20px">
  <div style="width:30%;display:inline-block">June 1, 2016</div> <div style="width:65%;display:inline-block">Challenge officially announced</div>
  <div style="color:blue;width:30%;display:inline-block">September 16, 2016</div> <div style="width:65%;display:inline-block">[Extended] Submission deadline (11:59 PST)</div>
  <div style="width:30%;display:inline-block">October 2, 2016</div> <div style="width:65%;display:inline-block">Challenge results released</div>
  <div style="width:30%;display:inline-block">October 9, 2016</div> <div style="width:65%;display:inline-block">Winners present at ECCV 2016 Workshop</div>
</div>

<!------------------------------------------------------------------------------------------------>
<p align="justify" class="titleSegoeLight" style="width:100%">3. Organizers</p>
<div class="bodyNormal" style="color:black;font-size:90%">Tsung-Yi Lin (Cornell)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Yin Cui (Cornell)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Genevieve Patterson (Brown)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Matteo Ruggero Ronchi (Caltech)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Ross Girshick (Facebook AI Research)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Piotr Dollár (Facebook AI Research)</div>

<!------------------------------------------------------------------------------------------------>
<p align="justify" class="titleSegoeLight" style="width:100%">4. Award Committee</p>
<div class="bodyNormal" style="color:black;font-size:90%">Yin Cui (Cornell)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Genevieve Patterson (Brown)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Matteo Ruggero Ronchi (Caltech)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Serge Belongie (Cornell)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Lubomir Bourdev (UC Berkeley)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Michael Maire (TTI Chicago)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Pietro Perona (Caltech)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Deva Ramanan (CMU)</div>

<!------------------------------------------------------------------------------------------------>
<p align="justify" class="titleSegoeLight" style="width:500px">5. Challenge Guidelines</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
The <a href="http://mscoco.org/dataset/#detections-eval">detection evaluation</a> page lists detailed information regarding how submissions will be scored. Instructions for submitting results are available on the <a href="http://mscoco.org/dataset/#detections-upload">detection upload</a> page.
</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
To limit overfitting while giving researchers more flexibility to test their system, we have divided the test set into a number of splits, including test-dev, test-standard, and test-challenge. Test-dev is used for debugging and validation experiments and allows for unlimited submission to the evaluation server. Test-standard is used to maintain a public <a href="http://mscoco.org/dataset/#detections-leaderboard">leaderboard</a> that is updated upon submission. Finally, test-challenge is used for the workshop competition; results will be revealed during the workshop at ECCV 2016. A more thorough explanation is available on the <a href="http://mscoco.org/dataset/#detections-upload">upload</a> page.
</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
Competitors are recommended but not restricted to train their algorithms on COCO 2014 train and val sets. The <a href="http://mscoco.org/dataset/#download">download</a> page contains links to all COCO 2014 train+val images and associated annotations as well as the 2015 test images.  Please specify any and all external data used for training in the "method description" when uploading results to the evaluation server.
</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
By the challenge deadline, results must be submitted to the evaluation server. Competitors' algorithms will be evaluated according to the rules described on the <a href="http://mscoco.org/dataset/#detections-eval">evaluation</a> page. Challenge participants with the most successful and innovative methods will be invited to present.
</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
After careful consideration, this challenge uses a more comprehensive comparison metric than the traditional AP at Intersection over Union (IoU) threshold of 0.5. Specifically, AP is averaged over multiple IoU values between 0.5 and 1.0; this rewards detectors with better localization. Please refer to the "Metrics" section of the <a href="http://mscoco.org/dataset/#detections-eval">evaluation</a> page for a detailed explanation of the competition metrics.
</p>

<!------------------------------------------------------------------------------------------------>
<p align="justify" class="titleSegoeLight" style="width:500px">6. Tools and Instructions</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
We provide extensive API support for the COCO images, annotations, and evaluation code. To download the COCO API, please visit our <a href="https://github.com/pdollar/coco">GitHub repository</a>. For an overview of how to use the API, please visit the <a href="http://mscoco.org/dataset/#download">download</a> page and consult the sections entitled COCO API and MASK API.
</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
Due to the large size of the COCO dataset and the complexity of this challenge, the process of competing in this challenge may not seem simple. To help, we provide explanations and instructions for each step of the process on the <a href="http://mscoco.org/dataset/#download">download</a>, <a href="http://mscoco.org/dataset/#format">format</a>, <a href="http://mscoco.org/dataset/#detections-eval">evaluation</a>, and <a href="http://mscoco.org/dataset/#detections-upload">upload</a> pages. For additional questions, please contact <a href="mailto:cocodataset@outlook.com">cocodataset@outlook.com</a>.
</p>
<br>
<br>
</div>
            <div role="tabpanel" class="tab-pane fade" id="keypoints-challenge2016">    <!------------------------------------------------------------------------------------------------>
<p align="center" class="titleSegoeLight" style="width:100%">Welcome to the COCO 2016 Keypoint Challenge!</p>
<p align="center"><a href="http://mscoco.org/static/images/keypoints-challenge2016-big.png">
  <img src="./MSCOCO_Image_Captioning_Challenge_Result_Table_files/keypoints-challenge2016.png" style="width:100%"></a></p>

<!------------------------------------------------------------------------------------------------>
<p align="justify" class="titleSegoeLight" style="width:100%">1. Overview</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
<b>Deadline has been extended to 09/16. We apologize for the delay of releasing evaluation code. The <a href="http://mscoco.org/dataset/#keypoints-eval">keypoint evaluation metrics</a> is finalized and the <a href="http://competitions.codalab.org/competitions/12061">keypoint evaluation server</a> is open for test-dev evaluation. The full test set evaluation will open shortly.  Thank you for your patience!</b>
</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
The COCO 2016 Keypoint Challenge requires localization of person keypoints in challenging, uncontrolled conditions. The keypoint challenge involves simultaneously detecting people <i>and</i> localizing their keypoints (person locations are <i>not</i> given at test time). For full details of this task please see the <a href="http://mscoco.org/dataset/#keypoints-eval">keypoint evaluation</a> page.
</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
This challenge is part of the <a href="http://image-net.org/challenges/ilsvrc+coco2016">ImageNet and COCO Visual Recognition</a> workshop at ECCV 2016. For further details about the joint workshop please visit the workshop website. Please also see the concurrent <a href="http://mscoco.org/dataset/#detections-challenge2016">COCO 2016 Detection Challenge</a>.
</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
Training and val data have now been released. The training set for this task consists of over 100K person instances labeled with keypoints (the majority of people in COCO at medium and large scales) and over 1 million total labeled keypoints. The val set has an addtional 50K annotated people.
</p>
<p></p>

<!------------------------------------------------------------------------------------------------>
<p align="justify" class="titleSegoeLight" style="width:100%">2. Dates</p>
<div class="json" style="width:99%;margin-bottom:20px">
  <div style="width:30%;display:inline-block">June 1, 2016</div> <div style="width:65%;display:inline-block">Challenge officially announced</div>
  <div style="color:blue;width:30%;display:inline-block">September 9, 2016</div> <div style="width:65%;display:inline-block">Evaluation server opens</div>
  <div style="color:blue;width:30%;display:inline-block">September 16, 2016</div> <div style="width:65%;display:inline-block">[Extended] Submission deadline (11:59 PST)</div>
  <div style="width:30%;display:inline-block">October 2, 2016</div> <div style="width:65%;display:inline-block">Challenge results released</div>
  <div style="width:30%;display:inline-block">October 9, 2016</div> <div style="width:65%;display:inline-block">Winners present at ECCV 2016 Workshop</div>
</div>

<!------------------------------------------------------------------------------------------------>
<p align="justify" class="titleSegoeLight" style="width:100%">3. Organizers</p>
<div class="bodyNormal" style="color:black;font-size:90%">Tsung-Yi Lin (Cornell)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Yin Cui (Cornell)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Genevieve Patterson (Brown)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Matteo Ruggero Ronchi (Caltech)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Lubomir Bourdev (UC Berkeley)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Ross Girshick (Facebook AI Research)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Piotr Dollár (Facebook AI Research)</div>

<!------------------------------------------------------------------------------------------------>
<p align="justify" class="titleSegoeLight" style="width:100%">4. Award Committee</p>
<div class="bodyNormal" style="color:black;font-size:90%">Yin Cui (Cornell)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Genevieve Patterson (Brown)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Matteo Ruggero Ronchi (Caltech)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Serge Belongie (Cornell)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Lubomir Bourdev (UC Berkeley)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Michael Maire (TTI Chicago)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Pietro Perona (Caltech)</div>
<div class="bodyNormal" style="color:black;font-size:90%">Deva Ramanan (CMU)</div>

<!------------------------------------------------------------------------------------------------>
<p align="justify" class="titleSegoeLight" style="width:500px">5. Challenge Guidelines</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
The <a href="http://mscoco.org/dataset/#keypoints-eval">keypoint evaluation</a> page lists detailed information regarding how submissions will be scored. Instructions for submitting results are available on the <a href="http://mscoco.org/dataset/#keypoints-upload">keypoint upload</a> page. Note that the keypoint challenge follows the <a href="http://mscoco.org/dataset/#detections-challenge2016">detection challenge</a> quite closely. Specifically, the same challenge rules apply and the same COCO images sets are used. Details follow below.
</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
To limit overfitting while giving researchers more flexibility to test their system, we have divided the test set into a number of splits, including test-dev, test-standard, and test-challenge. Test-dev is used for debugging and validation experiments and allows for unlimited submission to the evaluation server. Test-standard is used to maintain a public <a href="http://mscoco.org/dataset/#keypoints-leaderboard">leaderboard</a> that is updated upon submission. Finally, test-challenge is used for the workshop competition; results will be revealed during the workshop at ECCV 2016. A more thorough explanation is available on the <a href="http://mscoco.org/dataset/#keypoints-upload">upload</a> page.
</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
Competitors are recommended but not restricted to train their algorithms on COCO 2014 train and val sets. The <a href="http://mscoco.org/dataset/#download">download</a> page contains links to all COCO 2014 train+val images and associated annotations as well as the 2015 test images.  Please specify any and all external data used for training in the "method description" when uploading results to the evaluation server.
</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
By the challenge deadline, results must be submitted to the evaluation server. Competitors' algorithms will be evaluated according to the rules described on the <a href="http://mscoco.org/dataset/#keypoints-eval">evaluation</a> page. Challenge participants with the most successful and innovative methods will be invited to present.
</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
As noted earlier, the keypoint challenge involves simultaneously detecting people and localizing their keypoints (person locations are not given at test time). As this is a fairly under-explored setting, we have carefully designed a new set of metrics for this task. Please refer to the "Metrics" section of the <a href="http://mscoco.org/dataset/#keypoints-eval">evaluation</a> page for a detailed explanation of the competition metrics.
</p>

<!------------------------------------------------------------------------------------------------>
<p align="justify" class="titleSegoeLight" style="width:500px">6. Tools and Instructions</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
We provide extensive API support for the COCO images, annotations, and evaluation code. To download the COCO API, please visit our <a href="https://github.com/pdollar/coco">GitHub repository</a>. For an overview of how to use the API, please visit the <a href="http://mscoco.org/dataset/#download">download</a> page.
</p>
<p align="justify" class="bodyNormal" style="color:black;font-size:90%">
Due to the large size of the COCO dataset and the complexity of this challenge, the process of competing in this challenge may not seem simple. To help, we provide explanations and instructions for each step of the process on the <a href="http://mscoco.org/dataset/#download">download</a>, <a href="http://mscoco.org/dataset/#format">format</a>, <a href="http://mscoco.org/dataset/#keypoints-eval">evaluation</a>, and <a href="http://mscoco.org/dataset/#keypoints-upload">upload</a> pages. For additional questions, please contact <a href="mailto:cocodataset@outlook.com">cocodataset@outlook.com</a>.
</p>
<br>
<br>
</div>
            <div role="tabpanel" class="tab-pane fade active in" id="captions-leaderboard">       <script>
var initCaptioningLeaderboard = function() {
  // data columns
  var colsTable = ["CIDEr", "METEOR", "ROUGE_L", "Bleu_1", "Bleu_2", "Bleu_3", "Bleu_4", "date"];
  var colsChallenge = ["q1", "q2", "q3", "q4", "q5", "date"];
  // load single entry from database into leaderboard
  var loadEntry = function( dataset, results, name, publication, url, date ) {
    var i=dataset.main.length;
    var cols = dataset.cols;
    dataset.main[i] = new Array();
    results = JSON.parse(results);
    results["date"] = date;
    dataset.main[i][0] = name+"<sup>["+(i+1)+"]</sup>";
    for( var j=0; j<cols.length; j++ ) dataset.main[i][j+1] = results[cols[j]];
    if(url != "") name = "<a href='" + url + "' target='_blank'> " + name + " </a>";
    dataset.refs[i] = new Array();
    dataset.refs[i][0] = "["+(i+1)+"] " + name;
    dataset.refs[i][1] = publication;
  };
  // load all leaderboard data using Django
  var N = 3
  var datasets = new Array(N);
  var leaderboards = new Array(N);
  for( i=0; i<N; i++ )
    datasets[i] = { main:[], refs:[], cols:colsTable }
  
    var i = 0;
    leaderboards[i] = "cap-c5";
    if ("cap-c5" == "cap-challenge2015")
      datasets[i].cols = colsChallenge;
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.911, \u0022Bleu_4\u0022: 0.306, \u0022Bleu_3\u0022: 0.414, \u0022Bleu_2\u0022: 0.556, \u0022Bleu_1\u0022: 0.725, \u0022ROUGE_L\u0022: 0.528, \u0022METEOR\u0022: 0.246}", "ACVT", "Qi Wu, Chunhua Shen, Anton van den Hengel, Lingqiao Liu, Anthony Dick, What value high level concepts in vision to language problems?", "http://arxiv.org/abs/1506.01144", "2015-08-02" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.943, \u0022Bleu_4\u0022: 0.316, \u0022Bleu_3\u0022: 0.424, \u0022Bleu_2\u0022: 0.565, \u0022Bleu_1\u0022: 0.731, \u0022ROUGE_L\u0022: 0.535, \u0022METEOR\u0022: 0.25}", "ATT", "Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, Jiebo Luo, Image Captioning with Semantic Attention, CVPR 2016", "http://arxiv.org/abs/1603.03925", "2016-01-23" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.956, \u0022Bleu_4\u0022: 0.315, \u0022Bleu_3\u0022: 0.416, \u0022Bleu_2\u0022: 0.553, \u0022Bleu_1\u0022: 0.721, \u0022ROUGE_L\u0022: 0.531, \u0022METEOR\u0022: 0.251}", "AugmentCNNwithDet", "", "", "2016-03-29" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.921, \u0022Bleu_4\u0022: 0.306, \u0022Bleu_3\u0022: 0.409, \u0022Bleu_2\u0022: 0.548, \u0022Bleu_1\u0022: 0.718, \u0022ROUGE_L\u0022: 0.528, \u0022METEOR\u0022: 0.247}", "Berkeley LRCN", "J. Donahue, L. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, T. Darrell, Long\u002Dterm Recurrent Convolutional Networks for Visual Recognition and Description, CVPR 2015", "http://arxiv.org/abs/1411.4389", "2015-12-08" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.517, \u0022Bleu_4\u0022: 0.134, \u0022Bleu_3\u0022: 0.213, \u0022Bleu_2\u0022: 0.339, \u0022Bleu_1\u0022: 0.535, \u0022ROUGE_L\u0022: 0.403, \u0022METEOR\u0022: 0.195}", "Brno University", "Martin Kolar, Michal Hradis, Pavel Zemcik, Technical Report: Image Captioning with Semantically Similar Images, arXiv 2015", "http://arxiv.org/abs/1506.03995 ", "2015-05-29" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.955, \u0022Bleu_4\u0022: 0.309, \u0022Bleu_3\u0022: 0.414, \u0022Bleu_2\u0022: 0.553, \u0022Bleu_1\u0022: 0.723, \u0022ROUGE_L\u0022: 0.531, \u0022METEOR\u0022: 0.252}", "ChalLS", "", "", "2016-05-21" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.939, \u0022Bleu_4\u0022: 0.314, \u0022Bleu_3\u0022: 0.418, \u0022Bleu_2\u0022: 0.556, \u0022Bleu_1\u0022: 0.722, \u0022ROUGE_L\u0022: 0.53, \u0022METEOR\u0022: 0.248}", "Fukun_Jinjunqi", "", "", "2016-05-09" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.428, \u0022Bleu_4\u0022: 0.158, \u0022Bleu_3\u0022: 0.234, \u0022Bleu_2\u0022: 0.36, \u0022Bleu_1\u0022: 0.555, \u0022ROUGE_L\u0022: 0.408, \u0022METEOR\u0022: 0.162}", "GabYesh", "", "", "2015-09-24" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.943, \u0022Bleu_4\u0022: 0.309, \u0022Bleu_3\u0022: 0.407, \u0022Bleu_2\u0022: 0.542, \u0022Bleu_1\u0022: 0.713, \u0022ROUGE_L\u0022: 0.53, \u0022METEOR\u0022: 0.254}", "Google", "Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, Show and Tell: A Neural Image Caption Generator, CVPR 2015", "http://arxiv.org/abs/1411.4555", "2015-05-29" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.373, \u0022Bleu_4\u0022: 0.089, \u0022Bleu_3\u0022: 0.153, \u0022Bleu_2\u0022: 0.263, \u0022Bleu_1\u0022: 0.449, \u0022ROUGE_L\u0022: 0.356, \u0022METEOR\u0022: 0.175}", "HUCVL\u002DMETULcsL", "", "", "2015-07-12" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.854, \u0022Bleu_4\u0022: 0.217, \u0022Bleu_3\u0022: 0.321, \u0022Bleu_2\u0022: 0.469, \u0022Bleu_1\u0022: 0.663, \u0022ROUGE_L\u0022: 0.484, \u0022METEOR\u0022: 0.252}", "Human", "Human Baseline", "http://arxiv.org/pdf/1504.00325.pdf", "2015-03-23" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.917, \u0022Bleu_4\u0022: 0.299, \u0022Bleu_3\u0022: 0.404, \u0022Bleu_2\u0022: 0.545, \u0022Bleu_1\u0022: 0.716, \u0022ROUGE_L\u0022: 0.521, \u0022METEOR\u0022: 0.242}", "m\u002DRNN", "Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan Yuille, Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images. arXiv preprint arXiv:1504.06692 (2015) ", "http://arxiv.org/abs/1504.06692", "2015-05-30" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.886, \u0022Bleu_4\u0022: 0.302, \u0022Bleu_3\u0022: 0.41, \u0022Bleu_2\u0022: 0.553, \u0022Bleu_1\u0022: 0.72, \u0022ROUGE_L\u0022: 0.524, \u0022METEOR\u0022: 0.238}", "m\u002DRNN (Baidu/ UCLA)", "Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Yuille, Alan, Deep Captioning with Multimodal Recurrent Neural Networks (m\u002DRNN), arXiv 2014", "http://arxiv.org/abs/1412.6632", "2015-05-26" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.666, \u0022Bleu_4\u0022: 0.216, \u0022Bleu_3\u0022: 0.323, \u0022Bleu_2\u0022: 0.472, \u0022Bleu_1\u0022: 0.652, \u0022ROUGE_L\u0022: 0.468, \u0022METEOR\u0022: 0.214}", "MIL", "", "", "2015-05-29" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.74, \u0022Bleu_4\u0022: 0.26, \u0022Bleu_3\u0022: 0.362, \u0022Bleu_2\u0022: 0.498, \u0022Bleu_1\u0022: 0.666, \u0022ROUGE_L\u0022: 0.499, \u0022METEOR\u0022: 0.219}", "MLBL", "Multimodal Neural Language Models (Kiros et al, ICML 2014), Unifying Visual\u002DSemantic Embeddings with Multimodal Neural Language Models (Kiros et al, arXiv 2014), and Scalable Bayesian Optimization Using Deep Neural Networks (Snoek et al, arXiv 2015)", "http://arxiv.org/pdf/1411.2539.pdf", "2015-04-10" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.865, \u0022Bleu_4\u0022: 0.277, \u0022Bleu_3\u0022: 0.383, \u0022Bleu_2\u0022: 0.528, \u0022Bleu_1\u0022: 0.705, \u0022ROUGE_L\u0022: 0.516, \u0022METEOR\u0022: 0.241}", "Montreal/Toronto", "Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua, Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, arXiv 2015", "http://arxiv.org/abs/1502.03044 ", "2015-05-31" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.984, \u0022Bleu_4\u0022: 0.33, \u0022Bleu_3\u0022: 0.436, \u0022Bleu_2\u0022: 0.575, \u0022Bleu_1\u0022: 0.739, \u0022ROUGE_L\u0022: 0.542, \u0022METEOR\u0022: 0.256}", "MSM@MSRA", "", "", "2016-06-08" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.912, \u0022Bleu_4\u0022: 0.291, \u0022Bleu_3\u0022: 0.391, \u0022Bleu_2\u0022: 0.526, \u0022Bleu_1\u0022: 0.695, \u0022ROUGE_L\u0022: 0.519, \u0022METEOR\u0022: 0.247}", "MSR", "H. Fang, S. Gupta, F. Iandola, R. Srivastava, L. Deng, P. Dollár, J. Gao, X. He, M. Mitchell, J. Platt, C.L. Zitnick, and G. Zweig, From Captions to Visual Concepts and Back, CVPR 2015", "http://arxiv.org/abs/1411.4952", "2015-04-08" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.931, \u0022Bleu_4\u0022: 0.308, \u0022Bleu_3\u0022: 0.407, \u0022Bleu_2\u0022: 0.543, \u0022Bleu_1\u0022: 0.715, \u0022ROUGE_L\u0022: 0.526, \u0022METEOR\u0022: 0.248}", "MSR Captivator", "Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, Margaret Mitchell, Language Models for Image Captioning: The Quirks and What Works, arXiv 2015", "http://arxiv.org/abs/1505.01809", "2015-05-28" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.933, \u0022Bleu_4\u0022: 0.311, \u0022Bleu_3\u0022: 0.421, \u0022Bleu_2\u0022: 0.567, \u0022Bleu_1\u0022: 0.736, \u0022ROUGE_L\u0022: 0.532, \u0022METEOR\u0022: 0.246}", "MSRA\u002DMSM", "", "", "2016-03-13" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.886, \u0022Bleu_4\u0022: 0.28, \u0022Bleu_3\u0022: 0.382, \u0022Bleu_2\u0022: 0.521, \u0022Bleu_1\u0022: 0.697, \u0022ROUGE_L\u0022: 0.507, \u0022METEOR\u0022: 0.237}", "Nearest Neighbor", "Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C. Lawrence Zitnick, Exploring Nearest Neighbor Approaches for Image Captioning, arXiv 2015", "http://arxiv.org/abs/1505.04467", "2015-05-15" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.674, \u0022Bleu_4\u0022: 0.224, \u0022Bleu_3\u0022: 0.321, \u0022Bleu_2\u0022: 0.464, \u0022Bleu_1\u0022: 0.65, \u0022ROUGE_L\u0022: 0.475, \u0022METEOR\u0022: 0.21}", "NeuralTalk", "Andrej Karpathy, Li Fei\u002DFei, Deep Visual\u002DSemantic Alignments for Generating Image Descriptions, CVPR 2015", "http://arxiv.org/abs/1412.2306", "2015-04-15" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.833, \u0022Bleu_4\u0022: 0.268, \u0022Bleu_3\u0022: 0.371, \u0022Bleu_2\u0022: 0.512, \u0022Bleu_1\u0022: 0.692, \u0022ROUGE_L\u0022: 0.505, \u0022METEOR\u0022: 0.236}", "nlab\u002Dutokyo", "", "", "2016-03-03" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.899, \u0022Bleu_4\u0022: 0.299, \u0022Bleu_3\u0022: 0.399, \u0022Bleu_2\u0022: 0.536, \u0022Bleu_1\u0022: 0.707, \u0022ROUGE_L\u0022: 0.52, \u0022METEOR\u0022: 0.243}", "PicSOM", "", "", "2016-02-10" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.671, \u0022Bleu_4\u0022: 0.229, \u0022Bleu_3\u0022: 0.333, \u0022Bleu_2\u0022: 0.48, \u0022Bleu_1\u0022: 0.662, \u0022ROUGE_L\u0022: 0.482, \u0022METEOR\u0022: 0.207}", "Shijian Tang", "", "", "2015-08-21" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.791, \u0022Bleu_4\u0022: 0.251, \u0022Bleu_3\u0022: 0.353, \u0022Bleu_2\u0022: 0.498, \u0022Bleu_1\u0022: 0.678, \u0022ROUGE_L\u0022: 0.499, \u0022METEOR\u0022: 0.23}", "Snapshopr_Research", "", "", "2016-04-23" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.969, \u0022Bleu_4\u0022: 0.323, \u0022Bleu_3\u0022: 0.436, \u0022Bleu_2\u0022: 0.583, \u0022Bleu_1\u0022: 0.751, \u0022ROUGE_L\u0022: 0.541, \u0022METEOR\u0022: 0.251}", "THU_MIG", "", "", "2016-06-03" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.899, \u0022Bleu_4\u0022: 0.288, \u0022Bleu_3\u0022: 0.39, \u0022Bleu_2\u0022: 0.527, \u0022Bleu_1\u0022: 0.699, \u0022ROUGE_L\u0022: 0.52, \u0022METEOR\u0022: 0.247}", "Tsinghua Bigeye", "", "", "2015-11-29" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.906, \u0022Bleu_4\u0022: 0.318, \u0022Bleu_3\u0022: 0.423, \u0022Bleu_2\u0022: 0.559, \u0022Bleu_1\u0022: 0.719, \u0022ROUGE_L\u0022: 0.531, \u0022METEOR\u0022: 0.245}", "zhaofang_lv", "", "", "2016-05-25" );
    
  
    var i = 1;
    leaderboards[i] = "cap-c40";
    if ("cap-c40" == "cap-challenge2015")
      datasets[i].cols = colsChallenge;
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.924, \u0022Bleu_4\u0022: 0.582, \u0022Bleu_3\u0022: 0.694, \u0022Bleu_2\u0022: 0.803, \u0022Bleu_1\u0022: 0.892, \u0022ROUGE_L\u0022: 0.672, \u0022METEOR\u0022: 0.329}", "ACVT", "Qi Wu, Chunhua Shen, Anton van den Hengel, Lingqiao Liu, Anthony Dick, What value high level concepts in vision to language problems?", "http://arxiv.org/abs/1506.01144", "2015-08-02" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.958, \u0022Bleu_4\u0022: 0.599, \u0022Bleu_3\u0022: 0.709, \u0022Bleu_2\u0022: 0.815, \u0022Bleu_1\u0022: 0.9, \u0022ROUGE_L\u0022: 0.682, \u0022METEOR\u0022: 0.335}", "ATT", "Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, Jiebo Luo, Image Captioning with Semantic Attention, CVPR 2016", "http://arxiv.org/abs/1603.03925", "2016-01-23" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.968, \u0022Bleu_4\u0022: 0.597, \u0022Bleu_3\u0022: 0.706, \u0022Bleu_2\u0022: 0.815, \u0022Bleu_1\u0022: 0.905, \u0022ROUGE_L\u0022: 0.683, \u0022METEOR\u0022: 0.34}", "AugmentCNNwithDet", "", "", "2016-03-29" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.934, \u0022Bleu_4\u0022: 0.585, \u0022Bleu_3\u0022: 0.695, \u0022Bleu_2\u0022: 0.804, \u0022Bleu_1\u0022: 0.895, \u0022ROUGE_L\u0022: 0.678, \u0022METEOR\u0022: 0.335}", "Berkeley LRCN", "J. Donahue, L. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, T. Darrell, Long\u002Dterm Recurrent Convolutional Networks for Visual Recognition and Description, CVPR 2015", "http://arxiv.org/abs/1411.4389", "2015-12-08" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.536, \u0022Bleu_4\u0022: 0.278, \u0022Bleu_3\u0022: 0.392, \u0022Bleu_2\u0022: 0.541, \u0022Bleu_1\u0022: 0.716, \u0022ROUGE_L\u0022: 0.509, \u0022METEOR\u0022: 0.252}", "Brno University", "Martin Kolar, Michal Hradis, Pavel Zemcik, Technical Report: Image Captioning with Semantically Similar Images, arXiv 2015", "http://arxiv.org/abs/1506.03995 ", "2015-05-29" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.97, \u0022Bleu_4\u0022: 0.59, \u0022Bleu_3\u0022: 0.701, \u0022Bleu_2\u0022: 0.809, \u0022Bleu_1\u0022: 0.898, \u0022ROUGE_L\u0022: 0.679, \u0022METEOR\u0022: 0.34}", "ChalLS", "", "", "2016-05-21" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.946, \u0022Bleu_4\u0022: 0.601, \u0022Bleu_3\u0022: 0.711, \u0022Bleu_2\u0022: 0.817, \u0022Bleu_1\u0022: 0.902, \u0022ROUGE_L\u0022: 0.68, \u0022METEOR\u0022: 0.336}", "Fukun_Jinjunqi", "", "", "2016-05-09" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.443, \u0022Bleu_4\u0022: 0.323, \u0022Bleu_3\u0022: 0.427, \u0022Bleu_2\u0022: 0.564, \u0022Bleu_1\u0022: 0.725, \u0022ROUGE_L\u0022: 0.524, \u0022METEOR\u0022: 0.219}", "GabYesh", "", "", "2015-09-24" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.946, \u0022Bleu_4\u0022: 0.587, \u0022Bleu_3\u0022: 0.694, \u0022Bleu_2\u0022: 0.802, \u0022Bleu_1\u0022: 0.895, \u0022ROUGE_L\u0022: 0.682, \u0022METEOR\u0022: 0.346}", "Google", "Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, Show and Tell: A Neural Image Caption Generator, CVPR 2015", "http://arxiv.org/abs/1411.4555", "2015-05-29" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.399, \u0022Bleu_4\u0022: 0.202, \u0022Bleu_3\u0022: 0.306, \u0022Bleu_2\u0022: 0.452, \u0022Bleu_1\u0022: 0.639, \u0022ROUGE_L\u0022: 0.46, \u0022METEOR\u0022: 0.23}", "HUCVL\u002DMETULcsL", "", "", "2015-07-12" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.91, \u0022Bleu_4\u0022: 0.471, \u0022Bleu_3\u0022: 0.603, \u0022Bleu_2\u0022: 0.744, \u0022Bleu_1\u0022: 0.88, \u0022ROUGE_L\u0022: 0.626, \u0022METEOR\u0022: 0.335}", "Human", "Human Baseline", "http://arxiv.org/pdf/1504.00325.pdf", "2015-03-23" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.935, \u0022Bleu_4\u0022: 0.575, \u0022Bleu_3\u0022: 0.687, \u0022Bleu_2\u0022: 0.798, \u0022Bleu_1\u0022: 0.89, \u0022ROUGE_L\u0022: 0.666, \u0022METEOR\u0022: 0.325}", "m\u002DRNN", "Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan Yuille, Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images. arXiv preprint arXiv:1504.06692 (2015) ", "http://arxiv.org/abs/1504.06692", "2015-05-30" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.896, \u0022Bleu_4\u0022: 0.578, \u0022Bleu_3\u0022: 0.69, \u0022Bleu_2\u0022: 0.801, \u0022Bleu_1\u0022: 0.89, \u0022ROUGE_L\u0022: 0.668, \u0022METEOR\u0022: 0.32}", "m\u002DRNN (Baidu/ UCLA)", "Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Yuille, Alan, Deep Captioning with Multimodal Recurrent Neural Networks (m\u002DRNN), arXiv 2014", "http://arxiv.org/abs/1412.6632", "2015-05-26" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.69, \u0022Bleu_4\u0022: 0.432, \u0022Bleu_3\u0022: 0.564, \u0022Bleu_2\u0022: 0.707, \u0022Bleu_1\u0022: 0.827, \u0022ROUGE_L\u0022: 0.596, \u0022METEOR\u0022: 0.284}", "MIL", "", "", "2015-05-29" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.752, \u0022Bleu_4\u0022: 0.517, \u0022Bleu_3\u0022: 0.633, \u0022Bleu_2\u0022: 0.747, \u0022Bleu_1\u0022: 0.848, \u0022ROUGE_L\u0022: 0.635, \u0022METEOR\u0022: 0.294}", "MLBL", "Multimodal Neural Language Models (Kiros et al, ICML 2014), Unifying Visual\u002DSemantic Embeddings with Multimodal Neural Language Models (Kiros et al, arXiv 2014), and Scalable Bayesian Optimization Using Deep Neural Networks (Snoek et al, arXiv 2015)", "http://arxiv.org/pdf/1411.2539.pdf", "2015-04-10" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.893, \u0022Bleu_4\u0022: 0.537, \u0022Bleu_3\u0022: 0.658, \u0022Bleu_2\u0022: 0.779, \u0022Bleu_1\u0022: 0.881, \u0022ROUGE_L\u0022: 0.654, \u0022METEOR\u0022: 0.322}", "Montreal/Toronto", "Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua, Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, arXiv 2015", "http://arxiv.org/abs/1502.03044 ", "2015-05-31" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 1.003, \u0022Bleu_4\u0022: 0.632, \u0022Bleu_3\u0022: 0.74, \u0022Bleu_2\u0022: 0.842, \u0022Bleu_1\u0022: 0.919, \u0022ROUGE_L\u0022: 0.7, \u0022METEOR\u0022: 0.35}", "MSM@MSRA", "", "", "2016-06-08" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.925, \u0022Bleu_4\u0022: 0.567, \u0022Bleu_3\u0022: 0.678, \u0022Bleu_2\u0022: 0.789, \u0022Bleu_1\u0022: 0.88, \u0022ROUGE_L\u0022: 0.662, \u0022METEOR\u0022: 0.331}", "MSR", "H. Fang, S. Gupta, F. Iandola, R. Srivastava, L. Deng, P. Dollár, J. Gao, X. He, M. Mitchell, J. Platt, C.L. Zitnick, and G. Zweig, From Captions to Visual Concepts and Back, CVPR 2015", "http://arxiv.org/abs/1411.4952", "2015-04-08" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.937, \u0022Bleu_4\u0022: 0.601, \u0022Bleu_3\u0022: 0.71, \u0022Bleu_2\u0022: 0.819, \u0022Bleu_1\u0022: 0.907, \u0022ROUGE_L\u0022: 0.68, \u0022METEOR\u0022: 0.339}", "MSR Captivator", "Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, Margaret Mitchell, Language Models for Image Captioning: The Quirks and What Works, arXiv 2015", "http://arxiv.org/abs/1505.01809", "2015-05-28" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.954, \u0022Bleu_4\u0022: 0.591, \u0022Bleu_3\u0022: 0.705, \u0022Bleu_2\u0022: 0.815, \u0022Bleu_1\u0022: 0.901, \u0022ROUGE_L\u0022: 0.677, \u0022METEOR\u0022: 0.328}", "MSRA\u002DMSM", "", "", "2016-03-13" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.916, \u0022Bleu_4\u0022: 0.542, \u0022Bleu_3\u0022: 0.655, \u0022Bleu_2\u0022: 0.77, \u0022Bleu_1\u0022: 0.872, \u0022ROUGE_L\u0022: 0.648, \u0022METEOR\u0022: 0.318}", "Nearest Neighbor", "Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C. Lawrence Zitnick, Exploring Nearest Neighbor Approaches for Image Captioning, arXiv 2015", "http://arxiv.org/abs/1505.04467", "2015-05-15" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.692, \u0022Bleu_4\u0022: 0.446, \u0022Bleu_3\u0022: 0.566, \u0022Bleu_2\u0022: 0.701, \u0022Bleu_1\u0022: 0.828, \u0022ROUGE_L\u0022: 0.603, \u0022METEOR\u0022: 0.28}", "NeuralTalk", "Andrej Karpathy, Li Fei\u002DFei, Deep Visual\u002DSemantic Alignments for Generating Image Descriptions, CVPR 2015", "http://arxiv.org/abs/1412.2306", "2015-04-15" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.851, \u0022Bleu_4\u0022: 0.518, \u0022Bleu_3\u0022: 0.638, \u0022Bleu_2\u0022: 0.762, \u0022Bleu_1\u0022: 0.871, \u0022ROUGE_L\u0022: 0.646, \u0022METEOR\u0022: 0.316}", "nlab\u002Dutokyo", "", "", "2016-03-03" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.901, \u0022Bleu_4\u0022: 0.57, \u0022Bleu_3\u0022: 0.68, \u0022Bleu_2\u0022: 0.792, \u0022Bleu_1\u0022: 0.887, \u0022ROUGE_L\u0022: 0.666, \u0022METEOR\u0022: 0.328}", "PicSOM", "", "", "2016-02-10" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.685, \u0022Bleu_4\u0022: 0.465, \u0022Bleu_3\u0022: 0.59, \u0022Bleu_2\u0022: 0.72, \u0022Bleu_1\u0022: 0.836, \u0022ROUGE_L\u0022: 0.61, \u0022METEOR\u0022: 0.274}", "Shijian Tang", "", "", "2015-08-21" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.826, \u0022Bleu_4\u0022: 0.493, \u0022Bleu_3\u0022: 0.615, \u0022Bleu_2\u0022: 0.743, \u0022Bleu_1\u0022: 0.856, \u0022ROUGE_L\u0022: 0.636, \u0022METEOR\u0022: 0.308}", "Snapshopr_Research", "", "", "2016-04-23" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.988, \u0022Bleu_4\u0022: 0.616, \u0022Bleu_3\u0022: 0.727, \u0022Bleu_2\u0022: 0.833, \u0022Bleu_1\u0022: 0.913, \u0022ROUGE_L\u0022: 0.688, \u0022METEOR\u0022: 0.336}", "THU_MIG", "", "", "2016-06-03" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.908, \u0022Bleu_4\u0022: 0.558, \u0022Bleu_3\u0022: 0.67, \u0022Bleu_2\u0022: 0.783, \u0022Bleu_1\u0022: 0.881, \u0022ROUGE_L\u0022: 0.663, \u0022METEOR\u0022: 0.332}", "Tsinghua Bigeye", "", "", "2015-11-29" );
    
    loadEntry( datasets[i], "{\u0022CIDEr\u0022: 0.917, \u0022Bleu_4\u0022: 0.618, \u0022Bleu_3\u0022: 0.727, \u0022Bleu_2\u0022: 0.828, \u0022Bleu_1\u0022: 0.906, \u0022ROUGE_L\u0022: 0.686, \u0022METEOR\u0022: 0.335}", "zhaofang_lv", "", "", "2016-05-25" );
    
  
    var i = 2;
    leaderboards[i] = "cap-challenge2015";
    if ("cap-challenge2015" == "cap-challenge2015")
      datasets[i].cols = colsChallenge;
    
    loadEntry( datasets[i], "{\u0022q1\u0022: 0.154, \u0022q3\u0022: 3.516, \u0022q2\u0022: 0.19, \u0022q5\u0022: 0.155, \u0022q4\u0022: 2.599}", "ACVT", "", "", "2015-05-26" );
    
    loadEntry( datasets[i], "{\u0022q1\u0022: 0.246, \u0022q3\u0022: 3.924, \u0022q2\u0022: 0.268, \u0022q5\u0022: 0.204, \u0022q4\u0022: 2.786}", "Berkeley LRCN", "J. Donahue, L. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, T. Darrell, Long\u002Dterm Recurrent Convolutional Networks for Visual Recognition and Description, CVPR 2015", "http://arxiv.org/abs/1411.4389", "2015-04-25" );
    
    loadEntry( datasets[i], "{\u0022q1\u0022: 0.194, \u0022q3\u0022: 3.079, \u0022q2\u0022: 0.213, \u0022q5\u0022: 0.154, \u0022q4\u0022: 3.482}", "Brno University", "Martin Kolar, Michal Hradis, Pavel Zemcik, Technical Report: Image Captioning with Semantically Similar Images, arXiv 2015", "http://arxiv.org/abs/1506.03995 ", "2015-05-29" );
    
    loadEntry( datasets[i], "{\u0022q1\u0022: 0.273, \u0022q3\u0022: 4.107, \u0022q2\u0022: 0.317, \u0022q5\u0022: 0.233, \u0022q4\u0022: 2.742}", "Google", "Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, Show and Tell: A Neural Image Caption Generator, CVPR 2015", "http://arxiv.org/abs/1411.4555", "2015-05-29" );
    
    loadEntry( datasets[i], "{\u0022q1\u0022: 0.638, \u0022q3\u0022: 4.836, \u0022q2\u0022: 0.675, \u0022q5\u0022: 0.352, \u0022q4\u0022: 3.428}", "Human", "Human Baseline", "http://arxiv.org/pdf/1504.00325.pdf", "2015-03-23" );
    
    loadEntry( datasets[i], "{\u0022q1\u0022: 0.223, \u0022q3\u0022: 3.897, \u0022q2\u0022: 0.252, \u0022q5\u0022: 0.202, \u0022q4\u0022: 2.595}", "m\u002DRNN", "Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan Yuille, Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images. arXiv preprint arXiv:1504.06692 (2015) ", "http://arxiv.org/abs/1504.06692", "2015-05-30" );
    
    loadEntry( datasets[i], "{\u0022q1\u0022: 0.19, \u0022q3\u0022: 3.831, \u0022q2\u0022: 0.241, \u0022q5\u0022: 0.195, \u0022q4\u0022: 2.548}", "m\u002DRNN (Baidu/ UCLA)", "Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Yuille, Alan, Deep Captioning with Multimodal Recurrent Neural Networks (m\u002DRNN), arXiv 2014", "http://arxiv.org/abs/1412.6632", "2015-05-26" );
    
    loadEntry( datasets[i], "{\u0022q1\u0022: 0.168, \u0022q3\u0022: 3.349, \u0022q2\u0022: 0.197, \u0022q5\u0022: 0.159, \u0022q4\u0022: 2.915}", "MIL", "", "", "2015-05-29" );
    
    loadEntry( datasets[i], "{\u0022q1\u0022: 0.167, \u0022q3\u0022: 3.659, \u0022q2\u0022: 0.196, \u0022q5\u0022: 0.156, \u0022q4\u0022: 2.42}", "MLBL", "Multimodal Neural Language Models (Kiros et al, ICML 2014), Unifying Visual\u002DSemantic Embeddings with Multimodal Neural Language Models (Kiros et al, arXiv 2014), and Scalable Bayesian Optimization Using Deep Neural Networks (Snoek et al, arXiv 2015)", "http://arxiv.org/pdf/1411.2539.pdf", "2015-04-10" );
    
    loadEntry( datasets[i], "{\u0022q1\u0022: 0.262, \u0022q3\u0022: 3.932, \u0022q2\u0022: 0.272, \u0022q5\u0022: 0.197, \u0022q4\u0022: 2.832}", "Montreal/Toronto", "Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua, Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, arXiv 2015", "http://arxiv.org/abs/1502.03044 ", "2015-05-14" );
    
    loadEntry( datasets[i], "{\u0022q1\u0022: 0.268, \u0022q3\u0022: 4.137, \u0022q2\u0022: 0.322, \u0022q5\u0022: 0.234, \u0022q4\u0022: 2.662}", "MSR", "H. Fang, S. Gupta, F. Iandola, R. Srivastava, L. Deng, P. Dollár, J. Gao, X. He, M. Mitchell, J. Platt, C.L. Zitnick, and G. Zweig, From Captions to Visual Concepts and Back, CVPR 2015", "http://arxiv.org/abs/1411.4952", "2015-04-08" );
    
    loadEntry( datasets[i], "{\u0022q1\u0022: 0.25, \u0022q3\u0022: 4.149, \u0022q2\u0022: 0.301, \u0022q5\u0022: 0.233, \u0022q4\u0022: 2.565}", "MSR Captivator", "Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, Margaret Mitchell, Language Models for Image Captioning: The Quirks and What Works, arXiv 2015", "http://arxiv.org/abs/1505.01809", "2015-05-28" );
    
    loadEntry( datasets[i], "{\u0022q1\u0022: 0.216, \u0022q3\u0022: 3.801, \u0022q2\u0022: 0.255, \u0022q5\u0022: 0.196, \u0022q4\u0022: 2.716}", "Nearest Neighbor", "Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C. Lawrence Zitnick, Exploring Nearest Neighbor Approaches for Image Captioning, arXiv 2015", "http://arxiv.org/abs/1505.04467", "2015-05-15" );
    
    loadEntry( datasets[i], "{\u0022q1\u0022: 0.166, \u0022q3\u0022: 3.436, \u0022q2\u0022: 0.192, \u0022q5\u0022: 0.147, \u0022q4\u0022: 2.742}", "NeuralTalk", "Andrej Karpathy, Li Fei\u002DFei, Deep Visual\u002DSemantic Alignments for Generating Image Descriptions, CVPR 2015", "http://arxiv.org/abs/1412.2306", "2015-04-15" );
    
    loadEntry( datasets[i], "{\u0022q1\u0022: 0.202, \u0022q3\u0022: 3.965, \u0022q2\u0022: 0.25, \u0022q5\u0022: 0.182, \u0022q4\u0022: 2.552}", "PicSOM", "", "", "2015-05-26" );
    
    loadEntry( datasets[i], "{\u0022q1\u0022:0.007,\u0022q2\u0022:0.020,\u0022q3\u0022:1.084,\u0022q4\u0022:3.247,\u0022q5\u0022:0.013}", "Random", "", "", "2015-05-29" );
    
    loadEntry( datasets[i], "{\u0022q1\u0022: 0.1, \u0022q3\u0022: 3.51, \u0022q2\u0022: 0.146, \u0022q5\u0022: 0.116, \u0022q4\u0022: 2.163}", "Tsinghua Bigeye", "", "", "2015-04-23" );
    
  
  // metrics and challenge ranking are hardcoded
  var metricsMain = [
    ['CIDEr-D', '<a href="http://arxiv.org/pdf/1411.5726.pdf" target="_blank"> CIDEr: Consensus-based Image Description Evaluation</a>'],
    ['METEOR',  '<a href="http://www.cs.cmu.edu/~alavie/METEOR/pdf/meteor-1.5.pdf" target="_blank">Meteor Universal: Language Specific Translation Evaluation for Any Target Language</a>'],
    ['Rouge-L', '<a href="http://anthology.aclweb.org/W/W04/W04-1013.pdf" target="_blank"> ROUGE: A Package for Automatic Evaluation of Summaries</a>'],
    ['BLEU',    '<a href="http://www.aclweb.org/anthology/P02-1040.pdf" target="_blank"> BLEU: a Method for Automatic Evaluation of Machine Translation</a>']
  ];
  var metricsChallenge = [
    ['M1', 'Percentage of captions that are evaluated as better or equal to human caption.'],
    ['M2', 'Percentage of captions that pass the Turing Test.'],
    ['M3', 'Average correctness of the captions on a scale 1-5 (incorrect - correct).'],
    ['M4', 'Average amount of detail of the captions on a scale 1-5 (lack of details - very detailed).'],
    ['M5', 'Percentage of captions that are similar to human description.']
  ];
  var ranksChallenge = [
    ['Google', 5, 4, 9, '1st(tie)'],
    ['MSR', 4, 5, 9, '1st(tie)'],
    ['MSR Captivator', 2, 3, 5, '3rd(tie)'],
    ['Montreal/Toronto', 3, 2, 5, '3rd(tie)'],
    ['Berkeley LRCN', 1, 1, 2, '5th']
  ];
  // initialize and format DataTables https://www.datatables.net/
  var propsRefs = { 'paging':false, 'info':false, 'filter':false, 'sort':false, 'autoWidth':false };
  propsRefs.columnDefs = [{'targets':0,'sWidth':'25%'}];
  var propsData = { 'paging':false, 'info':false, 'filter':false, 'order':[[1, 'desc']] };
  propsData.columnDefs = [{'targets':['_all'],'orderSequence':['desc','asc']}];
  for( var i=0; i<N; i++ ) {
    $('#div-'+leaderboards[i]+' #data').DataTable(propsData).rows.add(datasets[i].main).draw();
    $('#div-'+leaderboards[i]+' #refs').DataTable(propsRefs).rows.add(datasets[i].refs).draw();
  };
  $('#div-cap-c5 #metrics').DataTable(propsRefs).rows.add(metricsMain).draw();
  $('#div-cap-c40 #metrics').DataTable(propsRefs).rows.add(metricsMain).draw();
  $('#div-cap-challenge2015 #metrics').DataTable(propsRefs).rows.add(metricsChallenge).draw();
  $('#div-cap-challenge2015 #rank').DataTable(propsData).rows.add(ranksChallenge).draw();
};
$( function() { initCaptioningLeaderboard(); } );
</script>

<!------------------------------------------------------------------------------------------------>
<style>
  #captioning td { padding: 6px 0px 6px 10px; vertical-align:middle }
  #captioning th { padding: 6px 16px 6px 0px; text-align:center }
</style>
<div style="margin-top:10px; margin-bottom:10px; display:inline-block">
  <ul class="nav nav-pills">
    <li>               <a href="http://mscoco.org/dataset/#div-cap-c5" data-toggle="tab" aria-expanded="true">Table-C5 </a></li>
    <li>               <a href="http://mscoco.org/dataset/#div-cap-c40" data-toggle="tab" aria-expanded="true">Table-C40</a></li>
    <li class="active"><a href="http://mscoco.org/dataset/#div-cap-challenge2015" data-toggle="tab" aria-expanded="true">Challenge2015</a></li>
  </ul>
</div>

<div class="tab-content" id="captioning">

  <!------------------------------------------------------------------------------------------------>
  <div id="div-cap-c5" style="font-size:13px" class="tab-pane">
    <div style="margin-bottom:20px;">
      Last updated: 07/12/2016. Please visit <a href="https://www.codalab.org/competitions/3221#results" target="_blank">CodaLab</a> for the latest results.
    </div>
    <div id="data_wrapper" class="dataTables_wrapper no-footer"><table id="data" class="table order-column hover dataTable no-footer" role="grid">
      <thead><tr role="row"><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label=": activate to sort column descending" style="width: 0px;"></th><th class="sorting_desc" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="CIDEr-D: activate to sort column ascending" aria-sort="descending" style="width: 0px;">CIDEr-D</th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="Meteor: activate to sort column descending" style="width: 0px;">Meteor</th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="ROUGE-L: activate to sort column descending" style="width: 0px;">ROUGE-L</th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="BLEU-1: activate to sort column descending" style="width: 0px;">BLEU-1</th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="BLEU-2: activate to sort column descending" style="width: 0px;">BLEU-2</th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="BLEU-3: activate to sort column descending" style="width: 0px;">BLEU-3</th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="BLEU-4: activate to sort column descending" style="width: 0px;">BLEU-4</th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="date: activate to sort column descending" style="width: 0px;">date</th></tr></thead>
    <tbody><tr role="row" class="odd"><td>MSM@MSRA<sup>[17]</sup></td><td class="sorting_1">0.984</td><td>0.256</td><td>0.542</td><td>0.739</td><td>0.575</td><td>0.436</td><td>0.33</td><td>2016-06-08</td></tr><tr role="row" class="even"><td>THU_MIG<sup>[27]</sup></td><td class="sorting_1">0.969</td><td>0.251</td><td>0.541</td><td>0.751</td><td>0.583</td><td>0.436</td><td>0.323</td><td>2016-06-03</td></tr><tr role="row" class="odd"><td>AugmentCNNwithDet<sup>[3]</sup></td><td class="sorting_1">0.956</td><td>0.251</td><td>0.531</td><td>0.721</td><td>0.553</td><td>0.416</td><td>0.315</td><td>2016-03-29</td></tr><tr role="row" class="even"><td>ChalLS<sup>[6]</sup></td><td class="sorting_1">0.955</td><td>0.252</td><td>0.531</td><td>0.723</td><td>0.553</td><td>0.414</td><td>0.309</td><td>2016-05-21</td></tr><tr role="row" class="odd"><td>ATT<sup>[2]</sup></td><td class="sorting_1">0.943</td><td>0.25</td><td>0.535</td><td>0.731</td><td>0.565</td><td>0.424</td><td>0.316</td><td>2016-01-23</td></tr><tr role="row" class="even"><td>Google<sup>[9]</sup></td><td class="sorting_1">0.943</td><td>0.254</td><td>0.53</td><td>0.713</td><td>0.542</td><td>0.407</td><td>0.309</td><td>2015-05-29</td></tr><tr role="row" class="odd"><td>Fukun_Jinjunqi<sup>[7]</sup></td><td class="sorting_1">0.939</td><td>0.248</td><td>0.53</td><td>0.722</td><td>0.556</td><td>0.418</td><td>0.314</td><td>2016-05-09</td></tr><tr role="row" class="even"><td>MSRA-MSM<sup>[20]</sup></td><td class="sorting_1">0.933</td><td>0.246</td><td>0.532</td><td>0.736</td><td>0.567</td><td>0.421</td><td>0.311</td><td>2016-03-13</td></tr><tr role="row" class="odd"><td>MSR Captivator<sup>[19]</sup></td><td class="sorting_1">0.931</td><td>0.248</td><td>0.526</td><td>0.715</td><td>0.543</td><td>0.407</td><td>0.308</td><td>2015-05-28</td></tr><tr role="row" class="even"><td>Berkeley LRCN<sup>[4]</sup></td><td class="sorting_1">0.921</td><td>0.247</td><td>0.528</td><td>0.718</td><td>0.548</td><td>0.409</td><td>0.306</td><td>2015-12-08</td></tr><tr role="row" class="odd"><td>m-RNN<sup>[12]</sup></td><td class="sorting_1">0.917</td><td>0.242</td><td>0.521</td><td>0.716</td><td>0.545</td><td>0.404</td><td>0.299</td><td>2015-05-30</td></tr><tr role="row" class="even"><td>MSR<sup>[18]</sup></td><td class="sorting_1">0.912</td><td>0.247</td><td>0.519</td><td>0.695</td><td>0.526</td><td>0.391</td><td>0.291</td><td>2015-04-08</td></tr><tr role="row" class="odd"><td>ACVT<sup>[1]</sup></td><td class="sorting_1">0.911</td><td>0.246</td><td>0.528</td><td>0.725</td><td>0.556</td><td>0.414</td><td>0.306</td><td>2015-08-02</td></tr><tr role="row" class="even"><td>zhaofang_lv<sup>[29]</sup></td><td class="sorting_1">0.906</td><td>0.245</td><td>0.531</td><td>0.719</td><td>0.559</td><td>0.423</td><td>0.318</td><td>2016-05-25</td></tr><tr role="row" class="odd"><td>PicSOM<sup>[24]</sup></td><td class="sorting_1">0.899</td><td>0.243</td><td>0.52</td><td>0.707</td><td>0.536</td><td>0.399</td><td>0.299</td><td>2016-02-10</td></tr><tr role="row" class="even"><td>Tsinghua Bigeye<sup>[28]</sup></td><td class="sorting_1">0.899</td><td>0.247</td><td>0.52</td><td>0.699</td><td>0.527</td><td>0.39</td><td>0.288</td><td>2015-11-29</td></tr><tr role="row" class="odd"><td>m-RNN (Baidu/ UCLA)<sup>[13]</sup></td><td class="sorting_1">0.886</td><td>0.238</td><td>0.524</td><td>0.72</td><td>0.553</td><td>0.41</td><td>0.302</td><td>2015-05-26</td></tr><tr role="row" class="even"><td>Nearest Neighbor<sup>[21]</sup></td><td class="sorting_1">0.886</td><td>0.237</td><td>0.507</td><td>0.697</td><td>0.521</td><td>0.382</td><td>0.28</td><td>2015-05-15</td></tr><tr role="row" class="odd"><td>Montreal/Toronto<sup>[16]</sup></td><td class="sorting_1">0.865</td><td>0.241</td><td>0.516</td><td>0.705</td><td>0.528</td><td>0.383</td><td>0.277</td><td>2015-05-31</td></tr><tr role="row" class="even"><td>Human<sup>[11]</sup></td><td class="sorting_1">0.854</td><td>0.252</td><td>0.484</td><td>0.663</td><td>0.469</td><td>0.321</td><td>0.217</td><td>2015-03-23</td></tr><tr role="row" class="odd"><td>nlab-utokyo<sup>[23]</sup></td><td class="sorting_1">0.833</td><td>0.236</td><td>0.505</td><td>0.692</td><td>0.512</td><td>0.371</td><td>0.268</td><td>2016-03-03</td></tr><tr role="row" class="even"><td>Snapshopr_Research<sup>[26]</sup></td><td class="sorting_1">0.791</td><td>0.23</td><td>0.499</td><td>0.678</td><td>0.498</td><td>0.353</td><td>0.251</td><td>2016-04-23</td></tr><tr role="row" class="odd"><td>MLBL<sup>[15]</sup></td><td class="sorting_1">0.74</td><td>0.219</td><td>0.499</td><td>0.666</td><td>0.498</td><td>0.362</td><td>0.26</td><td>2015-04-10</td></tr><tr role="row" class="even"><td>NeuralTalk<sup>[22]</sup></td><td class="sorting_1">0.674</td><td>0.21</td><td>0.475</td><td>0.65</td><td>0.464</td><td>0.321</td><td>0.224</td><td>2015-04-15</td></tr><tr role="row" class="odd"><td>Shijian Tang<sup>[25]</sup></td><td class="sorting_1">0.671</td><td>0.207</td><td>0.482</td><td>0.662</td><td>0.48</td><td>0.333</td><td>0.229</td><td>2015-08-21</td></tr><tr role="row" class="even"><td>MIL<sup>[14]</sup></td><td class="sorting_1">0.666</td><td>0.214</td><td>0.468</td><td>0.652</td><td>0.472</td><td>0.323</td><td>0.216</td><td>2015-05-29</td></tr><tr role="row" class="odd"><td>Brno University<sup>[5]</sup></td><td class="sorting_1">0.517</td><td>0.195</td><td>0.403</td><td>0.535</td><td>0.339</td><td>0.213</td><td>0.134</td><td>2015-05-29</td></tr><tr role="row" class="even"><td>GabYesh<sup>[8]</sup></td><td class="sorting_1">0.428</td><td>0.162</td><td>0.408</td><td>0.555</td><td>0.36</td><td>0.234</td><td>0.158</td><td>2015-09-24</td></tr><tr role="row" class="odd"><td>HUCVL-METULcsL<sup>[10]</sup></td><td class="sorting_1">0.373</td><td>0.175</td><td>0.356</td><td>0.449</td><td>0.263</td><td>0.153</td><td>0.089</td><td>2015-07-12</td></tr></tbody></table></div>
    <p class="titleSegoeLight" style="margin-bottom:0px">Metrics</p>
    <div id="metrics_wrapper" class="dataTables_wrapper no-footer"><table id="metrics" style="margin-bottom:10px" class="table table-striped hover dataTable no-footer" role="grid">
      <thead><tr role="row"><th class="sorting_disabled" rowspan="1" colspan="1" style="width: 25%;"></th><th class="sorting_disabled" rowspan="1" colspan="1"></th></tr></thead>
    <tbody><tr role="row" class="odd"><td>CIDEr-D</td><td><a href="http://arxiv.org/pdf/1411.5726.pdf" target="_blank"> CIDEr: Consensus-based Image Description Evaluation</a></td></tr><tr role="row" class="even"><td>METEOR</td><td><a href="http://www.cs.cmu.edu/~alavie/METEOR/pdf/meteor-1.5.pdf" target="_blank">Meteor Universal: Language Specific Translation Evaluation for Any Target Language</a></td></tr><tr role="row" class="odd"><td>Rouge-L</td><td><a href="http://anthology.aclweb.org/W/W04/W04-1013.pdf" target="_blank"> ROUGE: A Package for Automatic Evaluation of Summaries</a></td></tr><tr role="row" class="even"><td>BLEU</td><td><a href="http://www.aclweb.org/anthology/P02-1040.pdf" target="_blank"> BLEU: a Method for Automatic Evaluation of Machine Translation</a></td></tr></tbody></table></div>
    <div>
      For the details of data collection and evaluation, please read <a href="http://arxiv.org/pdf/1504.00325.pdf" target="_blank">Microsoft COCO Captions: Data Collection and Evaluation Server</a>.
    </div>
    <p class="titleSegoeLight" style="margin-bottom:0px">References</p>
    <div id="refs_wrapper" class="dataTables_wrapper no-footer"><table id="refs" class="table table-striped hover dataTable no-footer" role="grid">
      <thead><tr role="row"><th class="sorting_disabled" rowspan="1" colspan="1" style="width: 25%;"></th><th class="sorting_disabled" rowspan="1" colspan="1"></th></tr></thead>
    <tbody><tr role="row" class="odd"><td>[1] <a href="http://arxiv.org/abs/1506.01144" target="_blank"> ACVT </a></td><td>Qi Wu, Chunhua Shen, Anton van den Hengel, Lingqiao Liu, Anthony Dick, What value high level concepts in vision to language problems?</td></tr><tr role="row" class="even"><td>[2] <a href="http://arxiv.org/abs/1603.03925" target="_blank"> ATT </a></td><td>Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, Jiebo Luo, Image Captioning with Semantic Attention, CVPR 2016</td></tr><tr role="row" class="odd"><td>[3] AugmentCNNwithDet</td><td></td></tr><tr role="row" class="even"><td>[4] <a href="http://arxiv.org/abs/1411.4389" target="_blank"> Berkeley LRCN </a></td><td>J. Donahue, L. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, T. Darrell, Long-term Recurrent Convolutional Networks for Visual Recognition and Description, CVPR 2015</td></tr><tr role="row" class="odd"><td>[5] <a href="http://arxiv.org/abs/1506.03995" target="_blank"> Brno University </a></td><td>Martin Kolar, Michal Hradis, Pavel Zemcik, Technical Report: Image Captioning with Semantically Similar Images, arXiv 2015</td></tr><tr role="row" class="even"><td>[6] ChalLS</td><td></td></tr><tr role="row" class="odd"><td>[7] Fukun_Jinjunqi</td><td></td></tr><tr role="row" class="even"><td>[8] GabYesh</td><td></td></tr><tr role="row" class="odd"><td>[9] <a href="http://arxiv.org/abs/1411.4555" target="_blank"> Google </a></td><td>Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, Show and Tell: A Neural Image Caption Generator, CVPR 2015</td></tr><tr role="row" class="even"><td>[10] HUCVL-METULcsL</td><td></td></tr><tr role="row" class="odd"><td>[11] <a href="http://arxiv.org/pdf/1504.00325.pdf" target="_blank"> Human </a></td><td>Human Baseline</td></tr><tr role="row" class="even"><td>[12] <a href="http://arxiv.org/abs/1504.06692" target="_blank"> m-RNN </a></td><td>Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan Yuille, Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images. arXiv preprint arXiv:1504.06692 (2015) </td></tr><tr role="row" class="odd"><td>[13] <a href="http://arxiv.org/abs/1412.6632" target="_blank"> m-RNN (Baidu/ UCLA) </a></td><td>Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Yuille, Alan, Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN), arXiv 2014</td></tr><tr role="row" class="even"><td>[14] MIL</td><td></td></tr><tr role="row" class="odd"><td>[15] <a href="http://arxiv.org/pdf/1411.2539.pdf" target="_blank"> MLBL </a></td><td>Multimodal Neural Language Models (Kiros et al, ICML 2014), Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models (Kiros et al, arXiv 2014), and Scalable Bayesian Optimization Using Deep Neural Networks (Snoek et al, arXiv 2015)</td></tr><tr role="row" class="even"><td>[16] <a href="http://arxiv.org/abs/1502.03044" target="_blank"> Montreal/Toronto </a></td><td>Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua, Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, arXiv 2015</td></tr><tr role="row" class="odd"><td>[17] MSM@MSRA</td><td></td></tr><tr role="row" class="even"><td>[18] <a href="http://arxiv.org/abs/1411.4952" target="_blank"> MSR </a></td><td>H. Fang, S. Gupta, F. Iandola, R. Srivastava, L. Deng, P. Dollár, J. Gao, X. He, M. Mitchell, J. Platt, C.L. Zitnick, and G. Zweig, From Captions to Visual Concepts and Back, CVPR 2015</td></tr><tr role="row" class="odd"><td>[19] <a href="http://arxiv.org/abs/1505.01809" target="_blank"> MSR Captivator </a></td><td>Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, Margaret Mitchell, Language Models for Image Captioning: The Quirks and What Works, arXiv 2015</td></tr><tr role="row" class="even"><td>[20] MSRA-MSM</td><td></td></tr><tr role="row" class="odd"><td>[21] <a href="http://arxiv.org/abs/1505.04467" target="_blank"> Nearest Neighbor </a></td><td>Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C. Lawrence Zitnick, Exploring Nearest Neighbor Approaches for Image Captioning, arXiv 2015</td></tr><tr role="row" class="even"><td>[22] <a href="http://arxiv.org/abs/1412.2306" target="_blank"> NeuralTalk </a></td><td>Andrej Karpathy, Li Fei-Fei, Deep Visual-Semantic Alignments for Generating Image Descriptions, CVPR 2015</td></tr><tr role="row" class="odd"><td>[23] nlab-utokyo</td><td></td></tr><tr role="row" class="even"><td>[24] PicSOM</td><td></td></tr><tr role="row" class="odd"><td>[25] Shijian Tang</td><td></td></tr><tr role="row" class="even"><td>[26] Snapshopr_Research</td><td></td></tr><tr role="row" class="odd"><td>[27] THU_MIG</td><td></td></tr><tr role="row" class="even"><td>[28] Tsinghua Bigeye</td><td></td></tr><tr role="row" class="odd"><td>[29] zhaofang_lv</td><td></td></tr></tbody></table></div>
  </div>

  <!------------------------------------------------------------------------------------------------>
  <div id="div-cap-c40" style="font-size:13px" class="tab-pane">
    <div style="margin-bottom:20px;">
      Last updated: 07/12/2016. Please visit <a href="https://www.codalab.org/competitions/3221#results" target="_blank">CodaLab</a> for the latest results.
    </div>
    <div id="data_wrapper" class="dataTables_wrapper no-footer"><table id="data" class="table order-column hover dataTable no-footer" role="grid">
      <thead><tr role="row"><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label=": activate to sort column descending" style="width: 0px;"></th><th class="sorting_desc" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="CIDEr-D: activate to sort column ascending" aria-sort="descending" style="width: 0px;">CIDEr-D</th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="Meteor: activate to sort column descending" style="width: 0px;">Meteor</th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="ROUGE-L: activate to sort column descending" style="width: 0px;">ROUGE-L</th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="BLEU-1: activate to sort column descending" style="width: 0px;">BLEU-1</th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="BLEU-2: activate to sort column descending" style="width: 0px;">BLEU-2</th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="BLEU-3: activate to sort column descending" style="width: 0px;">BLEU-3</th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="BLEU-4: activate to sort column descending" style="width: 0px;">BLEU-4</th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="date: activate to sort column descending" style="width: 0px;">date</th></tr></thead>
    <tbody><tr role="row" class="odd"><td>MSM@MSRA<sup>[17]</sup></td><td class="sorting_1">1.003</td><td>0.35</td><td>0.7</td><td>0.919</td><td>0.842</td><td>0.74</td><td>0.632</td><td>2016-06-08</td></tr><tr role="row" class="even"><td>THU_MIG<sup>[27]</sup></td><td class="sorting_1">0.988</td><td>0.336</td><td>0.688</td><td>0.913</td><td>0.833</td><td>0.727</td><td>0.616</td><td>2016-06-03</td></tr><tr role="row" class="odd"><td>ChalLS<sup>[6]</sup></td><td class="sorting_1">0.97</td><td>0.34</td><td>0.679</td><td>0.898</td><td>0.809</td><td>0.701</td><td>0.59</td><td>2016-05-21</td></tr><tr role="row" class="even"><td>AugmentCNNwithDet<sup>[3]</sup></td><td class="sorting_1">0.968</td><td>0.34</td><td>0.683</td><td>0.905</td><td>0.815</td><td>0.706</td><td>0.597</td><td>2016-03-29</td></tr><tr role="row" class="odd"><td>ATT<sup>[2]</sup></td><td class="sorting_1">0.958</td><td>0.335</td><td>0.682</td><td>0.9</td><td>0.815</td><td>0.709</td><td>0.599</td><td>2016-01-23</td></tr><tr role="row" class="even"><td>MSRA-MSM<sup>[20]</sup></td><td class="sorting_1">0.954</td><td>0.328</td><td>0.677</td><td>0.901</td><td>0.815</td><td>0.705</td><td>0.591</td><td>2016-03-13</td></tr><tr role="row" class="odd"><td>Fukun_Jinjunqi<sup>[7]</sup></td><td class="sorting_1">0.946</td><td>0.336</td><td>0.68</td><td>0.902</td><td>0.817</td><td>0.711</td><td>0.601</td><td>2016-05-09</td></tr><tr role="row" class="even"><td>Google<sup>[9]</sup></td><td class="sorting_1">0.946</td><td>0.346</td><td>0.682</td><td>0.895</td><td>0.802</td><td>0.694</td><td>0.587</td><td>2015-05-29</td></tr><tr role="row" class="odd"><td>MSR Captivator<sup>[19]</sup></td><td class="sorting_1">0.937</td><td>0.339</td><td>0.68</td><td>0.907</td><td>0.819</td><td>0.71</td><td>0.601</td><td>2015-05-28</td></tr><tr role="row" class="even"><td>m-RNN<sup>[12]</sup></td><td class="sorting_1">0.935</td><td>0.325</td><td>0.666</td><td>0.89</td><td>0.798</td><td>0.687</td><td>0.575</td><td>2015-05-30</td></tr><tr role="row" class="odd"><td>Berkeley LRCN<sup>[4]</sup></td><td class="sorting_1">0.934</td><td>0.335</td><td>0.678</td><td>0.895</td><td>0.804</td><td>0.695</td><td>0.585</td><td>2015-12-08</td></tr><tr role="row" class="even"><td>MSR<sup>[18]</sup></td><td class="sorting_1">0.925</td><td>0.331</td><td>0.662</td><td>0.88</td><td>0.789</td><td>0.678</td><td>0.567</td><td>2015-04-08</td></tr><tr role="row" class="odd"><td>ACVT<sup>[1]</sup></td><td class="sorting_1">0.924</td><td>0.329</td><td>0.672</td><td>0.892</td><td>0.803</td><td>0.694</td><td>0.582</td><td>2015-08-02</td></tr><tr role="row" class="even"><td>zhaofang_lv<sup>[29]</sup></td><td class="sorting_1">0.917</td><td>0.335</td><td>0.686</td><td>0.906</td><td>0.828</td><td>0.727</td><td>0.618</td><td>2016-05-25</td></tr><tr role="row" class="odd"><td>Nearest Neighbor<sup>[21]</sup></td><td class="sorting_1">0.916</td><td>0.318</td><td>0.648</td><td>0.872</td><td>0.77</td><td>0.655</td><td>0.542</td><td>2015-05-15</td></tr><tr role="row" class="even"><td>Human<sup>[11]</sup></td><td class="sorting_1">0.91</td><td>0.335</td><td>0.626</td><td>0.88</td><td>0.744</td><td>0.603</td><td>0.471</td><td>2015-03-23</td></tr><tr role="row" class="odd"><td>Tsinghua Bigeye<sup>[28]</sup></td><td class="sorting_1">0.908</td><td>0.332</td><td>0.663</td><td>0.881</td><td>0.783</td><td>0.67</td><td>0.558</td><td>2015-11-29</td></tr><tr role="row" class="even"><td>PicSOM<sup>[24]</sup></td><td class="sorting_1">0.901</td><td>0.328</td><td>0.666</td><td>0.887</td><td>0.792</td><td>0.68</td><td>0.57</td><td>2016-02-10</td></tr><tr role="row" class="odd"><td>m-RNN (Baidu/ UCLA)<sup>[13]</sup></td><td class="sorting_1">0.896</td><td>0.32</td><td>0.668</td><td>0.89</td><td>0.801</td><td>0.69</td><td>0.578</td><td>2015-05-26</td></tr><tr role="row" class="even"><td>Montreal/Toronto<sup>[16]</sup></td><td class="sorting_1">0.893</td><td>0.322</td><td>0.654</td><td>0.881</td><td>0.779</td><td>0.658</td><td>0.537</td><td>2015-05-31</td></tr><tr role="row" class="odd"><td>nlab-utokyo<sup>[23]</sup></td><td class="sorting_1">0.851</td><td>0.316</td><td>0.646</td><td>0.871</td><td>0.762</td><td>0.638</td><td>0.518</td><td>2016-03-03</td></tr><tr role="row" class="even"><td>Snapshopr_Research<sup>[26]</sup></td><td class="sorting_1">0.826</td><td>0.308</td><td>0.636</td><td>0.856</td><td>0.743</td><td>0.615</td><td>0.493</td><td>2016-04-23</td></tr><tr role="row" class="odd"><td>MLBL<sup>[15]</sup></td><td class="sorting_1">0.752</td><td>0.294</td><td>0.635</td><td>0.848</td><td>0.747</td><td>0.633</td><td>0.517</td><td>2015-04-10</td></tr><tr role="row" class="even"><td>NeuralTalk<sup>[22]</sup></td><td class="sorting_1">0.692</td><td>0.28</td><td>0.603</td><td>0.828</td><td>0.701</td><td>0.566</td><td>0.446</td><td>2015-04-15</td></tr><tr role="row" class="odd"><td>MIL<sup>[14]</sup></td><td class="sorting_1">0.69</td><td>0.284</td><td>0.596</td><td>0.827</td><td>0.707</td><td>0.564</td><td>0.432</td><td>2015-05-29</td></tr><tr role="row" class="even"><td>Shijian Tang<sup>[25]</sup></td><td class="sorting_1">0.685</td><td>0.274</td><td>0.61</td><td>0.836</td><td>0.72</td><td>0.59</td><td>0.465</td><td>2015-08-21</td></tr><tr role="row" class="odd"><td>Brno University<sup>[5]</sup></td><td class="sorting_1">0.536</td><td>0.252</td><td>0.509</td><td>0.716</td><td>0.541</td><td>0.392</td><td>0.278</td><td>2015-05-29</td></tr><tr role="row" class="even"><td>GabYesh<sup>[8]</sup></td><td class="sorting_1">0.443</td><td>0.219</td><td>0.524</td><td>0.725</td><td>0.564</td><td>0.427</td><td>0.323</td><td>2015-09-24</td></tr><tr role="row" class="odd"><td>HUCVL-METULcsL<sup>[10]</sup></td><td class="sorting_1">0.399</td><td>0.23</td><td>0.46</td><td>0.639</td><td>0.452</td><td>0.306</td><td>0.202</td><td>2015-07-12</td></tr></tbody></table></div>
    <p class="titleSegoeLight" style="margin-bottom:0px">Metrics</p>
    <div id="metrics_wrapper" class="dataTables_wrapper no-footer"><table id="metrics" style="margin-bottom:10px" class="table table-striped hover dataTable no-footer" role="grid">
      <thead><tr role="row"><th class="sorting_disabled" rowspan="1" colspan="1" style="width: 25%;"></th><th class="sorting_disabled" rowspan="1" colspan="1"></th></tr></thead>
    <tbody><tr role="row" class="odd"><td>CIDEr-D</td><td><a href="http://arxiv.org/pdf/1411.5726.pdf" target="_blank"> CIDEr: Consensus-based Image Description Evaluation</a></td></tr><tr role="row" class="even"><td>METEOR</td><td><a href="http://www.cs.cmu.edu/~alavie/METEOR/pdf/meteor-1.5.pdf" target="_blank">Meteor Universal: Language Specific Translation Evaluation for Any Target Language</a></td></tr><tr role="row" class="odd"><td>Rouge-L</td><td><a href="http://anthology.aclweb.org/W/W04/W04-1013.pdf" target="_blank"> ROUGE: A Package for Automatic Evaluation of Summaries</a></td></tr><tr role="row" class="even"><td>BLEU</td><td><a href="http://www.aclweb.org/anthology/P02-1040.pdf" target="_blank"> BLEU: a Method for Automatic Evaluation of Machine Translation</a></td></tr></tbody></table></div>
    <div>
      For the details of data collection and evaluation, please read <a href="http://arxiv.org/pdf/1504.00325.pdf" target="_blank">Microsoft COCO Captions: Data Collection and Evaluation Server</a>.
    </div>
    <p class="titleSegoeLight" style="margin-bottom:0px">References</p>
    <div id="refs_wrapper" class="dataTables_wrapper no-footer"><table id="refs" class="table table-striped hover dataTable no-footer" role="grid">
      <thead><tr role="row"><th class="sorting_disabled" rowspan="1" colspan="1" style="width: 25%;"></th><th class="sorting_disabled" rowspan="1" colspan="1"></th></tr></thead>
    <tbody><tr role="row" class="odd"><td>[1] <a href="http://arxiv.org/abs/1506.01144" target="_blank"> ACVT </a></td><td>Qi Wu, Chunhua Shen, Anton van den Hengel, Lingqiao Liu, Anthony Dick, What value high level concepts in vision to language problems?</td></tr><tr role="row" class="even"><td>[2] <a href="http://arxiv.org/abs/1603.03925" target="_blank"> ATT </a></td><td>Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, Jiebo Luo, Image Captioning with Semantic Attention, CVPR 2016</td></tr><tr role="row" class="odd"><td>[3] AugmentCNNwithDet</td><td></td></tr><tr role="row" class="even"><td>[4] <a href="http://arxiv.org/abs/1411.4389" target="_blank"> Berkeley LRCN </a></td><td>J. Donahue, L. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, T. Darrell, Long-term Recurrent Convolutional Networks for Visual Recognition and Description, CVPR 2015</td></tr><tr role="row" class="odd"><td>[5] <a href="http://arxiv.org/abs/1506.03995" target="_blank"> Brno University </a></td><td>Martin Kolar, Michal Hradis, Pavel Zemcik, Technical Report: Image Captioning with Semantically Similar Images, arXiv 2015</td></tr><tr role="row" class="even"><td>[6] ChalLS</td><td></td></tr><tr role="row" class="odd"><td>[7] Fukun_Jinjunqi</td><td></td></tr><tr role="row" class="even"><td>[8] GabYesh</td><td></td></tr><tr role="row" class="odd"><td>[9] <a href="http://arxiv.org/abs/1411.4555" target="_blank"> Google </a></td><td>Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, Show and Tell: A Neural Image Caption Generator, CVPR 2015</td></tr><tr role="row" class="even"><td>[10] HUCVL-METULcsL</td><td></td></tr><tr role="row" class="odd"><td>[11] <a href="http://arxiv.org/pdf/1504.00325.pdf" target="_blank"> Human </a></td><td>Human Baseline</td></tr><tr role="row" class="even"><td>[12] <a href="http://arxiv.org/abs/1504.06692" target="_blank"> m-RNN </a></td><td>Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan Yuille, Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images. arXiv preprint arXiv:1504.06692 (2015) </td></tr><tr role="row" class="odd"><td>[13] <a href="http://arxiv.org/abs/1412.6632" target="_blank"> m-RNN (Baidu/ UCLA) </a></td><td>Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Yuille, Alan, Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN), arXiv 2014</td></tr><tr role="row" class="even"><td>[14] MIL</td><td></td></tr><tr role="row" class="odd"><td>[15] <a href="http://arxiv.org/pdf/1411.2539.pdf" target="_blank"> MLBL </a></td><td>Multimodal Neural Language Models (Kiros et al, ICML 2014), Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models (Kiros et al, arXiv 2014), and Scalable Bayesian Optimization Using Deep Neural Networks (Snoek et al, arXiv 2015)</td></tr><tr role="row" class="even"><td>[16] <a href="http://arxiv.org/abs/1502.03044" target="_blank"> Montreal/Toronto </a></td><td>Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua, Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, arXiv 2015</td></tr><tr role="row" class="odd"><td>[17] MSM@MSRA</td><td></td></tr><tr role="row" class="even"><td>[18] <a href="http://arxiv.org/abs/1411.4952" target="_blank"> MSR </a></td><td>H. Fang, S. Gupta, F. Iandola, R. Srivastava, L. Deng, P. Dollár, J. Gao, X. He, M. Mitchell, J. Platt, C.L. Zitnick, and G. Zweig, From Captions to Visual Concepts and Back, CVPR 2015</td></tr><tr role="row" class="odd"><td>[19] <a href="http://arxiv.org/abs/1505.01809" target="_blank"> MSR Captivator </a></td><td>Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, Margaret Mitchell, Language Models for Image Captioning: The Quirks and What Works, arXiv 2015</td></tr><tr role="row" class="even"><td>[20] MSRA-MSM</td><td></td></tr><tr role="row" class="odd"><td>[21] <a href="http://arxiv.org/abs/1505.04467" target="_blank"> Nearest Neighbor </a></td><td>Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C. Lawrence Zitnick, Exploring Nearest Neighbor Approaches for Image Captioning, arXiv 2015</td></tr><tr role="row" class="even"><td>[22] <a href="http://arxiv.org/abs/1412.2306" target="_blank"> NeuralTalk </a></td><td>Andrej Karpathy, Li Fei-Fei, Deep Visual-Semantic Alignments for Generating Image Descriptions, CVPR 2015</td></tr><tr role="row" class="odd"><td>[23] nlab-utokyo</td><td></td></tr><tr role="row" class="even"><td>[24] PicSOM</td><td></td></tr><tr role="row" class="odd"><td>[25] Shijian Tang</td><td></td></tr><tr role="row" class="even"><td>[26] Snapshopr_Research</td><td></td></tr><tr role="row" class="odd"><td>[27] THU_MIG</td><td></td></tr><tr role="row" class="even"><td>[28] Tsinghua Bigeye</td><td></td></tr><tr role="row" class="odd"><td>[29] zhaofang_lv</td><td></td></tr></tbody></table></div>
  </div>

  <!------------------------------------------------------------------------------------------------>
  <div id="div-cap-challenge2015" style="font-size:13px" class="tab-pane active in">
    <div style="margin-bottom:20px;">
      Results of the <a href="http://mscoco.org/dataset/#captions-challenge2015">2015 Captioning Challenge</a>. Finalized 06/2015. See other tabs for up-to-date results.
    </div>
    <div id="data_wrapper" class="dataTables_wrapper no-footer"><table id="data" style="width:70%" class="table order-column hover dataTable no-footer" role="grid">
      <thead><tr role="row"><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label=": activate to sort column descending" style="width: 0px;"></th><th class="sorting_desc" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="M1: activate to sort column ascending" aria-sort="descending" style="width: 0px;">M1</th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="M2: activate to sort column descending" style="width: 0px;">M2</th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="M3: activate to sort column descending" style="width: 0px;">M3</th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="M4: activate to sort column descending" style="width: 0px;">M4</th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="M5: activate to sort column descending" style="width: 0px;">M5</th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="date: activate to sort column descending" style="width: 0px;">date</th></tr></thead>
    <tbody><tr role="row" class="odd"><td>Human<sup>[5]</sup></td><td class="sorting_1">0.638</td><td>0.675</td><td>4.836</td><td>3.428</td><td>0.352</td><td>2015-03-23</td></tr><tr role="row" class="even"><td>Google<sup>[4]</sup></td><td class="sorting_1">0.273</td><td>0.317</td><td>4.107</td><td>2.742</td><td>0.233</td><td>2015-05-29</td></tr><tr role="row" class="odd"><td>MSR<sup>[11]</sup></td><td class="sorting_1">0.268</td><td>0.322</td><td>4.137</td><td>2.662</td><td>0.234</td><td>2015-04-08</td></tr><tr role="row" class="even"><td>Montreal/Toronto<sup>[10]</sup></td><td class="sorting_1">0.262</td><td>0.272</td><td>3.932</td><td>2.832</td><td>0.197</td><td>2015-05-14</td></tr><tr role="row" class="odd"><td>MSR Captivator<sup>[12]</sup></td><td class="sorting_1">0.25</td><td>0.301</td><td>4.149</td><td>2.565</td><td>0.233</td><td>2015-05-28</td></tr><tr role="row" class="even"><td>Berkeley LRCN<sup>[2]</sup></td><td class="sorting_1">0.246</td><td>0.268</td><td>3.924</td><td>2.786</td><td>0.204</td><td>2015-04-25</td></tr><tr role="row" class="odd"><td>m-RNN<sup>[6]</sup></td><td class="sorting_1">0.223</td><td>0.252</td><td>3.897</td><td>2.595</td><td>0.202</td><td>2015-05-30</td></tr><tr role="row" class="even"><td>Nearest Neighbor<sup>[13]</sup></td><td class="sorting_1">0.216</td><td>0.255</td><td>3.801</td><td>2.716</td><td>0.196</td><td>2015-05-15</td></tr><tr role="row" class="odd"><td>PicSOM<sup>[15]</sup></td><td class="sorting_1">0.202</td><td>0.25</td><td>3.965</td><td>2.552</td><td>0.182</td><td>2015-05-26</td></tr><tr role="row" class="even"><td>Brno University<sup>[3]</sup></td><td class="sorting_1">0.194</td><td>0.213</td><td>3.079</td><td>3.482</td><td>0.154</td><td>2015-05-29</td></tr><tr role="row" class="odd"><td>m-RNN (Baidu/ UCLA)<sup>[7]</sup></td><td class="sorting_1">0.19</td><td>0.241</td><td>3.831</td><td>2.548</td><td>0.195</td><td>2015-05-26</td></tr><tr role="row" class="even"><td>MIL<sup>[8]</sup></td><td class="sorting_1">0.168</td><td>0.197</td><td>3.349</td><td>2.915</td><td>0.159</td><td>2015-05-29</td></tr><tr role="row" class="odd"><td>MLBL<sup>[9]</sup></td><td class="sorting_1">0.167</td><td>0.196</td><td>3.659</td><td>2.42</td><td>0.156</td><td>2015-04-10</td></tr><tr role="row" class="even"><td>NeuralTalk<sup>[14]</sup></td><td class="sorting_1">0.166</td><td>0.192</td><td>3.436</td><td>2.742</td><td>0.147</td><td>2015-04-15</td></tr><tr role="row" class="odd"><td>ACVT<sup>[1]</sup></td><td class="sorting_1">0.154</td><td>0.19</td><td>3.516</td><td>2.599</td><td>0.155</td><td>2015-05-26</td></tr><tr role="row" class="even"><td>Tsinghua Bigeye<sup>[17]</sup></td><td class="sorting_1">0.1</td><td>0.146</td><td>3.51</td><td>2.163</td><td>0.116</td><td>2015-04-23</td></tr><tr role="row" class="odd"><td>Random<sup>[16]</sup></td><td class="sorting_1">0.007</td><td>0.02</td><td>1.084</td><td>3.247</td><td>0.013</td><td>2015-05-29</td></tr></tbody></table></div>
    <p class="titleSegoeLight" style="margin-bottom:10px">Metrics</p>
    <div align="justify">
      We conducted a human study to understand how satisfactory are the results obtained from the captions submitted in the COCO captioning challenge. We were interested in three main points:
      <br><br>
      <ul>
        <li> Which algorithm produces the best captions?</li>
        <li> What are the factors determining which is the best algorithm?</li>
        <li> Do the algorithms produce captions resembling human-generated sentences?</li>
      </ul>
      To address the above questions we developed five Graphical User Interfaces (GUI) on the Amazon Mechanical Turk (AMT) platform to collect human responses. From the responses we designed the following five metrics
    </div>
    <div id="metrics_wrapper" class="dataTables_wrapper no-footer"><table id="metrics" class="table table-striped hover dataTable no-footer" role="grid">
      <thead><tr role="row"><th class="sorting_disabled" rowspan="1" colspan="1" style="width: 25%;"></th><th class="sorting_disabled" rowspan="1" colspan="1"></th></tr></thead>
    <tbody><tr role="row" class="odd"><td>M1</td><td>Percentage of captions that are evaluated as better or equal to human caption.</td></tr><tr role="row" class="even"><td>M2</td><td>Percentage of captions that pass the Turing Test.</td></tr><tr role="row" class="odd"><td>M3</td><td>Average correctness of the captions on a scale 1-5 (incorrect - correct).</td></tr><tr role="row" class="even"><td>M4</td><td>Average amount of detail of the captions on a scale 1-5 (lack of details - very detailed).</td></tr><tr role="row" class="odd"><td>M5</td><td>Percentage of captions that are similar to human description.</td></tr></tbody></table></div>
    <p class="titleSegoeLight" style="margin-bottom:10px">Ranking</p>
    <div align="justify">
      The ranking for the competition was based on the results from M1 and M2. The other metrics have been used as diagnostic and interpretation of the results. Points are assigned to the top 5 teams for:
    </div>
    <div id="rank_wrapper" class="dataTables_wrapper no-footer"><table id="rank" style="width:60%" class="table table-striped hover dataTable no-footer" role="grid">
      <thead><tr role="row"><th class="sorting" tabindex="0" aria-controls="rank" rowspan="1" colspan="1" aria-label=": activate to sort column descending" style="width: 0px;"></th><th class="sorting_desc" tabindex="0" aria-controls="rank" rowspan="1" colspan="1" aria-label="M1: activate to sort column ascending" aria-sort="descending" style="width: 0px;">M1</th><th class="sorting" tabindex="0" aria-controls="rank" rowspan="1" colspan="1" aria-label="M2: activate to sort column descending" style="width: 0px;">M2</th><th class="sorting" tabindex="0" aria-controls="rank" rowspan="1" colspan="1" aria-label="TOTAL: activate to sort column descending" style="width: 0px;">TOTAL</th><th class="sorting" tabindex="0" aria-controls="rank" rowspan="1" colspan="1" aria-label="Ranking: activate to sort column descending" style="width: 0px;">Ranking</th></tr></thead>
    <tbody><tr role="row" class="odd"><td>Google</td><td class="sorting_1">5</td><td>4</td><td>9</td><td>1st(tie)</td></tr><tr role="row" class="even"><td>MSR</td><td class="sorting_1">4</td><td>5</td><td>9</td><td>1st(tie)</td></tr><tr role="row" class="odd"><td>Montreal/Toronto</td><td class="sorting_1">3</td><td>2</td><td>5</td><td>3rd(tie)</td></tr><tr role="row" class="even"><td>MSR Captivator</td><td class="sorting_1">2</td><td>3</td><td>5</td><td>3rd(tie)</td></tr><tr role="row" class="odd"><td>Berkeley LRCN</td><td class="sorting_1">1</td><td>1</td><td>2</td><td>5th</td></tr></tbody></table></div>
    <p class="titleSegoeLight" style="margin-bottom:0px">References</p>
    <div id="refs_wrapper" class="dataTables_wrapper no-footer"><table id="refs" class="table table-striped hover dataTable no-footer" role="grid">
      <thead><tr role="row"><th class="sorting_disabled" rowspan="1" colspan="1" style="width: 25%;"></th><th class="sorting_disabled" rowspan="1" colspan="1"></th></tr></thead>
    <tbody><tr role="row" class="odd"><td>[1] ACVT</td><td></td></tr><tr role="row" class="even"><td>[2] <a href="http://arxiv.org/abs/1411.4389" target="_blank"> Berkeley LRCN </a></td><td>J. Donahue, L. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, T. Darrell, Long-term Recurrent Convolutional Networks for Visual Recognition and Description, CVPR 2015</td></tr><tr role="row" class="odd"><td>[3] <a href="http://arxiv.org/abs/1506.03995" target="_blank"> Brno University </a></td><td>Martin Kolar, Michal Hradis, Pavel Zemcik, Technical Report: Image Captioning with Semantically Similar Images, arXiv 2015</td></tr><tr role="row" class="even"><td>[4] <a href="http://arxiv.org/abs/1411.4555" target="_blank"> Google </a></td><td>Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, Show and Tell: A Neural Image Caption Generator, CVPR 2015</td></tr><tr role="row" class="odd"><td>[5] <a href="http://arxiv.org/pdf/1504.00325.pdf" target="_blank"> Human </a></td><td>Human Baseline</td></tr><tr role="row" class="even"><td>[6] <a href="http://arxiv.org/abs/1504.06692" target="_blank"> m-RNN </a></td><td>Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan Yuille, Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images. arXiv preprint arXiv:1504.06692 (2015) </td></tr><tr role="row" class="odd"><td>[7] <a href="http://arxiv.org/abs/1412.6632" target="_blank"> m-RNN (Baidu/ UCLA) </a></td><td>Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Yuille, Alan, Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN), arXiv 2014</td></tr><tr role="row" class="even"><td>[8] MIL</td><td></td></tr><tr role="row" class="odd"><td>[9] <a href="http://arxiv.org/pdf/1411.2539.pdf" target="_blank"> MLBL </a></td><td>Multimodal Neural Language Models (Kiros et al, ICML 2014), Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models (Kiros et al, arXiv 2014), and Scalable Bayesian Optimization Using Deep Neural Networks (Snoek et al, arXiv 2015)</td></tr><tr role="row" class="even"><td>[10] <a href="http://arxiv.org/abs/1502.03044" target="_blank"> Montreal/Toronto </a></td><td>Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua, Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, arXiv 2015</td></tr><tr role="row" class="odd"><td>[11] <a href="http://arxiv.org/abs/1411.4952" target="_blank"> MSR </a></td><td>H. Fang, S. Gupta, F. Iandola, R. Srivastava, L. Deng, P. Dollár, J. Gao, X. He, M. Mitchell, J. Platt, C.L. Zitnick, and G. Zweig, From Captions to Visual Concepts and Back, CVPR 2015</td></tr><tr role="row" class="even"><td>[12] <a href="http://arxiv.org/abs/1505.01809" target="_blank"> MSR Captivator </a></td><td>Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, Margaret Mitchell, Language Models for Image Captioning: The Quirks and What Works, arXiv 2015</td></tr><tr role="row" class="odd"><td>[13] <a href="http://arxiv.org/abs/1505.04467" target="_blank"> Nearest Neighbor </a></td><td>Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C. Lawrence Zitnick, Exploring Nearest Neighbor Approaches for Image Captioning, arXiv 2015</td></tr><tr role="row" class="even"><td>[14] <a href="http://arxiv.org/abs/1412.2306" target="_blank"> NeuralTalk </a></td><td>Andrej Karpathy, Li Fei-Fei, Deep Visual-Semantic Alignments for Generating Image Descriptions, CVPR 2015</td></tr><tr role="row" class="odd"><td>[15] PicSOM</td><td></td></tr><tr role="row" class="even"><td>[16] Random</td><td></td></tr><tr role="row" class="odd"><td>[17] Tsinghua Bigeye</td><td></td></tr></tbody></table></div>
  </div>

</div>
<br>
<br>
</div>
            <div role="tabpanel" class="tab-pane fade" id="keypoints-leaderboard">      <script>
var initKptLeaderboard = function() {
  // constants and global variables
  var types = ["kpt_challenge2016", "kpt_standard", "kpt_dev"];
  var metrics = ["AP", "AP_50", "AP_75", "AP_medium", "AP_large", "AR", "AR_50", "AR_75", "AR_medium", "AR_large"];
  var N=types.length, current={type:'kpt_challenge2016',team:-1}, data=new Array(N);
  for( var i=0; i<N; i++ ) data[i]={ main:[], refs:[], teams:[], cats:[] };
  // extract main data for leaderboard for all teams
  var loadData = function( entries ) {
    for( var i=0; i<N; i++ ) {
      for( var j=0; j<entries[types[i]].length; j++ ) {
        var e=entries[types[i]][j], name=e.team.name, r=JSON.parse(e.results);
        data[i].main[j] = new Array(2+metrics.length); data[i].main[j][0]=name;
        for( var k=0; k<metrics.length; k++ ) data[i].main[j][k+1] = r[metrics[k]];
        data[i].main[j][metrics.length+1] = e.date;
        if(e.url != "") name = "<a href='" + e.url + "' target='_blank'> " + name + " </a>";
        data[i].refs[j] = ["["+(j+1)+"] "+name, e.team.members, e.description];
        data[i].teams[j]=e.team.id; data[i].cats[j]=[];
      }
    }
    setData(current.type,current.team);
  };
  // function to set the data shown in the leaderboard and event binders
  var bindSetType=function(e,type) { var t=type; e.click( function(){setData(t,current.team)} )};
  var bindSetTeam=function(e,team) { var t=team; e.click( function(){setData(current.type,t)} )};
  for( var i=0; i<N; i++ ) bindSetType($('#a_'+types[i]),types[i]);
  var setData = function( type, team ) {
    var i=$.inArray(type,types), j=$.inArray(team,data[i].teams);
    if(j==-1) team=-1; current.type=type; current.team=team;
    tableRefs.clear().rows.add(data[i].refs).draw();
    if( team==-1 ) {
      tableData.clear().rows.add(data[i].main).draw();
    } else if( data[i].cats[j].length>0 ) {
      tableData.clear().rows.add(data[i].cats[j]).draw();
      //bindSetTeam($('#clearTeam'),-1);
    }
  }
  // initialize and format DataTables https://www.datatables.net/
  var propsRefs = { 'paging':false, 'info':false, 'filter':false, 'sort':false, 'autoWidth':false };
  propsRefs.columnDefs = [{'targets':0,'sWidth':'20%'}];
  var propsData = { 'paging':false, 'info':false, 'order':[[1, 'desc']], 'dom':'fB<"toolbar_kpt">rtip' };
  propsData.buttons=[{extend:'copyHtml5',title:'leaderboard',exportOptions:{orthogonal:'export'},text:'Copy to Clipboard'}, {extend:'csvHtml5', title:'leaderboard',exportOptions:{orthogonal:'export'},text:'Export to CSV'}];
  propsData.columnDefs = [{'targets':0,'render':function(x,type,row,meta) {return (type==='export')?x:(current.team===-1)?x+'<sup>'+(meta.row+1)+'</sup>':'<span style="font-size:11px">'+x+'</span>'}}, {'targets':11,'render':function(x) {return '<span style="font-size:10px">'+x+'</span>'}}, {'targets':['_all'],'orderSequence':['desc', 'asc']}];
  var tableData = $('#leaderboard_kpt #data').DataTable(propsData);
  var tableRefs = $('#leaderboard_kpt #refs').DataTable(propsRefs);
  $('div.toolbar_kpt').css({'text-align':'center','padding-top':'4px','font-size':'14px'})
  // load core leaderboard data
  $.ajax({ type:'POST', url:'/leaderboard_get_kpts_entries/', data:{'team_id':0}})
    .done( function(json) { 
            loadData(JSON.parse(json).entries) 
            });
};
$( function() { initKptLeaderboard(); } );
</script>

<!------------------------------------------------------------------------------------------------>
<style>
  #leaderboard td { padding: 6px 4px; vertical-align:middle }
  #leaderboard th { padding: 6px 16px 6px 0px; text-align:right; width:36px }
</style>
<div align="justify" style="font-size:13px; margin:10px;">
</div>
<div style="margin:10px 0px; display:inline-block">
  <ul class="nav nav-pills">
    <li class="active"><a data-toggle="tab" style="padding:10px" id="a_kpt_standard">Standard</a></li>
    <li><a data-toggle="tab" style="padding:10px" id="a_kpt_dev">Dev</a></li>
    <li><a data-toggle="tab" style="padding:10px" id="a_kpt_challenge2016">Challenge2016</a></li>
  </ul>
</div>
<div class="tab-content" id="leaderboard_kpt" style="font-size:12px">
  <div id="data_wrapper" class="dataTables_wrapper no-footer"><div id="data_filter" class="dataTables_filter"><label>Search:<input type="search" class="" placeholder="" aria-controls="data"></label></div><div class="dt-buttons"><a class="dt-button buttons-copy buttons-html5" tabindex="0" aria-controls="data"><span>Copy to Clipboard</span></a><a class="dt-button buttons-csv buttons-html5" tabindex="0" aria-controls="data"><span>Export to CSV</span></a></div><div class="toolbar_kpt" style="text-align: center; padding-top: 4px; font-size: 14px;"></div><table id="data" class="table order-column hover dataTable no-footer" role="grid">
    <thead>
      <tr role="row"><th style="width:200px" class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label=": activate to sort column descending"></th><th class="sorting_desc" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="AP: activate to sort column ascending" aria-sort="descending" style="width: 0px;">AP</th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="AP50: activate to sort column descending" style="width: 0px;">AP<sup>50</sup></th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="AP75: activate to sort column descending" style="width: 0px;">AP<sup>75</sup></th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="APM: activate to sort column descending" style="width: 0px;">AP<sup>M</sup></th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="APL: activate to sort column descending" style="width: 0px;">AP<sup>L</sup></th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="AR: activate to sort column descending" style="width: 0px;">AR</th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="AR50: activate to sort column descending" style="width: 0px;">AR<sup>50</sup></th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="AR75: activate to sort column descending" style="width: 0px;">AR<sup>75</sup></th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="ARM: activate to sort column descending" style="width: 0px;">AR<sup>M</sup></th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="ARL: activate to sort column descending" style="width: 0px;">AR<sup>L</sup></th><th style="width:70px" class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="date: activate to sort column descending">date</th></tr>
    </thead>
  <tbody><tr role="row" class="odd"><td>CMU-Pose<sup>3</sup></td><td class="sorting_1">0.605</td><td>0.834</td><td>0.664</td><td>0.551</td><td>0.681</td><td>0.659</td><td>0.864</td><td>0.713</td><td>0.594</td><td>0.748</td><td><span style="font-size:10px">2016-09-16</span></td></tr><tr role="row" class="even"><td>G-RMI<sup>5</sup></td><td class="sorting_1">0.598</td><td>0.81</td><td>0.651</td><td>0.567</td><td>0.667</td><td>0.664</td><td>0.865</td><td>0.712</td><td>0.618</td><td>0.726</td><td><span style="font-size:10px">2016-09-16</span></td></tr><tr role="row" class="odd"><td>DL-61<sup>4</sup></td><td class="sorting_1">0.533</td><td>0.751</td><td>0.485</td><td>0.555</td><td>0.548</td><td>0.708</td><td>0.828</td><td>0.688</td><td>0.74</td><td>0.782</td><td><span style="font-size:10px">2016-09-16</span></td></tr><tr role="row" class="even"><td>R4D<sup>6</sup></td><td class="sorting_1">0.497</td><td>0.743</td><td>0.545</td><td>0.456</td><td>0.556</td><td>0.556</td><td>0.773</td><td>0.603</td><td>0.491</td><td>0.644</td><td><span style="font-size:10px">2016-09-16</span></td></tr><tr role="row" class="odd"><td>umich_vl<sup>7</sup></td><td class="sorting_1">0.434</td><td>0.722</td><td>0.449</td><td>0.364</td><td>0.534</td><td>0.499</td><td>0.758</td><td>0.52</td><td>0.387</td><td>0.652</td><td><span style="font-size:10px">2016-09-16</span></td></tr><tr role="row" class="even"><td>belagian<sup>1</sup></td><td class="sorting_1">0.4</td><td>0.604</td><td>0.433</td><td>0.279</td><td>0.569</td><td>0.441</td><td>0.623</td><td>0.478</td><td>0.3</td><td>0.631</td><td><span style="font-size:10px">2016-09-16</span></td></tr><tr role="row" class="odd"><td>Caltech<sup>2</sup></td><td class="sorting_1">0.389</td><td>0.637</td><td>0.404</td><td>0.333</td><td>0.482</td><td>0.527</td><td>0.79</td><td>0.553</td><td>0.435</td><td>0.651</td><td><span style="font-size:10px">2016-09-16</span></td></tr><tr role="row" class="even"><td>zjhuang<sup>9</sup></td><td class="sorting_1">0.105</td><td>0.3</td><td>0.056</td><td>0.082</td><td>0.141</td><td>0.19</td><td>0.439</td><td>0.142</td><td>0.108</td><td>0.301</td><td><span style="font-size:10px">2016-09-16</span></td></tr><tr role="row" class="odd"><td>USSR<sup>8</sup></td><td class="sorting_1">0.017</td><td>0.081</td><td>0.006</td><td>0.017</td><td>0.021</td><td>0.072</td><td>0.255</td><td>0.022</td><td>0.069</td><td>0.076</td><td><span style="font-size:10px">2016-09-16</span></td></tr></tbody></table></div>
  <p class="titleSegoeLight" style="margin-bottom:15px">Metrics</p>
  <div class="bodyNormal json" style="color:black; width:100%; font-family:courier new; ">
    <div style="display:inline-block; width:30%">
      <b>Average Precision (AP):</b><br>
      <div class="jsontab">AP<br></div>
      <div class="jsontab">AP<sup>IoU=.50</sup><br></div>
      <div class="jsontab">AP<sup>IoU=.75</sup><br></div>
      <b>AP Across Scales:</b><br>
      <div class="jsontab">AP<sup>medium</sup><br></div>
      <div class="jsontab">AP<sup>large</sup><br></div>
      <b>Average Recall (AR):</b><br>
      <div class="jsontab">AR<br></div>
      <div class="jsontab">AR<sup>IoU=.50</sup><br></div>
      <div class="jsontab">AR<sup>IoU=.75</sup><br></div>
      <b>AR Across Scales:</b><br>
      <div class="jsontab">AR<sup>medium</sup><br></div>
      <div class="jsontab">AR<sup>large</sup><br></div>
    </div>
    <div style="display:inline-block; width:74%; margin-left:-35px">
      <div class="jsontab">% AP at IoU=.50:.05:.95 <b>(primary challenge metric)</b></div>
      <div class="jsontab">% AP at IoU=.50 </div>
      <div class="jsontab">% AP at IoU=.75 (strict metric)</div>
      <br>
      <div class="jsontab">% AP for medium objects: 32<sup>2</sup> &lt; area &lt; 96<sup>2</sup></div>
      <div class="jsontab">% AP for large objects: area &gt; 96<sup>2</sup></div>
      <br>
      <div class="jsontab">% AR at IoU=.50:.05:.95 </div>
      <div class="jsontab">% AR at IoU=.50 </div>
      <div class="jsontab">% AR at IoU=.75 </div>
      <br>
      <div class="jsontab">% AR for medium objects: 32<sup>2</sup> &lt; area &lt; 96<sup>2</sup></div>
      <div class="jsontab">% AR for large objects: area &gt; 96<sup>2</sup></div>
    </div>
  </div>
  <p>Please see the <a href="http://mscoco.org/dataset/#keypoints-eval">evaluation</a> page for more detailed information about the metrics.</p>
  <p class="titleSegoeLight" style="margin-bottom:0px">References</p>
  <div id="refs_wrapper" class="dataTables_wrapper no-footer"><table id="refs" class="table table-striped hover dataTable no-footer" style="font-size:13px" role="grid">
    <thead><tr role="row"><th class="sorting_disabled" rowspan="1" colspan="1" style="width: 20%;"></th><th class="sorting_disabled" rowspan="1" colspan="1"></th><th class="sorting_disabled" rowspan="1" colspan="1"></th></tr></thead>
  <tbody><tr role="row" class="odd"><td>[1] belagian</td><td></td><td></td></tr><tr role="row" class="even"><td>[2] Caltech</td><td>Grant Van Horn, Matteo Ruggero Ronchi, David Hall</td><td>A pipeline approach with Multibox [1] to detect people and Stacked Hourglass Networks [2] to estimate their pose. [1] Scalable High Quality Object Detection. Erhan et al., CVPR 2014 [2] Stacked Hourglass Networks for Human Pose Estimation. Newell et al., POCV 2016</td></tr><tr role="row" class="odd"><td>[3] CMU-Pose</td><td>Zhe Cao, Shih-En Wei, Tomas Simon, Yaser Sheikh</td><td>We present a bottom-up method that first detects individual human joints and then connects joints to form bodies using a limb affinity score. Both the joint detections and limb affinity scores are predicted jointly by a fully convolutional neural network, based on the iterated prediction architecture of convolutional pose machines. A greedy algorithm then connects limbs into subsets to form individual person detections. The system was trained only on MSCOCO data.</td></tr><tr role="row" class="even"><td>[4] DL-61</td><td>Shaoli Huang, Dacheng Tao</td><td>The main idea of our method is we first samples a small number of representable pixels and then determine their labels via a convolutional layer followed by a softmax layer. We develop a CNN architecture to predict the key points from different layers, then fuse all the side predictions as the final prediction.</td></tr><tr role="row" class="odd"><td>[5] G-RMI</td><td>Nori Kanazawa*, George Papandreou*, Alex Toshev*, Tyler Zhu*, Hartwig Adam, Chris Bregler, Kevin Murphy, Jonathan Tompson, (alphabetical order, core system developers marked with asterisk)</td><td>The G-RMI detection entry is a two-stage system consisting of a bounding box person detector followed by a human pose estimator. For person bounding box detection we use a combination of G-RMI's generic ResNet-Inception-based object detector trained only on MS-COCO data, as well as a person-specific box detector trained on both MS-COCO and in-house data. For pose estimation we use a single ResNet-101 based model trained to predict dense keypoint heatmap and offset fields, whose fusion yields highly localized keypoint activation maps which are used for both keypoint localization and proposal rescoring. The pose estimator has been trained on both MS-COCO and in-house data.</td></tr><tr role="row" class="even"><td>[6] R4D</td><td>Yuxiang Peng, Gang Yu</td><td>1. Use faster RCNN to generate a lot of bounding boxes.  2. Use single-person pose estimator to generate keypoints regression.</td></tr><tr role="row" class="odd"><td>[7] <a href="https://github.com/anewell/pose-hg-train" target="_blank"> umich_vl </a></td><td>Alejandro Newell, Jia Deng</td><td>A single network for multi-person keypoint localization based on the stacked hourglass design (Newell et al. ECCV 2016). No additional training data or separate person detection is used. For each body joint at each pixel location, our network predicts a detection score, plus a scalar score that serves as a “tag”. Body joints with similar tags are grouped together to form individual persons. The detection scores and tags are jointly trained.</td></tr><tr role="row" class="even"><td>[8] <a href="https://github.com/NaasCraft/SSR_COCO" target="_blank"> USSR </a></td><td>Raphael Canyasse, Guillaume Demonet, Charles Sutton</td><td>We have implemented a « specialist model » that specializes its last layes on specific body parts. To do so we have stacked in parrallel, block of convolutional layers on features map of a pertained model (VGG16), and we have jointly trained these regressor on the task of keypoint detection.</td></tr><tr role="row" class="odd"><td>[9] zjhuang</td><td></td><td></td></tr></tbody></table></div>
</div>
<br>
<br>
</div>
            <div role="tabpanel" class="tab-pane fade" id="detections-leaderboard">     <script>
var initLeaderboard = function() {
  // constants and global variables
  var types = ["bbox_challenge2016", "segm_challenge2016", "bbox_challenge2015", "segm_challenge2015", "bbox_standard2015", "segm_standard2015", "bbox_dev2015", "segm_dev2015"];
  var metrics = ["AP", "AP_50", "AP_75", "AP_small", "AP_medium", "AP_large", "AR_max_1", "AR_max_10", "AR_max_100", "AR_small", "AR_medium", "AR_large"];
  var supercats = ["accessory", "animal", "appliance", "electronic", "food", "furniture", "indoor", "kitchen", "outdoor", "person", "sports", "vehicle"];
  var cats =[[9,"person"], [11,"bicycle"], [11,"car"], [11,"motorcycle"], [11,"airplane"], [11,"bus"], [11,"train"], [11,"truck"], [11,"boat"], [8,"traffic light"], [8,"fire hydrant"], [8,"stop sign"], [8,"parking meter"], [8,"bench"], [1,"bird"], [1,"cat"], [1,"dog"], [1,"horse"], [1,"sheep"], [1,"cow"], [1,"elephant"], [1,"bear"], [1,"zebra"], [1,"giraffe"], [0,"backpack"], [0,"umbrella"], [0,"handbag"], [0,"tie"], [0,"suitcase"], [10,"frisbee"], [10,"skis"], [10,"snowboard"], [10,"sports ball"], [10,"kite"], [10,"baseball bat"], [10,"baseball glove"], [10,"skateboard"], [10,"surfboard"], [10,"tennis racket"], [7,"bottle"], [7,"wine glass"], [7,"cup"], [7,"fork"], [7,"knife"], [7,"spoon"], [7,"bowl"], [4,"banana"], [4,"apple"], [4,"sandwich"], [4,"orange"], [4,"broccoli"], [4,"carrot"], [4,"hot dog"], [4,"pizza"], [4,"donut"], [4,"cake"], [5,"chair"], [5,"couch"], [5,"potted plant"], [5,"bed"], [5,"dining table"], [5,"toilet"], [3,"tv"], [3,"laptop"], [3,"mouse"], [3,"remote"], [3,"keyboard"], [3,"cell phone"], [2,"microwave"], [2,"oven"], [2,"toaster"], [2,"sink"], [2,"refrigerator"], [6,"book"], [6,"clock"], [6,"vase"], [6,"scissors"], [6,"teddy bear"], [6,"hair drier"], [6,"toothbrush"]];
  var N=types.length, current={type:'bbox_standard2015',team:-1}, data=new Array(N);
  for( var i=0; i<N; i++ ) data[i]={ main:[], refs:[], teams:[], cats:[] };
  // extract main data for leaderboard for all teams
  var loadData = function( entries ) {
    for( var i=0; i<N; i++ ) {
      for( var j=0; j<entries[types[i]].length; j++ ) {
        var e=entries[types[i]][j], name=e.team.name, r=JSON.parse(e.results);
        data[i].main[j] = new Array(2+metrics.length); data[i].main[j][0]=name;
        for( var k=0; k<metrics.length; k++ ) data[i].main[j][k+1] = r[metrics[k]];
        data[i].main[j][metrics.length+1] = e.date;
        if(e.url != "") name = "<a href='" + e.url + "' target='_blank'> " + name + " </a>";
        data[i].refs[j] = ["["+(j+1)+"] "+name, e.team.members, e.description];
        data[i].teams[j]=e.team.id; data[i].cats[j]=[];
      }
    }
    setData(current.type,current.team);
  };
  // extract category data for leaderboard for current team
  var loadCats = function( entries ) {
    for( var i=0; i<N; i++ ) {
      var j, d, K, r, c;
      j=$.inArray(current.team,data[i].teams); if(j==-1) continue;
      d=JSON.parse(entries[types[i]][0].results_details);
      K=cats.length+supercats.length; data[i].cats[j]=new Array(K);
      for( var k=0; k<K; k++ ) {
        if( k<supercats.length ) {
          c=supercats[k]; r=d.per_supercategory[c]; c+=": [all]";
        } else {
          c=cats[k-supercats.length]; r=d.per_category[c[1]]; c=supercats[c[0]]+": "+c[1];
        }
        data[i].cats[j][k]=new Array(2+metrics.length); data[i].cats[j][k][0]=c;
        for( var l=0; l<metrics.length; l++ ) data[i].cats[j][k][l+1]=r[l];
        data[i].cats[j][k][metrics.length+1] = data[i].main[j][metrics.length+1];
      }
      data[i].cats[j][K]=[]; data[i].cats[j][K][0]="[all]: [all]";
      for(var l=1; l<metrics.length+2; l++) data[i].cats[j][K][l]=data[i].main[j][l];
    }
    setData(current.type,current.team);
  }
  // function to set the data shown in the leaderboard and event binders
  var bindSetType=function(e,type) { var t=type; e.click( function(){setData(t,current.team)} )};
  var bindSetTeam=function(e,team) { var t=team; e.click( function(){setData(current.type,t)} )};
  for( var i=0; i<N; i++ ) bindSetType($('#a_'+types[i]),types[i]);
  var setData = function( type, team ) {
    var i=$.inArray(type,types), j=$.inArray(team,data[i].teams);
    if(j==-1) team=-1; current.type=type; current.team=team;
    tableRefs.clear().rows.add(data[i].refs).draw();
    if( team==-1 ) {
      $('div.toolbar').html('');
      tableData.clear().rows.add(data[i].main).draw();
    } else if( data[i].cats[j].length>0 ) {
      $('div.toolbar').html('<a id="clearTeam"><b>&#8629; '+data[i].main[j][0]+'</a><sup>'+(j+1)+'</sup>');
      tableData.clear().rows.add(data[i].cats[j]).draw(); bindSetTeam($('#clearTeam'),-1);
    } else {
      $.ajax({ type:'POST', url:'/leaderboard_get_dets_entries/',data:{'team_id':team}})
        .done( function(json) { loadCats(JSON.parse(json).entries) });
    }
  }
  // initialize and format DataTables https://www.datatables.net/
  var propsRefs = { 'paging':false, 'info':false, 'filter':false, 'sort':false, 'autoWidth':false };
  propsRefs.columnDefs = [{'targets':0,'sWidth':'20%'}];
  var propsData = { 'paging':false, 'info':false, 'order':[[1, 'desc']], 'dom':'fB<"toolbar">rtip' };
  propsData.buttons=[{extend:'copyHtml5',title:'leaderboard',exportOptions:{orthogonal:'export'},text:'Copy to Clipboard'}, {extend:'csvHtml5', title:'leaderboard',exportOptions:{orthogonal:'export'},text:'Export to CSV'}];
  propsData.columnDefs = [{'targets':0,'render':function(x,type,row,meta) {return (type==='export')?x:(current.team===-1)?'<a id="setTeam'+meta.row+'" style="font-size:12px">'+x+'</a><sup>'+(meta.row+1)+'</sup>':'<span style="font-size:11px">'+x+'</span>'}}, {'targets':13,'render':function(x) {return '<span style="font-size:10px">'+x+'</span>'}}, {'targets':['_all'],'orderSequence':['desc', 'asc']}];
  propsData.drawCallback = function() {
    if(current.team>0) return; var teams=data[$.inArray(current.type,types)].teams;
    for( var k=0; k<teams.length; k++) bindSetTeam($('#setTeam'+k),teams[k])
  }
  var tableData = $('#leaderboard #data').DataTable(propsData);
  var tableRefs = $('#leaderboard #refs').DataTable(propsRefs);
  $('div.toolbar').css({'text-align':'center','padding-top':'4px','font-size':'14px'})
  // load core leaderboard data
  $.ajax({ type:'POST', url:'/leaderboard_get_dets_entries/', data:{'team_id':0}})
    .done( function(json) { loadData(JSON.parse(json).entries) });
};
$( function() { initLeaderboard(); } );
</script>

<!------------------------------------------------------------------------------------------------>
<style>
  #leaderboard td { padding: 6px 4px; vertical-align:middle }
  #leaderboard th { padding: 6px 16px 6px 0px; text-align:right; width:36px }
</style>
<div align="justify" style="font-size:13px; margin:10px;">
  Updated: 04/01/2016 (results migrated weekly from <a href="https://www.codalab.org/competitions/5181" target="_blank">CodaLab</a>). For information about each test split please see the <a href="http://mscoco.org/dataset/#detections-upload">upload</a> page. Note that the <a href="http://mscoco.org/dataset/#detections-challenge2015">2015 Detection Challenge</a> results are final as of 12/2015, see the other tabs for up-to-date results. <b>Hints</b>: use the buttons below to select the dataset <i>split</i>, click on column headers to <i>sort</i> the data, use the <i>search</i> bar to filter results, use the buttons to <i>export</i> the current table view, and click on method names to show <i>per-category</i> results.
</div>
<div style="margin:10px 0px; display:inline-block">
  <ul class="nav nav-pills">
    <li><a style="margin-left:-10px; color:black; padding:5px;font-size:small">BBOX:</a></li>
    <li class="active"><a data-toggle="tab" style="padding:5px;font-size:small" id="a_bbox_standard2015">Standard</a></li>
    <li><a data-toggle="tab" style="padding:5px;font-size:small" id="a_bbox_dev2015">Dev</a></li>
    <li><a data-toggle="tab" style="padding:5px;font-size:small" id="a_bbox_challenge2015">Challenge2015</a></li>
    <li><a data-toggle="tab" style="padding:5px;font-size:small" id="a_bbox_challenge2016">Challenge2016</a></li>
    <li><a style="margin-left: 20px; color:black; padding:5px;font-size:small">SEGM:</a></li>
    <li><a data-toggle="tab" style="padding:5px;font-size:small" id="a_segm_standard2015">Standard</a></li>
    <li><a data-toggle="tab" style="padding:5px;font-size:small" id="a_segm_dev2015">Dev</a></li>
    <li><a data-toggle="tab" style="padding:5px;font-size:small" id="a_segm_challenge2015">Challenge2015</a></li>
    <li><a data-toggle="tab" style="padding:5px;font-size:small" id="a_segm_challenge2016">Challenge2016</a></li>
  </ul>
</div>
<div class="tab-content" id="leaderboard" style="font-size:12px">
  <div id="data_wrapper" class="dataTables_wrapper no-footer"><div id="data_filter" class="dataTables_filter"><label>Search:<input type="search" class="" placeholder="" aria-controls="data"></label></div><div class="dt-buttons"><a class="dt-button buttons-copy buttons-html5" tabindex="0" aria-controls="data"><span>Copy to Clipboard</span></a><a class="dt-button buttons-csv buttons-html5" tabindex="0" aria-controls="data"><span>Export to CSV</span></a></div><div class="toolbar" style="text-align: center; padding-top: 4px; font-size: 14px;"></div><table id="data" class="table order-column hover dataTable no-footer" role="grid">
    <thead>
      <tr role="row"><th style="width:200px" class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label=": activate to sort column descending"></th><th class="sorting_desc" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="AP: activate to sort column ascending" aria-sort="descending" style="width: 36px;">AP</th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="AP50: activate to sort column descending" style="width: 36px;">AP<sup>50</sup></th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="AP75: activate to sort column descending" style="width: 36px;">AP<sup>75</sup></th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="APS: activate to sort column descending" style="width: 36px;">AP<sup>S</sup></th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="APM: activate to sort column descending" style="width: 36px;">AP<sup>M</sup></th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="APL: activate to sort column descending" style="width: 36px;">AP<sup>L</sup></th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="AR1: activate to sort column descending" style="width: 36px;">AR<sup>1</sup></th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="AR10: activate to sort column descending" style="width: 36px;">AR<sup>10</sup></th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="AR100: activate to sort column descending" style="width: 36px;">AR<sup>100</sup></th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="ARS: activate to sort column descending" style="width: 36px;">AR<sup>S</sup></th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="ARM: activate to sort column descending" style="width: 36px;">AR<sup>M</sup></th><th class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="ARL: activate to sort column descending" style="width: 36px;">AR<sup>L</sup></th><th style="width:70px" class="sorting" tabindex="0" aria-controls="data" rowspan="1" colspan="1" aria-label="date: activate to sort column descending">date</th></tr>
    </thead>
  <tbody><tr role="row" class="odd"><td><a id="setTeam6" style="font-size:12px">G-RMI</a><sup>7</sup></td><td class="sorting_1">0.413</td><td>0.62</td><td>0.45</td><td>0.231</td><td>0.436</td><td>0.547</td><td>0.34</td><td>0.548</td><td>0.604</td><td>0.424</td><td>0.641</td><td>0.748</td><td><span style="font-size:10px">2016-09-18</span></td></tr><tr role="row" class="even"><td><a id="setTeam10" style="font-size:12px">MSRA_2015</a><sup>11</sup></td><td class="sorting_1">0.371</td><td>0.588</td><td>0.398</td><td>0.173</td><td>0.415</td><td>0.525</td><td>0.319</td><td>0.475</td><td>0.489</td><td>0.267</td><td>0.552</td><td>0.679</td><td><span style="font-size:10px">2015-11-26</span></td></tr><tr role="row" class="odd"><td><a id="setTeam13" style="font-size:12px">Trimps-Soushen</a><sup>14</sup></td><td class="sorting_1">0.359</td><td>0.58</td><td>0.383</td><td>0.158</td><td>0.407</td><td>0.509</td><td>0.312</td><td>0.479</td><td>0.497</td><td>0.269</td><td>0.557</td><td>0.683</td><td><span style="font-size:10px">2016-09-16</span></td></tr><tr role="row" class="even"><td><a id="setTeam8" style="font-size:12px">Imagine Lab</a><sup>9</sup></td><td class="sorting_1">0.35</td><td>0.529</td><td>0.387</td><td>0.146</td><td>0.372</td><td>0.517</td><td>0.315</td><td>0.498</td><td>0.524</td><td>0.294</td><td>0.582</td><td>0.719</td><td><span style="font-size:10px">2016-09-17</span></td></tr><tr role="row" class="odd"><td><a id="setTeam16" style="font-size:12px">Wall</a><sup>17</sup></td><td class="sorting_1">0.345</td><td>0.559</td><td>0.369</td><td>0.165</td><td>0.389</td><td>0.478</td><td>0.303</td><td>0.47</td><td>0.493</td><td>0.266</td><td>0.565</td><td>0.668</td><td><span style="font-size:10px">2016-09-19</span></td></tr><tr role="row" class="even"><td><a id="setTeam5" style="font-size:12px">FAIRCNN</a><sup>6</sup></td><td class="sorting_1">0.332</td><td>0.519</td><td>0.363</td><td>0.136</td><td>0.372</td><td>0.478</td><td>0.299</td><td>0.46</td><td>0.483</td><td>0.234</td><td>0.56</td><td>0.664</td><td><span style="font-size:10px">2015-11-26</span></td></tr><tr role="row" class="odd"><td><a id="setTeam2" style="font-size:12px">CMU_A2_VGG16</a><sup>3</sup></td><td class="sorting_1">0.323</td><td>0.528</td><td>0.341</td><td>0.144</td><td>0.355</td><td>0.452</td><td>0.295</td><td>0.462</td><td>0.471</td><td>0.246</td><td>0.522</td><td>0.65</td><td><span style="font-size:10px">2016-09-19</span></td></tr><tr role="row" class="even"><td><a id="setTeam9" style="font-size:12px">ION</a><sup>10</sup></td><td class="sorting_1">0.307</td><td>0.529</td><td>0.317</td><td>0.118</td><td>0.328</td><td>0.448</td><td>0.277</td><td>0.428</td><td>0.454</td><td>0.23</td><td>0.501</td><td>0.63</td><td><span style="font-size:10px">2015-11-26</span></td></tr><tr role="row" class="odd"><td><a id="setTeam12" style="font-size:12px">ToConcoctPellucid</a><sup>13</sup></td><td class="sorting_1">0.284</td><td>0.497</td><td>0.291</td><td>0.1</td><td>0.327</td><td>0.427</td><td>0.273</td><td>0.394</td><td>0.402</td><td>0.167</td><td>0.466</td><td>0.595</td><td><span style="font-size:10px">2016-09-16</span></td></tr><tr role="row" class="even"><td><a id="setTeam7" style="font-size:12px">hust-mclab</a><sup>8</sup></td><td class="sorting_1">0.276</td><td>0.481</td><td>0.288</td><td>0.103</td><td>0.3</td><td>0.404</td><td>0.259</td><td>0.369</td><td>0.375</td><td>0.149</td><td>0.416</td><td>0.556</td><td><span style="font-size:10px">2016-09-05</span></td></tr><tr role="row" class="odd"><td><a id="setTeam1" style="font-size:12px">CMU_A2</a><sup>2</sup></td><td class="sorting_1">0.257</td><td>0.459</td><td>0.26</td><td>0.058</td><td>0.283</td><td>0.422</td><td>0.248</td><td>0.355</td><td>0.364</td><td>0.106</td><td>0.422</td><td>0.587</td><td><span style="font-size:10px">2015-11-27</span></td></tr><tr role="row" class="even"><td><a id="setTeam15" style="font-size:12px">UofA</a><sup>16</sup></td><td class="sorting_1">0.252</td><td>0.432</td><td>0.267</td><td>0.076</td><td>0.265</td><td>0.393</td><td>0.249</td><td>0.35</td><td>0.355</td><td>0.138</td><td>0.378</td><td>0.558</td><td><span style="font-size:10px">2015-11-27</span></td></tr><tr role="row" class="odd"><td><a id="setTeam3" style="font-size:12px">COCO Baseline</a><sup>4</sup></td><td class="sorting_1">0.242</td><td>0.453</td><td>0.234</td><td>0.072</td><td>0.264</td><td>0.369</td><td>0.238</td><td>0.341</td><td>0.347</td><td>0.115</td><td>0.389</td><td>0.544</td><td><span style="font-size:10px">2016-03-06</span></td></tr><tr role="row" class="even"><td><a id="setTeam4" style="font-size:12px">Decode</a><sup>5</sup></td><td class="sorting_1">0.222</td><td>0.408</td><td>0.218</td><td>0.045</td><td>0.232</td><td>0.369</td><td>0.227</td><td>0.329</td><td>0.335</td><td>0.095</td><td>0.381</td><td>0.539</td><td><span style="font-size:10px">2015-11-27</span></td></tr><tr role="row" class="odd"><td><a id="setTeam17" style="font-size:12px">Wall_2015</a><sup>18</sup></td><td class="sorting_1">0.205</td><td>0.363</td><td>0.211</td><td>0.042</td><td>0.197</td><td>0.339</td><td>0.217</td><td>0.308</td><td>0.317</td><td>0.104</td><td>0.328</td><td>0.493</td><td><span style="font-size:10px">2015-11-27</span></td></tr><tr role="row" class="even"><td><a id="setTeam11" style="font-size:12px">SinicaChen</a><sup>12</sup></td><td class="sorting_1">0.189</td><td>0.362</td><td>0.178</td><td>0.042</td><td>0.194</td><td>0.313</td><td>0.207</td><td>0.3</td><td>0.306</td><td>0.095</td><td>0.329</td><td>0.496</td><td><span style="font-size:10px">2015-11-19</span></td></tr><tr role="row" class="odd"><td><a id="setTeam14" style="font-size:12px">UCSD</a><sup>15</sup></td><td class="sorting_1">0.188</td><td>0.368</td><td>0.176</td><td>0.033</td><td>0.187</td><td>0.317</td><td>0.205</td><td>0.301</td><td>0.311</td><td>0.084</td><td>0.338</td><td>0.522</td><td><span style="font-size:10px">2015-11-27</span></td></tr><tr role="row" class="even"><td><a id="setTeam0" style="font-size:12px">1026</a><sup>1</sup></td><td class="sorting_1">0.178</td><td>0.319</td><td>0.175</td><td>0.025</td><td>0.177</td><td>0.306</td><td>0.178</td><td>0.248</td><td>0.253</td><td>0.049</td><td>0.28</td><td>0.415</td><td><span style="font-size:10px">2015-11-27</span></td></tr></tbody></table></div>
  <p class="titleSegoeLight" style="margin-bottom:15px">Metrics</p>
  <div class="bodyNormal json" style="color:black; width:100%; font-family:courier new; ">
    <div style="display:inline-block; width:30%">
      <b>Average Precision (AP):</b><br>
      <div class="jsontab">AP<br></div>
      <div class="jsontab">AP<sup>IoU=.50</sup><br></div>
      <div class="jsontab">AP<sup>IoU=.75</sup><br></div>
      <b>AP Across Scales:</b><br>
      <div class="jsontab">AP<sup>small</sup><br></div>
      <div class="jsontab">AP<sup>medium</sup><br></div>
      <div class="jsontab">AP<sup>large</sup><br></div>
      <b>Average Recall (AR):</b><br>
      <div class="jsontab">AR<sup>max=1</sup><br></div>
      <div class="jsontab">AR<sup>max=10</sup><br></div>
      <div class="jsontab">AR<sup>max=100</sup><br></div>
      <b>AR Across Scales:</b><br>
      <div class="jsontab">AR<sup>small</sup><br></div>
      <div class="jsontab">AR<sup>medium</sup><br></div>
      <div class="jsontab">AR<sup>large</sup><br></div>
    </div>
    <div style="display:inline-block; width:74%; margin-left:-35px">
      <div class="jsontab">% AP at IoU=.50:.05:.95 <b>(primary challenge metric)</b></div>
      <div class="jsontab">% AP at IoU=.50 (PASCAL VOC metric)</div>
      <div class="jsontab">% AP at IoU=.75 (strict metric)</div>
      <br>
      <div class="jsontab">% AP for small objects: area &lt; 32<sup>2</sup></div>
      <div class="jsontab">% AP for medium objects: 32<sup>2</sup> &lt; area &lt; 96<sup>2</sup></div>
      <div class="jsontab">% AP for large objects: area &gt; 96<sup>2</sup></div>
      <br>
      <div class="jsontab">% AR given 1 detection per image </div>
      <div class="jsontab">% AR given 10 detections per image</div>
      <div class="jsontab">% AR given 100 detections per image</div>
      <br>
      <div class="jsontab">% AR for small objects: area &lt; 32<sup>2</sup></div>
      <div class="jsontab">% AR for medium objects: 32<sup>2</sup> &lt; area &lt; 96<sup>2</sup></div>
      <div class="jsontab">% AR for large objects: area &gt; 96<sup>2</sup></div>
    </div>
  </div>
  <p>Please see the <a href="http://mscoco.org/dataset/#detections-eval">evaluation</a> page for more detailed information about the metrics.</p>
  <p class="titleSegoeLight" style="margin-bottom:0px">References</p>
  <div id="refs_wrapper" class="dataTables_wrapper no-footer"><table id="refs" class="table table-striped hover dataTable no-footer" style="font-size:13px" role="grid">
    <thead><tr role="row"><th class="sorting_disabled" rowspan="1" colspan="1" style="width: 20%;"></th><th class="sorting_disabled" rowspan="1" colspan="1"></th><th class="sorting_disabled" rowspan="1" colspan="1"></th></tr></thead>
  <tbody><tr role="row" class="odd"><td>[1] 1026</td><td>Shu Liu, Xiaojuan Qi</td><td>We use CNN to segment and detect objects. We use a patch merging step to generate final results.</td></tr><tr role="row" class="even"><td>[2] CMU_A2</td><td>Abhinav Shrivastava, Abhinav Gupta, Ross Girshick</td><td>We use hard example mining to train Fast R-CNN (submitted to CVPR16).</td></tr><tr role="row" class="odd"><td>[3] <a href="http://abhinav-shrivastava.info/context_priming_feedback.pdf" target="_blank"> CMU_A2_VGG16 </a></td><td>Abhinav Shrivastava, Abhinav Gupta</td><td>We used Contextual Priming and Feedback (via semantic segmentation) in Faster R-CNN framework (single VGG16 model).</td></tr><tr role="row" class="even"><td>[4] <a href="http://arxiv.org/abs/1506.01497" target="_blank"> COCO Baseline </a></td><td>Ross Girshick</td><td>Faster R-CNN with end-to-end training (https://github.com/rbgirshick/py-faster-rcnn). This entry uses VGG16 pre-trained on ImageNet-1k and an RPN with 4 anchor scales (those in the NIPS paper plus 64^2).</td></tr><tr role="row" class="odd"><td>[5] Decode</td><td>Zeeshan Hayder, Xuming He, Mathieu Salzmann</td><td>Deep Co-Objectness and Detection.</td></tr><tr role="row" class="even"><td>[6] <a href="http://arxiv.org/abs/1604.02135" target="_blank"> FAIRCNN </a></td><td>Sergey Zagoruyko*, Tsung-Yi Lin*, Pedro O. Pinheiro*, Adam Lerer, Sam Gross, Soumith Chintala, Piotr Dollár (*equal contribution)</td><td>Our approach is built on DeepMask proposals fed into the Fast R-CNN pipeline. The DeepMask proposals have been substantially improved to encourage proposal diversity and mask quality. We also augmented our CNN classifier with a novel foveal structure, skip-connections, an improved cost function that encourages better localization, and a few additional modifications. Finally we utilize aggressive ensembling (model and inference) to further improve performance.</td></tr><tr role="row" class="odd"><td>[7] G-RMI</td><td>Jonathan Huang, Chen Sun, Vivek Rathod, Anoop Korattikara, Alireza Fathi, Kevin Murphy, Zbigniew Wojna, Ian Fischer, Menglong Zhu, Yang Song, Sergio Guadarrama</td><td>Ensemble of five Faster RCNN based models jointly trained end-to-end using a pure Tensorflow implementation with differentiable ROI cropping.  We use a combination of Inception-Resnet and (stride 8) Resnet-101 base feature extractors.  All models are trained on train+val (minus minival) and we use multi-crop inference at test time.</td></tr><tr role="row" class="even"><td>[8] hust-mclab</td><td>Peng Tang, Xinggang Wang, Wenfei Jiang, Wenyu Liu</td><td></td></tr><tr role="row" class="odd"><td>[9] Imagine Lab</td><td>Sergey Zagoruyko, Spyros Gidaris, Nikos Komodakis</td><td>Our region based detector consists of the following components. 1) 300 box proposals generated by AttractioNet[1] which is a VGG16-Net based box proposal model pre-trained on ImageNet and fine-tuned on COCO 2014 train set. 2) A WRN-32[2] (Wide Residual Network) based detection model is applied on those proposals. It consists of two subnetworks. -- a Multi-Path[3] recognition module for the categorization of the box proposals and -- a LocNet InOut[4] localization module for category-specific object location refinement. It was pre-trained on ImageNet and fine-tuned on COCO 2014 train+val35 sets. 3) As post-processing we apply Non-Maximum-Suppression with Box Voting[5]. During test-time we use horizontal image flipping and four image scales (400, 600, 800, and 1000). We did not use model ensembles. [1] "Attend Refine Repeat: Active Box Proposal Generation via In-Out Localization", in BMVC 2016. Arxiv link: https://arxiv.org/abs/1606.04446 [2] "Wide Residual Networks", in BMVC 2016. Arxiv link: https://arxiv.org/abs/1605.07146. [3] "A MultiPath Network for Object Detection", in BMVC 2016. Arxiv link:https://arxiv.org/abs/1604.02135. [4] "LocNet: Improving Localization Accuracy for Object Detection", in CVPR 2016. Arxiv link:http://arxiv.org/abs/1511.07763. [5] "Object Detection via a Multi-Region &amp; Semantic Segmentation-Aware CNN Model", in ICCV 2015. Arxiv link: https://arxiv.org/abs/1505.01749.</td></tr><tr role="row" class="even"><td>[10] <a href="http://arxiv.org/abs/1512.04143" target="_blank"> ION </a></td><td>Sean Bell, Kavita Bala, Ross Girshick, Larry Zitnick</td><td>We empirically investigate two complementary methods of improving object detection: (1) we incorporate global image context using stacked lateral recurrent neural networks with rectified linear units and (2) we use skip connections to pool from many different layers in the network and combine them with L2 normalization and re-scaling.  We use a single VGG16 ConvNet model based on Fast R-CNN, with a mix of proposal boxes from MCG (Multiscale Combinatorial Grouping) and RPN (Region Proposal Network, from Faster R-CNN).  We have no augmentation or ensembling at test time (single ConvNet, single scale, single crop, and no flipping).  The method will be posted to arXiv soon, and open source code will be released.  Runtime: ~2.7s per image on a Titan X GPU to evaluate 4000 candidate boxes (excluding proposal generation).</td></tr><tr role="row" class="odd"><td>[11] <a href="http://arxiv.org/abs/1512.03385" target="_blank"> MSRA_2015 </a></td><td>Kaiming He, Shaoqing Ren, Jifeng Dai, Xiangyu Zhang, Jian Sun</td><td>Our object detection entry is based on the ultra-deep ResNet-101 [a] and Faster R-CNN [b]. The details are in [a]. [a] “Deep Residual Learning for Image Recognition”, Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. arXiv 2015. [b] “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks”, Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. NIPS 2015.</td></tr><tr role="row" class="even"><td>[12] SinicaChen</td><td>Ting-Yen Chen, Chu-Song Chen</td><td>background pre-processing filter and sliding based training algorithm for object detection on COCO datasets</td></tr><tr role="row" class="odd"><td>[13] ToConcoctPellucid</td><td>Digvijay Singh</td><td>This method utilizes the faster-rcnn algorithm to classify and detect objects.</td></tr><tr role="row" class="even"><td>[14] Trimps-Soushen</td><td>Jie Shao, Lin Mei, Chuanping Hu</td><td>Models were trained following the Faster-RCNN pipeline. In test phase,  AttractioNet proposals were used instead of RPN proposals. We average the softmax scores and the box regression outputs across multiple models. Other improvements include boxes voting and features maxout. </td></tr><tr role="row" class="odd"><td>[15] UCSD</td><td>Yongxi Lu</td><td>We use AZ-Net to generate region proposal. The detector is Fast R-CNN. Both models are trained on MS COCO trainval set, using VGG 16 models pre-trained on ImageNet. The region proposals are augmented with six neighboring boxes. The test time is 380 ms per image.  The Fast R-CNN detector is trained at 400k iterations.</td></tr><tr role="row" class="even"><td>[16] UofA</td><td>Zifeng Wu</td><td>This method is developed based on fast rcnn.</td></tr><tr role="row" class="odd"><td>[17] Wall</td><td>Yin Li, James M. Rehg</td><td>We use a two-branch network to capture small objects in COCO datasets. Different branches pool a shared convolutional features using different spatial resolutions. Our system is based on Faster-RCNN with ResNet101 models pre-trained on ImageNet. The results are reported for a single model tested on a single scale. For Challenge2016, the team had mistakenly submitted a model that is not sufficiently trained.</td></tr><tr role="row" class="even"><td>[18] Wall_2015</td><td>Yin Li, Prateek Singhal, Ahmad Humayun, Zhengyang Wu, Henrik Christensen, James M. Rehg</td><td>This is a baseline of POISE + fast RCNN with 260K iterations</td></tr></tbody></table></div>
</div>
<br>
<br>
</div>
          </div>
        </div>
      </div>
    </div>
  </div>
  <div class="footer">
  <div class="centerFullArea" style="font-weight:100">
    <div style="display:inline-block; padding-left:85%; line-height: 30px; font-size:14px; font-weight:100">
      <a href="http://mscoco.org/terms_of_use" target="_blank" style:"color:white"="">Terms of Use</a>
    </div>
  </div>
</div>




</body></html>